I")<h1 id="intro-to-rl">Intro To RL</h1>
<ul>
  <li>之前对RL的理解一直是只言片语，没有“系统地入个门”（雾），之后把RL相关的一些碎片知识都整理到这个post里面把</li>
</ul>

<h2 id="核心四元结构">核心四元结构</h2>
<p><img src="https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908082919.png" alt="" /></p>
<ul>
  <li>对Agent（智能体）从环境中学习的过程建模</li>
</ul>

<h2 id="分类辨析-a-simple-taxonomy">分类辨析 A Simple Taxonomy</h2>
<p><img src="https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908083740.png" alt="" /></p>
<ul>
  <li>Model Free &amp; Model Based 理不理解所处的环境
    <ul>
      <li>不去对环境建模，直接接受真实世界的反馈 - <strong>Model-Free</strong>
        <ul>
          <li><em>Q-Learning,Sarsa,Policy Gradients</em></li>
        </ul>
      </li>
      <li>智能体自己建立一个模型去模拟真实世界对自己行为的反馈 <strong>Model-Based</strong>
        <ul>
          <li>相比于Model-Free多了一个建模的工具，多了一个沙盒模拟的过程</li>
          <li>相比Model-Free的方法，多了“想象力”和“推测”，Model-Free只能等待真实世界的反馈，一步步进行；而Model-Based的方法可以预测出未来几步，并且选择一个更加pleasant的预测来执行（思考一下围棋，看几步）</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Policy-Based &amp; Value-Based
    <ul>
      <li>Policy-Based 基于概率的方法：通过分析所处的环境，对所有action（决策）给一个发生概率
        <ul>
          <li>Policy Gradient</li>
        </ul>
      </li>
      <li>Value-Based 基于价值的方法，输出所有action的价值然后选择一个最好的
        <ul>
          <li>Q-Learning,Sarsa</li>
        </ul>
      </li>
      <li>两者结合
        <ul>
          <li>Actor-Critic: Actor基于Policy，基于概率给出动作，然后Critic对所作出的动作给出价值（相比于单纯的Policy Gradient加速了学习过程）</li>
        </ul>
      </li>
      <li>❓: 有点像Hard/SoftMax?</li>
    </ul>
  </li>
  <li>回合更新/单步更新
    <ul>
      <li>传统的比如Monte-Carlo Leaning，Policy Gradient（OLD），一整个回合结束才会总结更新</li>
      <li>Q-Learing，Sarsa，Policy Gradient（NEW），每一步都更新，<em>效率更高</em>，现在大部分的都是</li>
    </ul>
  </li>
  <li>On-Policy/Off-Policy
    <ul>
      <li>在线： Sarsa</li>
      <li>离线： QLearning</li>
    </ul>
  </li>
</ul>

<h2 id="关键定义">关键定义</h2>
<ul>
  <li>RL的<strong>核心</strong>：
    <ol>
      <li>如何定义采取动作之后的Reward的评价标准（reward的设计不仅要符合最后我们所需要达到的目的，还需要注意在学习过程中不同action的比较）
        <ul>
          <li>本质是需要让agent学到最好的状态转移矩阵</li>
        </ul>
      </li>
      <li>我们如何更新操作
        <blockquote>
          <p>Q-Learning中，会有一张很大的Table，记录了所有state对应的所有action的值，我们按样本学习每个单元格，使最后的Reward最大</p>
          <ul>
            <li>基于Value的方法 - DQN</li>
          </ul>
        </blockquote>
        <ul>
          <li>但是由于表过度大，DQN用一个NN来查表</li>
          <li>DQN通过experience reply(解决机器学习独立同分布的问题) ❔</li>
          <li>用一个网络估计Q和现实Q之差，作为loss
        * 基于Policy的方法</li>
          <li>直接优化Policy简单明快（NN直接输出Policy），问题被转化为如何更新NN参数
            <ul>
              <li>有stochastic / deterministic
                <ul>
                  <li>Stochastic 需要采样，计算大  - PPO</li>
                  <li>Deterministic 已经证明过了策略梯度公式 - DPG/DDPG
                    <ul>
                      <li>融合Value与Policy - Actor-critic方法 -&gt; A3C</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
  <li>State：对于现实世界情况的完全描述（往往是比较抽象的建模）
    <ul>
      <li>用一个Tensor来作为<em>Representation</em></li>
    </ul>
  </li>
  <li>Observation与State相对应，是当前世界不完全的描述，但是一般会比较具体  <del>我不到啊我瞎猜的</del></li>
  <li>Action State-决策空间
    <ul>
      <li>可能连续（机器人在实际场景中）可能离散AlphaGO</li>
    </ul>
  </li>
  <li>Policy - a rule used by agent to decide which action to take <del>我好像想不到一个太好的中文翻译，决策方式？</del>
    <ul>
      <li>可以是Deterministic（Hard） <em>Max</em></li>
      <li>也可以是Stochastic （Soft） <em>Soft</em>
        <ul>
          <li>Categorical Policies - Used In Discrete
            <ul>
              <li>就类似建立一个对离散Action的Classifier</li>
              <li>🤔已知了各个Action的概率分布，对输出需要做一个<em>Sampling</em></li>
            </ul>
          </li>
          <li>Diagonal Gaussion Policies - Used In Continuous
            <ul>
              <li>一个高维的高斯分布，可以由一个均值Vector&amp;协方差矩阵来描述
                <ul>
                  <li>Diagnol Gaussian Distribution是协方差矩阵是对角阵的时候,这个时候协方差矩阵也可以拿一个Vector来表示</li>
                </ul>
              </li>
              <li>一般这种方式通过一个NN把输入的Observation映射为Mean Vector以及Covariance Matrix的一个代表，可以是
                <ul>
                  <li>直接是Vector Of Log Standard Variation （标准差的Log的向量）</li>
                  <li>通过一个NN建立一个state到log(σ)的映射（或许会与输出mean的网络share param）</li>
                  <li><em>用Log的原因是值域覆盖整个实数域，而标准差在(0,1),而且没有什么精度损失</em></li>
                </ul>
              </li>
              <li>Sampling就是一个采样，已知一个分布之后 Mean + Variation*Noise</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Trajectory (episodes / rollouts)
    <ul>
      <li>A Sequence T={s0,a0,s1,a1…}</li>
      <li>The 1st State Is ramdom sample of state-state-distribution</li>
      <li>状态之间的转换是由Environment所决定的</li>
      <li>为了算法研究，一般具有马尔科夫性，即只依赖于前一状态</li>
    </ul>
  </li>
  <li><strong>Reward</strong>
    <ul>
      <li>rt = R{st,at,st+1}</li>
      <li>经常被简化为R(st)或者是R(st,at)</li>
      <li>The Goal Of a Agent is to get <em>Maxium cumulative reward</em></li>
      <li>Final Reward(return)
        <ul>
          <li>Infinite with discount R = sum((discount_factor)*rt)
            <ul>
              <li>discount factor (0,1) The Earlier Than Now,The Less</li>
              <li><em>Used For Better Convergence</em></li>
            </ul>
          </li>
          <li>Finite Without Discount</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>The Main Problem</strong>
    <ul>
      <li></li>
    </ul>
  </li>
</ul>

<h2 id="q-learning">Q-Learning</h2>
<ul>
  <li>基于表格的方法（表格的XY轴分别是当前state和当前时刻所作出的决策；里面的值是潜在奖励-Q值）
    <ul>
      <li>这个表格<em>Q-table</em>就像是个状态转移矩阵：决定了每一个状态需要采取什么样的策略</li>
    </ul>
  </li>
  <li>那么Q值是怎么定义和更新呢？
    <ul>
      <li>如果出现了新的state（插入全0或者特定初始化）拓展QTable
<img src="https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908104247.png" alt="" /></li>
    </ul>
  </li>
  <li>每次更新都会用到Q的真实值和预测值</li>
  <li>alpha是学习率，表示学习速度 / gamma是对未来reward的衰减值（深谋远虑的程度）</li>
  <li>Q-Learning是Off-Policy的（Q-Table存储了之前的经验，其更新可以不基于正在经历的经验）
    <ul>
      <li>但是我们并没有利用到Q-Learning OFF-policy的特性，在DQN中会有用到</li>
    </ul>
  </li>
</ul>

<h2 id="sarsastate-action-reward-state-action-这个名字也太魔性了">Sarsa(State-Action-Reward-State-Action-这个名字也太魔性了…)</h2>
<ul>
  <li>前向推断方式类似于Q-Learning（都是从一个QTable中选取最大Q值的策略）</li>
  <li>更新方式不同：
<img src="https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908135538.png" alt="" /></li>
  <li>可以理解为对差距的定义不一样：
    <ul>
      <li>Q-Learning是 “下一状态的最佳Q” - “当前状态Q” （可以看做观察了下一状态会怎么样，再做判断（还有转机））</li>
      <li>而Sarsa直接对 “下一状态的Q” - “当前状态的Q” （可以看做已经做出了决定到了下一状态，不再有回旋余地）</li>
      <li>Q-Learning在当前时刻只会定当前时刻的Action（决定了下一时刻的State）但是不会确定下一时刻的Action（需要看MaxQ(s’,a’)）</li>
      <li>Sarsa的每一时刻都决定了当前时刻的Action以及下一时刻的Action
        <ul>
          <li>每一时刻参与学习的参数有(State-Action-Reward-State’-Action’)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>这么看Q-Learning更加<strong>贪婪</strong>（体现在最大化maxQ）一些，如果两个模型都表现得好的话，Q-Learning能找到最近的路线，而Sarsa则会表现的相对<strong>保守</strong>（因为不保守的都死了）</li>
  <li>Sarsa属于<em>单步更新</em>的算法</li>
</ul>

<h2 id="improved-sarsa---sarsalambda">Improved-Sarsa -&gt; Sarsa(lambda)</h2>
<ul>
  <li>加速的Sarsa，主要就是把<em>单步更新</em>改成了<em>N步更新</em>，能够避免“原地打转”的情况
    <ul>
      <li>lambda在[0,1] - 0就退化为简单Sarsa，1就变成直到获取了reward才更新</li>
    </ul>
  </li>
  <li></li>
</ul>

<h2 id="ref">Ref</h2>
<ul>
  <li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#id20">OpenAI SpinningUp</a></li>
  <li><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-1-A-q-learning/">Morvan’s Blog</a></li>
</ul>
:ET