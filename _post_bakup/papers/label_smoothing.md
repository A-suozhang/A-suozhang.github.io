

# [Regularization via Structural Label Smoothing]()

##  ğŸ”‘ Key:        

* æå‡ºäº†ä¸€ç§è®¾è®¡Label Smoothingçš„æ–¹æ³•ï¼Œå¹¶ä¸”é€šè¿‡æ¨å¯¼BER(Bayesian Error Rate)æ¥å†³å®šæœ€ä½³çš„ç­–ç•¥

##  ğŸ“ Source: 

* AISTATS2020

##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

* ä¾æ®Dataä»¥BERä¸ºå‡†åˆ™æ”¹LSçš„alpha

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  





# [Does Label Smoothing Mitigate Label Noise?]()

##  ğŸ”‘ Key:        

* proposed Label Smearing(a more general case of label smoothing)
* è¯´æ˜äº†label smoothingèƒ½å¤Ÿæ”¹å–„åœ¨noisy labelä¸‹çš„classificationä»»åŠ¡ï¼Œè¯´æ˜äº†å…¶ä¸æœ¬èº«noisy labelæƒ…å¢ƒä¸‹å¸¸ç”¨çš„forward/backward correctionæ–¹æ³•æ˜¯ç›¸å…³çš„

##  ğŸ“ Source: 



##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210216205049.png)

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  

* è°ƒæ•´çš„æ˜¯label smoothingçš„smoothing factor(\alpha)ä¹Ÿå°±æ˜¯



# [Label Confusion Learning to Enhance Text Classification Models](http://arxiv.org/abs/2012.04987)

##  ğŸ”‘ Key:         

* Instances may related to multi-labels
* one-hot, over confidence, model overfitting
* confused/noisy datasets

##  ğŸ“ Source: 

* Shufe

##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

* LCM(Label Confusion Model)ï¼š 
  * discover the semantic overlap among labels
  * label distribution changes for different instances with same label
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210210112038.png)
* è®¾è®¡ä¸€ä¸ªlabelçš„embedding networkï¼Œä¸dataçš„embeddingè®¡ç®—KLæ•£åº¦
  * è¾“å‡ºä»¥ä¸€ä¸ª\alphaæ¯”ä¾‹åŠ ä¸Šone-hotå†softmaxä½œä¸ºæœ€åçš„softlabel

##  ğŸ“ Exps:

* åœ¨mnistä¸Šåšçš„å®éªŒâ€¦å¯ä»¥è¯´æ˜¯åŸºæœ¬å®Œå…¨æ²¡ç”¨äº†*

  ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210210113052.png)

##  ğŸ’¡ Ideas:  



----

# Det + KD

## [Learning Efï¬cient Detector with Semi-supervised Adaptive Distillation]()

##  ğŸ”‘ Key:         

* é’ˆå¯¹One-stageçš„detector
* é’ˆå¯¹ä¸¤ç§éš¾æ ·æœ¬(hard sample)å»åˆ©ç”¨teacheræŒ‡å¯¼studentå­¦ä¹ 
  * hard-to-learn: teacher with low certainty
  * hard-to-mimic: teacher & student differes greatly
* åªå¯¹åˆ†ç±»çš„Focal Lossè¿›è¡Œäº†Distillation

##  ğŸ“ Source: 

* Sensetime

##  ğŸŒ± Motivation: 

* ç”¨åˆ°äº†SemiSupervisedçš„Distillation flow

##  ğŸ’Š Methodology: 

 ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210218210255.png)

##  ğŸ“ Exps:

* Teacher backbone res101; student res50

  ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210218210323.png)

##  ğŸ’¡ Ideas:  



## [Learning Efficient Object Detection Models with Knowledge Distillation]()

##  ğŸ”‘ Key:        

* Hint Learning
  * adaptive layers of feature learning
  * å¯¹é½intermediateçš„feature representation
* KD: 
  * weighted CE for solving the class imbalance
  * teacher-bounded loss

##  ğŸ“ Source: 

* NIPS2017

##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210218211617.png)

1. Weighted CE for Class Imbalance:
   * æ˜¯ä¸€ä¸ªone-hardçš„Labelçš„hard loss
   * ä»¥åŠåˆ©ç”¨teacher logitsçš„soft lossçš„åŠ æƒ
   * ç”±äºåœ¨classificationä¸­åªå­˜åœ¨foregroundä¹‹é—´çš„è¯¯åˆ†ç±»ï¼Œè€Œdetä¸­å¾ˆå¤šçš„æ˜¯foregroundå’Œbackgroundä¹‹é—´çš„è¯¯åˆ†(åªæ˜¯ä¸€ä¸ªå›ºå®šdefineçš„å¯¹äºbackgroundæ›´å¤§çš„æƒé‡1.5/1)
2. KD Regression Loss
   * å¯èƒ½ä¼šæ˜¯éå¸¸errorçš„guidance
   * è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªstudentçš„upper boundï¼Œå¦‚æœteacherçš„é”™è¯¯æ¯”studentè¿˜å¤§ï¼Œå°±ä¸ç”¨äº†
   * [?] - æ—¢ç„¶FPNå¹¶ä¸æ˜¯å…¬ç”¨çš„ï¼Œå¦‚ä½•ä¿è¯è¾“å‡ºçš„anchoræ˜¯ä¸€æ ·çš„å‘¢ï¼Ÿ(Hintæ˜¯å¦åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯åœ¨å¯¹é½ï¼Ÿ)
3. Hint Learning
   * ä»¥L1 Losså¯¹é½intermediate activationï¼Œä¸­é—´æ’ä¸€ä¸ªadaptation layer(ç”¨çš„æ˜¯1x1 conv)ï¼Œå‘ç°äº†å³ä½¿æ˜¯channel sizeä¸€æ ·è¿™ä¸ªå±‚ä¹Ÿå¾ˆå…³é”®
   * å¦‚æœresolutionå¯¹ä¸ä¸Šï¼Œç”¨äº†ä¸€ä¸ªç®€å•çš„padding trick

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  



## [LabelEnc: A New Intermediate Supervision Method for Object Detection](http://arxiv.org/abs/2009.14416)
##  ğŸ”‘ Key:       

* æ‰¾åˆ°ä¸€ç§intermediateçš„representationæ¥enforce det taskçš„learning(é€šè¿‡å¯¹Labelè¿›è¡Œencoding)
* ç”¨ä¸€ä¸ªAutoEncoderå­¦ä¹ åˆ°autoencoding labelçš„æ–¹æ³•ï¼› ç„¶åå†ç”¨è¿™ä¸ªlabel representationï¼Œç”¨ä¸€ä¸ªauxiliary lossæ¥è®­ç»ƒbackbone

##  ğŸ“ Source: 

##  ğŸŒ± Motivation: 

* insightæ¥è‡ªäºbakcboneçš„weightçš„pretrainå¹¶ä¸æ˜¯det-awareçš„ï¼Œæ‰€ä»¥å¯èƒ½suboptimal
  * ç”±äºdet headä¸­çš„weightæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œæ‰€ä»¥è¿™ä¸€æ”¯çš„gradå¾ˆnoisyï¼Œç”šè‡³ä¼šå¯¹backboneçš„è®­ç»ƒdegradation(æ¯”å¦‚æœ‰çš„æ—¶å€™è®­ç»ƒheadçš„æ—¶å€™éœ€è¦freezeä½backboneçš„å‚æ•°ï¼Œå°±æ˜¯ä¸ºäº†é¿å…è¿™ç§æƒ…å†µ)

##  ğŸ’Š Methodology: 

* LabelEncï¼š 

  * label encoding functionåº”è¯¥æ˜¯å»approximate optimalçš„det headçš„é€†
  * æ±‚é€†å›°éš¾ï¼š ç”¨ä¸€ä¸ªLabelç©ºé—´çš„AutoEncoderå»å»ºæ¨¡NNçš„é€†(å°±æ˜¯det headï¼Œç”±äºé«˜åº¦éçº¿æ€§ï¼Œæ‰€ä»¥å¾ˆéš¾è·å–)
    * è®­ç»ƒlossæ˜¯reconstruction lossä»¥åŠdet loss
  * å¯¹äºå¦‚ä½•æ‰¾åˆ°optimalçš„headï¼Œä¸€èµ·alternativeçš„è®­ç»ƒå­˜åœ¨recursiveçš„ä¾èµ–ï¼Œé‡‡ç”¨äº†ä¸€ä¸ªâ€œunrollingâ€
    * æœ‰ç‚¹åƒæ˜¯Bi-Levelçš„formulate?åœ¨appendixä¸­æœ‰æ¨å¯¼
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210219104759.png)

  ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210219104226.png)

  ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210219104247.png)

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  

* Joint training of auto-encoderå¾ˆå…³é”®ï¼š
  * åœ¨è®­ç»ƒlabel encodingçš„æ—¶å€™ä¹Ÿéœ€è¦det backboneï¼Œä¸èƒ½åªç”¨reconstruction



## [ONE SIZE DOESNâ€™T FIT ALL: ADAPTIVE LABEL SMOOTHING]([2009.06432.pdf (arxiv/org)](https://arxiv.org/pdf/2009.06432.pdf))

##  ğŸ”‘ Key:         

* æ ¹æ®GTçš„bboxåœ¨æ•´ä¸ªå›¾ç‰‡ä¸­çš„å æ¯”ï¼Œè°ƒæ•´distillationçš„æ¯”ä¾‹

##  ğŸ“ Source: 

##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  





## [MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection](http://arxiv.org/abs/2009.11528)

##  ğŸ”‘ Key:         

* åœ¨one-stageä»¥åŠtwo-stageä¹‹é—´æ’ä¸€ä¸ªtransform layerå»è®©anchorå¯¹é½ï¼Œèƒ½å¤Ÿdistillation

##  ğŸ“ Source: 

##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  



# [Distilling Object Detectors with Fine-grained Feature Imitation](http://arxiv.org/abs/1906.03609)


##  ğŸ”‘ Key:         

##  ğŸ“ Source: 

##  ğŸŒ± Motivation: 

##  ğŸ’Š Methodology: 

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  