## [AutoTrans: Automating Transformer Design via Reinforced Architecture Search](arXiv:2009.02070 [cs])

ğŸ”‘ Key:    

*  

ğŸ“ Source: 

ğŸŒ± Motivation: 

ğŸ’Š Methodology: 

* Search Space

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210303083620.png)

- LSTM(Controller) - Reinforceï¼› ENAS/NASNet-like flow
- Param Sharing:
  - Cross-operation: 
    - é™åˆ¶äº†different num of heads has the same param size
  - Cross-Layerï¼š
    - Albert proves that cross-layer sharing could stabilize training
    - the parameters in the previous transformer layer is shared to the next one.

ğŸ“ Exps:

ğŸ’¡ Ideas:  



## [Evolved Speech-Transformer: Applying Neural Architecture Search to End-to-End Automatic Speech Recognition](http://www.isca-speech.org/archive/Interspeech_2020/abstracts/1233.html)
ğŸ”‘ Key:    

*  

ğŸ“ Source: 

ğŸŒ± Motivation: 

ğŸ’Š Methodology: 

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210303090703.png)

* Search Space (Cell-based)

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210303090116.png)

* Cell-based
  * æ„Ÿè§‰å®ƒæ²¡æ€ä¹ˆè®²æ˜ç™½å®ƒçš„SSæ˜¯æ€ä¹ˆè®¾è®¡çš„â€¦æœ‰ä¸¤ä¸ªbranch
* Evo-based Search: Progressive Dynamic Hundles

ğŸ“ Exps:

ğŸ’¡ Ideas:  



## [Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition](http://arxiv.org/abs/2008.06808)

ğŸ”‘ Key:    

ğŸ“ Source: 

ğŸŒ± Motivation: 

ğŸ’Š Methodology: 

* Search Spaceï¼š
  1. Attention Key/Query Dimensions
  2. Width/Depth for the feed-forward layers
  3. Num of attention heads
  4. Layer norm mean computation:ï¼ˆä¸€äº›æœ€è¿‘çš„å·¥ä½œè¡¨ç¤ºä¸ä¸€å®šéœ€è¦zero-meanï¼Œè€Œæ˜¯å‡å»w*mean;wæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ï¼‰

* å¯¹äºæ¯ä¸ªComponentï¼Œé€šè¿‡profilingæ¥è·å¾—å…¶Costï¼Œcomponentè¦ä¹ˆæ˜¯æ¥åœ¨ä¸Šä¸€ä¸ªcomponentçš„å(vertical)ï¼Œè¦ä¹ˆæ˜¯å¹¶è¡Œ(Parallel)
  * å¯¹æ¯ä¸ªcomponentæœ‰ä¸€ä¸ªconnecting weight
  * å®šä¹‰äº†ä¸€ä¸ªconnector Connect(f,w)(X,R) = (X+w(f(X)+R), (1-w)(f(X)+R)
  * ç»´æŠ¤äº†ä¸€ä¸ªcumultaive outputä½œä¸ºæ¯ä¸ªconnectorçš„è¾“å…¥

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210303101448.png)

ğŸ“ Exps:

ğŸ’¡ Ideas:  

* Smaller models do not always mean faster models:
  * åˆ†æäº†å‡ ç§å¸¸ç”¨çš„efficient computingçš„æ–¹å¼ï¼Œä»¥åŠFLOPs-orientedçš„ï¼Œå¹¶ä¸ä¸€å®šèƒ½å¤Ÿæå‡é€Ÿåº¦
  * Param Sharing: ä¸€èˆ¬éœ€è¦æ›´å¤§çš„modelï¼Œcomputeæ›´æ…¢äº†
  * Pruning/Quantizeéœ€è¦specialized hardware

* vertical feedforward connection and layernorm are relatively more expensive
  * FF as 8 componnets
  * Query-Key as 2 
  * Each head as 1

* ä¸€äº›æ¶æ„é€‰æ‹©çš„insight
  * Layer NormåŸºæœ¬æ²¡æœ‰è¢«é€‰æ‹©åˆ°ï¼Œæ€€ç–‘å…¶æœ‰æ•ˆæ€§
  * key-query similarity in attention can be dropped in earlier layer
  * dkå¯ä»¥å‡å°ï¼Œä½†æ˜¯dvä¸è¡Œ
  * FF vertically connectedæ›´å¥½



## [Point Transformer](https://arxiv.org/abs/2102.12627)

ğŸ”‘ Key:      

* å°†Transformerå¼•å…¥3dï¼Œdesignäº†ä¸€ä¸ªPointTransformerçš„Layer,åšäº†classification & dense prediction task(also as backbone for the 3d scene understanding)

ğŸ“ Source: 

* Oxford & Intel

ğŸŒ± Motivation: 

* Transformerå°†æ•°æ®çœ‹ä½œäº†ä¸€ä¸ª**set**æ¥æè¿°ï¼Œä¸point cloudçš„intrinsicæœ¬èº«ç›¸åŒ(positional informationè¢«ä½œä¸ºattributeåŠ å…¥)

ğŸ’Š Methodology: 

1. Vector Attentionä¸æ ‡å‡†çš„Scalar Attentionä¸åŒï¼Œä¸åŒäºä½¿ç”¨ä¸€ä¸ªdot productè·å¾—ä¸€ä¸ªå€¼ï¼Œç”¨äº†ä¸€ä¸ªrelation+mappingçš„æ–¹å¼å»ä¿æŒdimï¼Œæœ¬æ–‡ä¸­ç”¨åˆ°äº†subtractä½œä¸ºrelationï¼Œä»¥åŠä¸€ä¸ªMLPä½œä¸ºmapping
2. å…¶ä¸­å‚ä¸Attention operationçš„ä¸€ç³»åˆ—ç‚¹æ˜¯local neighborhood points(å¯èƒ½æ˜¯ç”±KNNæ‰€è·å¾—çš„)
3. å¯¹äºpositional encodingï¼ŒåŒæ ·æ˜¯ MLP(x_i - x_j)
   * empirically foundå¾ˆé‡è¦
4. åŸºç¡€Transformer Block
5. Transition down(downsample) block: reduce the cardinality of the point set.
   * Farthest Point Sampling to find a well-spread point set
   * KNN
   * Linear Transformation(MLP)
   * Max-pool onto each point
6. Transition-Up Task: å¯¹äºdense prediction taskï¼Œéœ€è¦ä¸€ä¸ªU-Netç»“æ„ï¼Œç”¨ä¸€ä¸ªsymmetricçš„encoder-decoderç»“æ„
   * å…ˆè¿‡ä¸€ä¸ªLinear T
   * ç„¶åæ˜¯ä¸€ä¸ªTrilinear Interpolation
   * ç„¶åfeatureå’Œå¯¹åº”çš„encoderè¾“å‡ºsumèµ·æ¥è¾“å‡º
7. Output Head
   * Seg:
   * Classification: Global Avg Pool

* è¿™å…¶ä¸­æ•´ä¸ªè¿‡ç¨‹ç”¨åˆ°éƒ½æ˜¯æ­£å¸¸çŸ©é˜µä¹˜æ³•ï¼Œæ‰€ä»¥shapeä¸€ç›´æ˜¯ä¸€è‡´çš„

ğŸ“ Exps:

ğŸ’¡ Ideas:  



## [Point Cloud Transformer](http://arxiv.org/abs/2012.09688)

ğŸ”‘ Key:         

ğŸ“ Source: 

ğŸŒ± Motivation: 

ğŸ’Š Methodology: 

1. Coord-based Input embedding: replacing the original positional encoding 
   * çœ‹èµ·æ¥æ˜¯2ä¸ª1x1 conv [3,64], [64,64]
2. A offset-attention module: replacing the original self attention
   * insight from the Laplacian Matrix (offset of degree matrix and adjacent matrix)
   * æ”¹å˜äº†atentionçš„æ–¹å¼ï¼Œç”±äºæ­¤å¤„çš„attention mapçš„shapeæ˜¯[num_point, dims],å¯¹1st dimåšsoftmaxï¼Œ2nd dimç”¨L1 normæ¥å½’ä¸€åŒ–(åŸæœ¬çš„transformerå¯¹äº[num_token, num_token]çš„ç»“æ„ï¼Œç”¨stdæ ‡å‡†åŒ–1st dimï¼Œå¹¶ä¸”softmax 2nd dim)
3. Neighborhood Embedding module: stress the local geometric infoï¼š
   * å‚è€ƒäº†PointNet++ä¸­çš„local neighbor aggregationï¼Œç”¨ä¸¤ä¸ªSG(sample & group)å»æ‰©å¤§receptive field
     * å¯¹Pä¸ªç‚¹ï¼Œå…ˆé€šè¿‡Farthest point samplingç»™downsampleåˆ°P_sä¸ªsample pointï¼Œå†å¯¹å…¶ä¸­çš„æ¯ä¸ªsample pointï¼Œæ‰¾knnä¸ªç‚¹
   * åŠ åœ¨æœ€åçš„embeddingåé¢ï¼ˆä»¥åŠpossibleç”¨åœ¨ä¸€å¼€å§‹çš„stemåé¢ï¼‰

ğŸ“ Exps:

ğŸ’¡ Ideas:  

* Transformer is suitable for point cloud for its inherently permutation invariant



## [SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks](http://arxiv.org/abs/2006.10503)
* æå‡ºäº†ä¸€ç§å¯¹SE-3ä¿æŒequivalenceçš„ç‚¹äº‘self-attentionçš„æ–¹å¼
  * invariant to input rotation and translation
  * equivariant(åŒå˜æ€§) to permutation

ğŸ”‘ Key:         

* è´´åˆç‚¹äº‘æ•°æ®ç‰¹æ–°çš„Self-Attentionæ–¹æ³•
* ä¸¤ä¸ªä¸»è¦éƒ¨ä»¶:åƒæ˜¯ä¸€å¼ å›¾é‡Œçš„ç‚¹å’Œè¾¹
  1. input-dependent attention-weights (Edges)
  2. embedding of the input

ğŸ“ Source: 

ğŸŒ± Motivation: 

* Should be invariant to  global changes(transformation) of the input pose

ğŸ’Š Methodology: 

* 

ğŸ“ Exps:

ğŸ’¡ Ideas:  

* roto-translation equivariant operation for 3d point-cloud

* Tensor filed Network: map point-cloud to point-cloud under constraint of SE3-equvariance