---
layout:     post                    # 使用的布局（不需要改）
title:      Wild Ideas           # 标题 
subtitle:   一些不太成熟的idea，期待打脸       #副标题
date:       2020-01-01             # 时间
author:     tianchen                      # 作者
header-img:  img/bg-term.jpg  #这篇文章标题背景图片  
catalog: false                     # 是否归档
tags:                               #标签
     - private
---

# 2020-05 毕设最终


### BG

* 主要focus在对DA问题的低比特训练
* 传统低比特训练的核心问题
  * 从历史谈起，WA的8很容易
  * 引用WAGE，纯8很难
  * 尤其是梯度的问题，引用sensetime文章的观点
* 在一定量的误差率下
  * 把数学约束写出来 - 参考RangeBN
    * Q-error由两者共同决定
* 关注在BN的部分
  * BN的梯度更新复杂
  * Fix-BN的训练方式，在变的情况下不同
  * 更加Noisy
  * 采用浮点运算 
* 核心是关注如何节省**硬件资源**
  * 之前节省的一些硬件资源
    * OPs
  * 《Towards》中凯哥的分析
  * 8比特的白给4倍-存储与计算
  * 8比特乘法器的资源消耗
    * LUT数量只能做一个大概的预估

### methods
* 按照layer-wise浮点scale
  * 扩大动态范围，浮点scale的设定
* 利用stochastic rounding加大等效精度
  * 主要是对grad，8b比特范围不够
* grad-clipping
* BN
  * BN的mean/var保持浮点
  * BN的梯度计算
    * 仅对weight与bias
  * training中固定BN-param随之累积
    * 在两个领域分别累计BN参数
    * stu网络的BN-param和tch分别积累


### Perf

### Analysis





# 2019-10-11 ~ 2020-02 Online Learning on Edge

* 借用给振华做PPT的机会梳理一下这个东西
* HardwareTraining
1. Challenges (在线训练，而不是Deploy(不清楚rram会不会有这个问题)就是Motivation相关的一些问题)
  * 不知道是PIM所以是不是可以做到一些修正 / 感觉有一些问题也可以不care，因为实际上并不大可能完成所有任务
  * 相比于预先训练好再Apply，有何优势
  * Dataset的储存相对大量-对比于参数量来说
  * 在线的时候Label从哪里来？
2. 
  * Plain Training - Sparse-Training
  * Finetune Certain Layer (User/Domain Adaptation) - Transfer to a relative Small Problem
  * Semi-Supervised Learning(Unsupervised Learning)
  * Incremental/Continual Learning/MetaLearning
* 每个Field的展开点： 问题定义 / 可能的Motivation / 算法的思想(算法对硬件的要求) / 算法本身的成熟程度 / 
  

# 2020-02 GATES

* 提升基于predictor的NAS
* embedding-将离散的网络架构映射到一个离散空间
* NAS的两个主要模块
  * Searching module - Controller
    * 基于predictor来sample，更好的sample
  * Architecture Evaluation - Predictor
    * 主要问题 - 慢（对于每个架构都要训练好多个epoch）
    * 解决方案
      * shared weights - 一个supernet训练各个部分
        * 加速architecture的evaluation过程
      * predictor是增加sample efficiency（与shared weights对应）(提高sample efficiency也可以用优化controller)
* 早期的encoding基于序列化的方法
  * 人为选择一个方法（规则）把一个图给序列化
  * 而本文使用了GCN的方式（还对gcn进行了一些修改）
    * 传统的GCN无法处理边上带有信息的（边只是表示连接或者不连接）
    * 传统GCN的边的意义是affinity（相似性），edge在NN中是information flow（相连的节点不一定相似），GCN节点上的信息是attribute也就是属性。
      * 有的文章直接把两个节点的embedding拼接起来（并不能保证同构的图能够获得一样的embedding）
      * 本文的目的GATES是去建模NN的information flow，首先将NN建模为一个异构图（包含了数据节点和op节点），information（是一个vector）经过每一个op，会做一个attention，相当于对information加上了一个该op所对应的mask，经过所有op之后最后的那个vector就是该网络的embedding

# 2020-03 毕设中期

* FPGA加速器从Inference，到具有一定的Training能力，改进模型
* 主要在Finetune层面，FewLabel的Finetune；做到User-Adaptive或者是Scenario Adaptive的迁移(半监督，甚至是无监督的迁移)
  * 手写数字迁移 
    * 4 Domain ->
  * Office31 -> 
    * ImageNet Pretrained -> Different Domains
* 算法本身的调整 + 以及定点稀疏
  * 定点
  * Higher Precision BN
    * Fixed - Seldom Update


# 2020-03 SemiNAS

1. 对arch角度的Semi
  * Predictor训练的时候相当于只用到了少量的Label数据(Evaluation可以看作一个Label的过程)而大量的SS中Unlabel的数据并没有被利用起来
  * 还可以付诸到ActiveLearning，挑选一些比较Hard的样本来做Evaluation以提高效率
  * 经过一系列的Survey之后我们发现 - 主流的处理方法(SemiSupervised&ActiveLearning)
    * 都是建立在一个分类问题去Push Back Decision Boundary的前提上去做的
    * 基础都是分类，而NAS更是一个回归(或者是Rank)
    * 而且输入图片的空间(高维度)与输入Arch的空间上有一定差异性
  * 可不可以把MoCo的想法结合起来(主要这个Dictionary Learning的问题……看上去好像和NAS不是很搭)
  * **最大的问题是图这种结构本身是否符合Semi有意义的**
    * 或者说是Semi这个到底好在哪里？是利用的input数据当中的一些相关性，还是说Embedding空间本身更合理
      * 我觉得如果说是原本的那些TrainingTrick，并没有对embedding空间有一个多大的改变
      * 但是GAN或者是新的Unsupervised的一些方法是能够做到去获得一个更合理的Embedding空间的？
      * 甚至我们的问题Setting不是要把每一类分好，只是需要一个较好的簇就可以了
  * 不过这种Encoder做Momentum的操作倒是可能可以用在MultiStage的GATES当中（不知道是不是只能够Smooth训练还是有具体作用）

2.1 通过NAS去搜到一个对Semi更有效的架构
  * 类似RobustnessNAS / FewShotNAS等套起来的方法
  * 听起来比较naive没有太大的创新点，而且主观的想用处不会特大

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200326201819.png)

2.2 在数据(而不是Architecture)是Semi的情况下完成NAS
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200326113947.png)
  * Motivation: 目前完成NAS算法需要完全的数据来做训练，当我的数据集还未完全形成的时候，如果我希望拿到一个比较好的架构来做后期的训练(比如说训练是在线做的，Label的数据是在线来的时候，我就需要预先通过半监督的方式来获得一个很好的架构)
  * 实际上因为NAS对架构是一个Rank的过程，是否不需要完全的数据集，也可以完成训练(Evaluation)获得不同arch之间Arch的相对关系
  * (目前常用的NAS算法(Shared Weights/Predictor)的Evaluation相关性都相对较低，而Semi直观看来也会降低一定的相关性，是否能够容忍，或者加以设计以保证相关性不会太低。
  * TrainingTricks - 可以协助训练，但是不能作为衡量的标准
  * WorkFlow：
    * (主要对Shared Weights做)
  * 找到了一支用Self-Supervised的方法，其中DataEfficientSemiwithCPC
    * 这一支已经走到了Representation Learning一支的深处……开始很尼玛玄学了
    * 这支现在看上去还是比较新的，而老的一些基于GAN的Representative的方法在2016年之后就已经比较少见了
  * 对于基于GAN的方法，搜Encoder本身(本质上是一个CNN)是可以，没有什么问题
  * 后面接的那个Classifier可以直接换成Rank-based?（会带来什么好处嘛）？
  * 对于Unsupervised等领域，其实是找到了一个更好的Representation，是否可以我不需要去找最好的representation我只需要找到能对Rank有最大区分度的Representation就可以了(听上去好像不是太靠谱)，是否可以把这个思想融入到设计中

  * 对于MultiObj
    * 有一个Obj它的分辨率很低(代表着我们实际Valid的)，还有一个Obj的分辨率相对高，对应着Unlabel上的，但是会相对间接; 如何处理相对关系
    * 比如在一个上高出一定阈值，就可以不用看Unsupervised了，如果确实是可比的
    * 设置一个Reward的Threshold，如果小于Threshold的时候，我们才更加倾向Unsupervised
    * 在训练RL Controller的时候，基于Ranking的RL方法(不一定是RL)，当保序的时候希望把它拉大 - 感觉这个点上Semi就不是很关键了
    * 还可能有controller不训练的(SA/EA)，感觉就费劲了

---

* 发现了2003-Kaming-[UnNAS]()，这个核心观点已经被人做了，所以说需要找其他的method，follow这个东西


# 2020-04 Surgery

* 在于分析Evaluator
  * shared-weights + Predictor能不能合理
    * Shared-Weights的本身就是一个Predictor，为何要再训练一次作为predictor
    * 方向： 
      1. One-Shot在整个空间的相关性低，但是One-Shot在某个部分会做的更好(共享一份Weights的相关性)，剩下的用Predictor来搞，对整个空间从整个subset来插值
      2. One-Shot的Signal相对比较Noisy，Predictor可以做到Denoise，给Predictor加Prior来降噪


# 2020-04 Online learning

* 主要的故事
  * 在端上做训练User/Scene Customization，对更小的基于Pre-trained Model进行Finetune，做到更好性能以及精简模型
* 算法上的可能方向：
  * 对于同一个Pretrained Backbone网络，对于新的Sub-Task/Domain/Scene，设计一种高效的Finetune，同时精简模型的方式
  * 既然已经有了一个能用的Pretrained Model，是否可以直接用其来推断，做FewLabel的Tuning
* 对于硬件来说的Contribution
  * 可能可以从定点方法本身有一些改进，比如对BN的处理
  * 能把Fine-Tunning的过程部署到硬件平台上
  * 同一个Pre-train模型，对不同用户给不同的Mask，随着使用过程精简计算(硬件加速器可以通过指令重配置完成精简)

* 《DAC19-Personalization》
* 《Fixed Point Training》