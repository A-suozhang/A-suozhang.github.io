---
layout:     post                    # 使用的布局（不需要改）
title:      定点量化训练综述              # 标题 
subtitle:   其实是7月份做的...       #副标题
date:       2019-09-13              # 时间
author:     tianchen                      # 作者
header-img:  img/diffusion/scenary-1.png  #这篇文章标题背景图片
catalog: false                       # 是否归档
tags:                               #标签
    - DL
    - 综述
---

# Survey Of Quantize In Training

## BinaryConnect: Training Deep Neural Networks with binary weights during propagations
* Properties
    * Yoshua Bengio
    * Cite: 890 
    * Year 2015
    * [link](http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-b)

### 贡献概述
* 领域的开山之作
* 首先提出了将网络的参数量化为[-1,1]
    * 将离散化(量化)看为引入随机噪声,和SGD同理,可以被cancel out
    * Dropout/DropConnect与Binary Connect类似,noise could be beneficial
    * BinaryConect可以被认为是一个Regularizer

### 技术细节
* 量化方式
    (Stochastic Determination)不是简单的Sign函数(Sign函数可导性质差),而是一个"hard sigmoid(y = (x+1)/2)",在[-1,1]中的浮点形式weight以一定的几率被转化为-1/1
        
* 实现方式
    * 实际训练: 
        * (这一类训练的模式最是由Hinton提出来的,叫STE(Straight Through Estimator),将二值参数的梯度作为对应浮点参数的梯度(训练之中浮点参数的值会暂时保留,为了能让SGD正常Work))
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913143520.png)
        * 前向传播时,对梯度做二值化处理,BackProp的梯度是是由经过二值化之后的计算结果反传而来的(参与反传的是二值化的参数),但更新的是浮点形式的参数(Weight)
        * 在更新浮点形式的梯度时,做一个[-1,1]的clip,防止参数数值无限增长
        
### 实验结果
* 数据集: Mnist, Cifar, SVNH
* 实验细节(采用了SVM作为final layer)
* 对测试采用了两种方式: 1. 直接采用二值化进行推断测试(图中det)  2. 只在训练中采取二值化,实际测试还是高精度weight  (2稍优于1,显然)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913142846.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913142936.png)
    * 能获得与正常网路Compatible的性能(差距不明显)

### 评述
* 领域的开山之作,首先提出了将网络的参数量化,并阐述了量化的随机误差可以被抵消
* 提出量化可以看做是一种Regularization
* 测试数据集相对较小(Mnist, SVNH, Cifar)


## Binarized Neural Networks
* Properties
    * Yoshua Bengio
    * Cite: 438
    * Year 2016
    * [link](http://papers.nips.cc/paper/6573-binarized-neural-networks)

### 贡献概述
* 将网络的weight和activation都做二值化,并提出了一套可行的训练方案(在比如BatchNorm(shift-based)以及Adam(shift-based AdaMAX)等地方也做了对应的优化)

### 技术细节
* 量化的方式与上文相同,都是采用hard tan做soft determination
    * 对weight进行二值化,同时对activation(原先是浮点的)进行二值化
    * 等效为在前向传播采用sign函数,而在反向传播的时候采用hardtan函数(文章里面叫Saturated STE(饱和的STE))
    * 由于hardtan的特性,当梯度值超过一定值的时候,会取消梯度传递
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913143948.png)
* 训练算法:
    * Main Scheme
    ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913144014.png)
    * Shift-Based BN
    ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913144045.png)
    * Shift-Based AdaMax
    ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913144140.png)


### 实验结果
* 结果还是和之前那篇差不多，能达到全精度网络的水平（但是都是基于小数据集，对于大数据集不太Work，后一篇文章会讲到）

### 评述
* 基于上一篇文章进行了一些修改,将weight和activation都二值化 

---

## XnorNet
* Properties
    * Washintong Univ.
    * Year 2016

### 贡献概述
* 提出了两种网络
    * Binarized CNN: 采用了一种不同的二值化方法，对weight进行了二值化
    * XnorNet： 对weight和activation都进行了二值化
    * 文章核心： **Scaling Factor** based Quantization

### 技术细节
* 采用Scaling Factor作为补偿因子(a)是所有参数的平均值
* 另外的Binarize直接采用sign函数，实际的w = a*b
* 在保证了一定的参数绝对值的基础上，可以利用bitconv (由于经过了二值化，乘法转化为xor & bitcount)
    * ![](xornet/pic0.png)
* 训练过程中,仍然采取全精度的参数进行更新(代表着实际上对BackProp并没有加速)
    * 对于sign函数不可导的问题,为它assign了一种新的导数

### 实验结果
* Binarized网络第一次在大规模数据集(ImageNet)上进行测试并获得好效果
    * 对比了之前的BNN和BinaryConnect,Top1 Acuuracy超16个点
    * 利用AlexNet获得了与全精度AlexNet相compatible的性能
    * 对于更加Compact的网络ResNet和GoogleNet(top1 Accuracy掉10%-20%)

### 评述
* ScalingFactor相对较为普适,可以用在weight,activation甚至gradient(没有测试)
* scaling factor使得量化既保持了绝对值,又能够使用bitconv进行简化conv(但是求平均需要在训练时候频繁调用)

## DoReFaNet
* Properties
    * MegVII
    * 2017

### 贡献概述
* 主要提出了训练过程中的低比特量化(6)
* 测试了w,a,g量化位数对计算复杂度,模型大小对精度的影响
* 对BitConv做了N位的拓展
* 炫技很多...

### 技术细节
* (...量化时候用了很多Trick)

### 实验结果
* 有不同量化位宽之间的对比
    ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913144323.png)
* 大小数据集上都Work

---

## Training And Inference With Integers in Deep Neural Networks
* Proporties
    * 清华大学
    * 2018

### 贡献概述
* 全integer训练推断方案 -   Bidirectional low-precision integer dataflow
* 对WAGE都能够量化
* 新的量化方式: Linear Mapping + Dynamic Shift + Stochastic Determinatoin 
* 更适合定点数的训练方式

### 技术细节
* 新的量化方式
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913144353.png)
* WAGE(W - ternary, AG - 8bit, E - Heuristic设定)
* 训练时候
    * 不采用Adam, Adagrad等方法(需要存多个dw,增加存储开销)
    * 不采用L2norm,量化已经可以认为是一种正则化
    * 不采用交叉熵(指数运算复杂)用平方误差函数





