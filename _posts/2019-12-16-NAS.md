---
layout:     post                    # 使用的布局（不需要改）
title:      Neural Architecture Search尝试理解      # 标题 
subtitle:   (...炼出一个丹方子)  #副标题
date:       2020-02-03           # 时间
author:     tianchen                      # 作者
header-img:  img/11_30/bg-ucentre.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 综述
---


# NAS

> 本文的宗旨是对NAS有一个感性的认知，对具体算法的细节可以慢慢补充

* 组成部分
  * 搜索空间 Search Space
  * 策略 Action/Decision
    * 我们需要一个Controller去从Search Space中采样出优秀的架构
  * 策略评估 
    * 需要一个Evaluator来评估获得的架构效果如何，我们希望它**尽量快**
  * *迭代Decision和Evaluation直到得出最后结果*

* 搜索空间： 包含了对NN的结构定义（有多少哪些层和他们的类型 / 层内的超参数(NxN卷积) / 前驱节点和后续节点直接链接关系(add/Concat)
  * 对搜索空间进行限制以提升速度 （切分为基本单元Cell或者Block）
  * 由于搜索空间是**离散的**，且搜索空间很大，无法得到一个显示的优化函数，所以是一个**黑盒问题**
* 搜索策略(做出决策) - 本质上是一个超参数优化的问题
  * 随机搜索
  * 贝叶斯搜索
  * RL
    * Agent(Controller)完成对网络的结构设计，输出的(Action)是网络结构，Reward是性能最优化
    * 由于结构参数长度不固定，可以用RNN来建模
    * *Exmaple: NASNet:预测出基本Block，将预测的Block分为普通(不改变输出结构)以及简约(减少输出尺寸)*
    * *Example：ENAS: Shared Weights来加速*
  * 遗传算法 (Mutation-Based)
    * 将网络结构编码成二进制串(首先分为几个Stage，再在下面分为Node)，首先随机若干个初始解，训练所有子网络，计算适应值，随机选择结构交叉，产生后代
    * 在初始化/样本选择/交叉/变异等多个子方向都有改进空间
  * 基于梯度的方法
    * 将离散问题连续化，更好的利用**梯度信息**
    * 做松弛化，将层之间的链接从0-1松弛为一定概率
    * *Exmaple：DARTS，将网络单元表示为有向无环图，可以在搜索架构的同时获得结构参数*
* 策略评估
  * 核心目标：减少训练轮数
  * *Example：*
    * 在一个小的集（比如降采样的图像）上进行训练
    * 粗训，并排序(有错误可能)
    * 对结果进行预测(外推Extrapolation)
    * Weight-Sharing，避免TrainFromScratch


# Paper Reading

1. [NAS with RL](https://arxiv.org/pdf/1611.01578.pdf) - Google Brain
  * 真的很早，算是开端了
  * 比cifar10上的baseline提了0.1个点，并且快了一丢丢1.05，以及其他的dataset-PennTreebank以及language modeling的task
  * 建立在认为NN的connectivity和structure可以被表达成一个variable-length string（可用RNN做embedding，也就是作为controller）
    * 将采样（RNN生成）出来的子架构进行训练，将Acc作为reward信号，通过Policy Gradient的方式回传去update controller（Original RNN）
  * 在relatedwork中讲到
    * hyper-param optimization只能够做到在fixed-length的space进行模型优化，且对good initial model比较依赖
    * Bayesian方法可以寻找一个“不定长”的架构，但是不是很有意义
  * The controller in Neural Architecture Search is auto-regressive, which means it predicts hyperpa- rameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning. 
  * search space
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203201724.png)
  * RNN
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203202126.png)
    * 将RNN看作一个树的结构
  * 不是这是不是语法错误…
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203202827.png)

2. [Accelerating NAS using performance prediction](https://arxiv.org/pdf/1705.10823.pdf)
  * 核心在于**提出predictor来加速evaluation**
    * 还搭了一个early stopping
  * 认为human是从training curve来观察的，本文parameterize了这个过程，训练regression model来预测acc
  * 对应的counterpart是Bayesian的一些方法
    * 目测是人工设计一些拟合learning curve的base function，然后用expensive的MCMC来拟合
    * 也有用高斯过程核函数的
  * 说白了建模的是Training Curve
    * 输出val acc，输入是configuration，在每个time step
    * 做了一个SRM（Sequential Regression model）比如RBM，RandomForest等
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203204919.png)

3. [Efficient Architecture Search by Network Transformation](https://arxiv.org/pdf/1707.04873.pdf)
  * **Reusing Weight(Weight Manager)不需要from scratch来训练网络**

  




##　Baselines


# AW_NAS

* ROOT下的base.py定义了MetaClass Components供其他类实现
  * Component包含了:
    * 有Logger
    * 有Set/Get State
    * 以及config
* ROOT下的Plugin.py ❓ 管理诸如controller/dataset/evaluator取什么组件的,或者说是用来Manage各个Module的?


## Search Space

> 实现了一些具体的SearchSpace

* common.py 中包括了对SearchSpace相关的一些定义
  * SearchSpace <== CellSearchpace <== CNN/RNNSearchSpace
    * 提供了init和一些get以及plot方法
  * 包含了CNN和RNN的
  * (没有实现Hierarchy)

## Datasets

> 从Dataset中取数,实现了一个DataSet类

* BaseDataSet <== Cifar/ImageNet
  * 实现了一些split/get_name等方法
* ❓:PTB.py - 看上去是维护了一些语料库?(为什么要这么封装)

## Rollout

> 含义是NetworkStructure的一个编码,是Conreoller/WeightManager/Trainer都需要的一个InterfaceClass,跨组件之间交流的对象

* *其中__init__.py内调用了Utils当中的expect方法,列举了每个文件中应该包含的模块*
* BaseRollout <== Rollout / DifferentiableRollout -- 描述的是采样出来结构的信息
  * ❓ 一个Rollout和其SearchSapce相关(合理),但是内部为何会有RandomSample方法呢?
  * 一个Rollout包含了
    * 属性
      * arch
      * info
      * SearchSpace
      * candidate_net
    * 方法
      * random_sample_arch
      * 一些plot和get方法
      * -------- Differential 特有的 ---------------
        * discrete_arch_and_prob
          * 调用了parse:得到最终的DiscreteRollout
          * *@Propoerty装饰器将类方法伪装为类属性(只有一个self参数),产生的原因是为了保持一个属性getter和setter方法的精简(如果不用getter/setter又暴露在外面)*
  * BaseRollout中有一些抽象方法等待子类去继承
* Dense.py内比较特殊,定义了Dense的SearchSapce以及Dense的Rollout(继承了Rollout而不是BaseRollout)
* MNasNet同理
  * ❓ 为啥有个OFA的...
* Mutation.py 包含了一系列与Mutation-Based相关的组件
  * objetc <== Population (物理意义上是SearchSpace的子集,Mutation中产生的模型集合)
  * CellMutation
  * BaseRollout <== MutationRollout

* *repr*方法显示Obj的属性 (~~那我为啥还在用vars?~~)

## Controller

* 负责采样(Sample)出一个Neural Architecture(网络架构)
* Component <=== BaseController
  * 属性
    * SearchSpace
    * RolloutType
    * Mode
  * 方法
    * SetMode
    * Sample(Sample a Rollout From Population(SearchSpace))
    * Step(Upadte Controller)
    * Summary / Save / Loads
* Population.py(❓不是说没有实现吗?)
  * *包含了Mutation-Based的Agent训练方法*
  * ❓文档中说包含了Bayesian方法，是否是RandomSamplerttr
    * Component <== BaseMutaionSampler <== RandomMutationSampler
      * 内部重要的是sample_mutation方法
    * BaseController <== Population Controller
      * 有一个mutation_Sampler的属性(BaseSampler)
      * ScoreFunction相关
      * ChooseParents
* dense_controller(Unfinished)
* BaseController/nn.Module <== DiffController
  * 属性
    * use_prob,是直接使用Probabilty还是relax Sampling
    * gumble_hard:对RelaxedVector前向使用GumbleSoftmax
    * gumble_Temperature
    * _init_nodes / edge_list
  * 方法
    * forward - 执行sample
    * Calc Gradient &　Choose Loss
    * Summary
* rl_networks.py
  * ❓ SCEDULABLE_ATTR 表示?
  * Components <== BaseRlController <== BaseLSTM <== AnchorControlNet /　EmbeddedControlNet
    * 属性
      * 一些描述LSTM大小的参数
    * 方法
      * forward - Sample
* rl_agent.py 
  * Component <== BaseRLAgent
    * 属性 
      * Controller
    * 方法
      * Step
  * BaseRLAgent <== PGAgent (Policy Gradient)
    * 方法
      * step / _step 方法的区别 ❓
  * BaseRLAgent <== PPOAgent (Proximal Policy Optimization)
* rl.py
  * BaseRLAgent / nn.Module <== RLController
    * 属性
      * SearchSapce
      * RolloutType
      * ControllerNetwork
      * RLAgent
      * Mode (Train/Eval)
    * 方法
      * step - 计算loss
      * Sample - 获取Rollout

## WeightManager

> 与Evaluator联系，因为TrainFromScratch太慢了，所以需要一定程度上ShareWeights或者以其他方式利用之前的Weights

* base.py
  * Component <== BaseWeightManager
    * 属性
      * SearchSpace
      * RolloutType
    * 方法
      * Assemble_Candidate(via Rollout)
      * Step - Update Weight Manager Accoding to Gradients
  * nn.Module <== CandidateNet
    * Different Forward Methods
    * Get Gradient
    * Train/Eval　Queue
* share.py - ****
  * BaseWeightManager / nn.Module <== SharedNet
    * ❓ use_stem?
  * SharedCell <== nn.Module
  * SharedOp <== nn.Module
* SuperNet.py
  * CandidateNetwork <== SubCandidateNet
  * SharedNet <== SuperNet  - (Cell—Based SuperNet)
* DiffSuperNet.py
  * CandidateNet <== DiffSubCandidateNet
* dense.py
  * BaseWeightManager <== DenseMorphismWeightManager
    * 属性
      * NoiseType
    * 方法
      * Assemble Rollout
      * add noise
      * widen
      * Deepen



---

## Trainer

> 用来调和(Orchestra)整个搜索过程

* *GenoType ❓*

* async.py
  * 多进程相关
* base.py
  * Component <== BaseTrainer
    * 属性
      * Cotroller
      * Evaluator
      * RolloutType
    * 方法
      * Setup
* simple.py
  * BaseTrainer <==  SimplerTrainer
    * _evaluator_update
    * _controller_update
    * _backward_rollout_to_controller
    * Train
    * Test
    * derive

## Evaluator

> 用来测试Sample出来的Rollout（或者说是CandidateNetwork❓这两者是不是共同，或者说CandidateNetwork在什么时候会选择）

* base.py
  * Component <== BaseEvaluator
    * 方法
      * Evaluate_Rollout
      * Upadte Rollout / update evaluator
* mepa.py
* ❓ MEPA是啥？
  * Component/nn.Module <== LearnableLROutPlaceSGD
  * BaseEvaluator <==  MepaEvaluator
* tune.py
  * BaseEvaluator <== TuneEvaluator

## OPs

> 其中列举了一些NN Operator

* Baseline_Ops.py
  * 一些常见网络的Block，比如VGG/MobileNet等
* OPs.py
  * 一些自定义的Block和NN组件 
    * 类似Conv-BN-Relu

## Objectives

> 指定不同的任务的

* *❓Perfs的全程？这个模块的意义？描述了什么东西，以及在社么场景下使用(这些问题概括下来好像就是我完全没懂这个是干啥的)*


## Utils


## Final