---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      Paper Wrting å¿ƒå¾— & Digest           # æ ‡é¢˜ 
subtitle:   é«˜è€ƒè‹±è¯­ä½œæ–‡éƒ½è¢«è¯´å£è¯­åŒ–çš„äººæ€ä¹ˆå†™æ–‡ç«     #å‰¯æ ‡é¢˜
date:       2020-04-21            # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/1_28/nantong-3.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - å¸¸ç”¨
     - é‡æ–°å­¦ä¹ 
---


# LaTex

> Linuxä¸‹å®‰è£…```sudo apt-get install texlive-full cjk-latex latex-cjk-chinese```

### ä½¿ç”¨å§¿åŠ¿


* ä¸€èˆ¬flowæ˜¯xelatex-bibtex-xelatex
	* ç”¨xelatexæ¥åˆ›å»º(pdflatexå‡ ä¹åŒç­‰)
	* ç”¨bibtex xx.auxæ¥å»ºç«‹ref
* æ–‡ä»¶æ„ä¹‰
	* .texæ˜¯ä¸»è¦çš„ä»£ç æ–‡ä»¶
	* .bibå†…éƒ¨åŒ…å«äº†ä»google scholarç­‰åœ°æ–¹è·å–çš„å¼•ç”¨
	* .auxé¦–æ¬¡ç¼–è¯‘çš„æ—¶å€™äº§ç”Ÿ
		* .blgä¹Ÿæ˜¯ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶
	* .bbl bibtexæ‰€è¾“å‡ºçš„bib
	* ä¸€äº›æ¨¡æ¿ä¼šåŒ…å«æ¯”å¦‚è¯´
		* ruler.sty
		* splncs04.bst
		* llncs.cls

* æœ‰çš„æ—¶å€™ä¼šå› ä¸ºpdfæ–‡ä»¶è¢«å ç”¨è€Œå¯¼è‡´å¤±è´¥ï¼Œå¯¼è‡´äº†bibç‚¸äº†


### Table 

* basic template


> [overleaf-guide](https://no.overleaf.com/learn/latex/Tables)

```
\begin{table*}[hb]   % hb means here-bottom
\centering
\caption{head of the table}
\label{tab: some_table}    % for reference \cite{tab:xx}
\begin{tabular}{c|c|c|c}   % denote the vlines
	something & something & something & something \\
	...
\end{tabular}
\end{table}
```

* use \hline \vline \cline{1-2} if lines are needed
* é»˜è®¤æƒ…å†µçš„vlineåªæœ‰åœ¨å¯¹åº”ä½ç½®çš„åœ°æ–¹æœ‰ä¸œè¥¿çš„æ—¶å€™æ‰ä¼šå‡ºç°
	* æ‰€ä»¥æœ‰çš„æ—¶å€™è¦å†™ç©ºçš„ ```& & & & \\```
* for multi-row - use ```\userpackage{multirow}```

```
\multirow{2}{*}{some word} A1 & B1 & C1 \\
						   A2 & B2 & C2 \\
\hline
```


### Formula

* ç”¨\begin{equation} \end{equation} - è®²é“ç†å®ƒä¼šè‡ªå·±æ ‡ä¸Šåºå·
* åœ¨å…¬å¼å‰ååŠ ä¸Š\begin{split} \end{split} ç”¨ä½œåˆ†å‰²
* å¯¹äºä¸¤æ¡å…¬å¼çš„å¯¹é½ï¼Œæ‰‹åŠ¨åœ¨å†…éƒ¨åŠ ä¸Š & - å…¬å¼é‡Œçš„&çš„ä½ç½®ä¼šè‡ªåŠ¨å¯¹é½
* å¯¹äºs.t.è¿™æ ·çš„ä¸œè¥¿éœ€è¦ç»™å®ƒåŠ ä¸Š\mbox{}ï¼Œ å¹¶ä¸”ä¿æŒä¸€ä¸ªç©ºæ ¼

---

> æ‰“å…¬å¼å¸¸ç”¨çš„ä¸€äº›ç¬¦å·çš„cheat sheet,[æ›´å®Œæ•´](https://www.rpi.edu/dept/arc/training/latex/LaTeX_symbols.pdf)

* `\int` - ç§¯åˆ†
* `\l(g)eq` - å¤§äºç­‰äºå·
* `\cdot` - ç‚¹ä¹˜
* `\times` - å‰ä¹˜
* `\infty` - æ— é™ç¬¦å·

### checkmark

```
usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
```

### Tips

* å¯¹ä¸€ä¸ªsectionç”¨```\label{sec:method-grouping}```æ¥æŒ‡å®šï¼Œåœ¨åˆ«çš„åœ°æ–¹ç”¨```~\ref{sec:method-grouping}```
* åœ¨citeå‰é¢åŠ ä¸€ä¸ª```~```è¡¨ç¤ºå°äºä¸€ä¸ªç©ºæ ¼çš„ç©ºé—´
* æ’å›¾ç‰‡ä¸€èˆ¬é»˜è®¤[ht]ï¼Œhè¡¨ç¤ºæ˜¯å½“å‰ä½ç½®ï¼Œtè¡¨ç¤ºåœ¨æœ€ä¸Šé¢ï¼ˆæŒ‡çš„æ˜¯ä¸‹ä¸€é¡µçš„æœ€ä¸Šé¢ï¼‰
* è¡¨æ ¼ä¸­cmidrï¼Œè¡¨ç¤ºåŠ ä¸€éƒ¨åˆ†æ¨ªçº¿ \cmidrule(lr){1-2}
  * \hlineæ˜¯ä¸€æ•´æ¡çº¿
* [åˆå¹¶è¡¨æ ¼](https://blog.csdn.net/sinat_36301420/article/details/79334767)
* ```\multirow{2}{*}{$n_i$}``` å½“è¡¨æ ¼åŒä¸€è¡Œçš„ä¸œè¥¿å çš„åˆ—æ•°ç›®ä¸ä¸€æ ·çš„æ—¶å€™
* [å¸¸è§é”™è¯¯](https://www.weibo.com/ttarticle/p/show?id=2309403955741387052924)
* [Tableç›¸å…³](https://zhuanlan.zhihu.com/p/19749566)
* Fig. ï¼ˆå¤§å†™ï¼ŒåŠ ç‚¹ï¼‰
* ç‰¹æœ‰åè¯ï¼Œæ¯”å¦‚GPUï¼Œä»¥åŠæ–¹æ³•AMCéœ€è¦å¤§å†™
* ä¸¤å¼ å›¾å¹¶åˆ—(æ³¨æ„è¿™ç§æ–¹å¼ä¸¤å¼ å›¾æ ‡å·æ˜¯a/bï¼Œä½†æ˜¯å¦‚æœç”¨```\begin{subfigure}```çš„è¯æ ‡æ³¨å°±æ˜¯æ–°çš„fig)
* è°ƒæ•´å›¾ç‰‡æ ¼å¼å®åœ¨æ˜¯å¤ªç—›è‹¦äº†ï¼å¾ˆå®¹æ˜“ä¸€å¼ å›¾å°±ä¼šè¢«å¡åˆ°æœ€åå»ï¼Œæ•´ä¸ªæ ¼å¼éƒ½ä¼šå´©æºƒ
	* ç›®å‰å‘ç°å¯ä»¥ç”¨```[H]```å¤§å†™çš„Hæ¥å¼ºåˆ¶å°†å…¶é”åœ¨Hereï¼
* \footnote{} è„šæ³¨
* è¡¨ç¤ºæ˜Ÿå·è¦ç”¨```$^*$```
* å¼•ç”¨é“¾æ¥  ```\href{http://www.sharelatex.com}{Something Linky}```
* aliasçš„æ–¹å¼ ```\newcommand{\method}{BARS\xspace}```
	* xspaceæ˜¯è¡¨ç¤ºåœ¨éœ€è¦çš„æ—¶å€™åŠ ç©ºæ ¼

\begin{figure*}[th]
  % include first image
  \subfigure[]{
    \includegraphics[width=0.45\linewidth]{eccv2020kit/figs/layer_nasbench101.pdf} 
    \label{fig:layers_nb101}
  }
  \subfigure[]{
    \includegraphics[width=0.45\linewidth]{eccv2020kit/figs/layer_nasbench201.pdf} 
    \label{fig:layers_nb201}
  }
\caption{The effect of the number of GCN or GATES layers. (a) Experiments on NAS-Bench-101. The proportion of training samples is 0.1\% (381 training architectures, 42362 testing architectures). (b) Experiments on NAS-Bench-201. The proportion of training samples is 10\% (781 training architectures, 7812 testing architectures).}
\label{fig:gates_layers}
\end{figure*}

```

* å½“å†…å®¹å®åœ¨é•¿éœ€è¦å‹ç©ºé—´çš„æ—¶å€™ï¼Œåœ¨figæˆ–è€…tableä¸­åŠ ```\vspace{-10pt}```æ¥å‹ç¼©ç©ºé—´
* åœ¨æ ‡é¢˜ä¸­æ¢è¡Œ \title{something \protect\\ something}
* æœ‰çš„æ—¶å€™ç¼–è¯‘è«åé”™è¯¯ ```File ended while scanning use of ```
	* é¦–å…ˆåˆ é™¤æœ¬æ¬¡ç¼–è¯‘äº§ç”Ÿçš„auxï¼Œbrfï¼Œbblæ–‡ä»¶
	* ç„¶åè®°å¾—è¦å…³é—­pdfè§£å†³æ–‡ä»¶å ç”¨é—®é¢˜

# Digest

### å¸¸ç”¨è¯è¯­åŠSynonymæ•´ç†

* ample è¶³å¤Ÿçš„ï¼Œå¤šçš„
* alleviate ç¼“å’Œ
* asymptotic æ¸è¿›çš„
* excel è¶…è¿‡
* extensive å¹¿æ³›çš„
* subpar æ¬¡ä½³
* paradigm èŒƒä¾‹
* scheme æ–¹å¼
* generic ä¸€èˆ¬
* derive è·å¾—
* compelling å¼•äººç©ç›®çš„
* consolidate å·©å›º
* coincide ä¸€è‡´
* deviation è¯¯å·®
* counteractive æŠµæ¶ˆ
* need-demand-requirement-desire éœ€æ±‚
* derivation æ¨å¯¼
* facilicate ä¿ƒè¿›ï¼Œå¸®åŠ©
* neutralize æŠµæ¶ˆ
* implication å«ä¹‰
* anticipate é¢„è®¡
* contemporary å½“ä»£
* prohibitive ç¦æ­¢çš„(å¾ˆå›°éš¾çš„)

### å¸¸ç”¨çŸ­è¯­

* In light of above
* is based on å¯ä»¥ç”¨
* seamlessly incorporated into / work in parallel with
  * å‰è€…çš„ä¸»è¯­ç¨å¾®å°ä¸€äº›
* seek to do
* be prone to æœ‰å€¾å‘
* put forward
* deem to è®¤ä¸º
* sth. degree (Lr decay degree)
* engage/involve with æ¶‰åŠ ï¼ˆengage with -> involveï¼‰
* in-depth comparison
* ouptut response - è¾“å‡ºçš„æ¿€æ´»å€¼

### å¸¸ç”¨ç¼©å†™

* ```et al.``` - æ‹‰ä¸è¯­ä¸­çš„et aliaï¼Œè¡¨ç¤ºä½œè€…æ—¶å€™çš„çœç•¥
	* etåé¢ä¸åŠ .å› ä¸ºetæœ¬èº«ä¸æ˜¯ç¼©å†™
	* å¦‚æœæ˜¯åœ¨å¥å­æœ€åï¼Œä¸éœ€è¦ä¸¤ä¸ª.
	* å¦‚æœåé¢çš„ä¸æ˜¯å¥å·ï¼Œé‚£åé¢çš„.è¿˜æ˜¯éœ€è¦çš„ 
* ```etc.``` - è¡¨ç¤ºç­‰ç­‰
	* et cetera - ä¸€èˆ¬æ”¾åœ¨ä¸¾ä¾‹çš„æœ€åï¼Œè¡¨ç¤ºå‰é¢çš„ä¾‹å­è¿˜æ²¡æœ‰ä¸¾å®Œ
	* ä¸ä¸Šé¢çš„åŒºåˆ«æ˜¯äººç”¨et alï¼Œæ²¡æœ‰ç”Ÿå‘½çš„ç”¨etc
	* å‰é¢éœ€è¦æœ‰é€—å·ï¼å’Œä¸Šé¢é‚£ä¸ªä¸ä¸€æ ·
* ```e.g.``` - for example
	* exampli gratia
	* ä»£æ›¿for instance, such as
	* å‰é¢æœ‰é€—å·ï¼Œæœ€åæœ‰å¥å·ä»¥åŠé€—å·   
* ```i.e.``` - ä¹Ÿå°±æ˜¯
	* id est
	* ç­‰åŒthat is, in other words
	* ä¹Ÿæ˜¯æœ€åæœ‰ç‚¹å’Œé€—å·

### å¸¸ç”¨è¯­å¥

* To sum up, we make the following contributions:
* The rest of this paper is organized as follows. The related studies are introduced in Sec.~\ref{sec:related}. In Sec.~\ref{sec:methods}, we introduced our fixed-point training. Then in Sec.~\ref{sec:exp}, the effectiveness of our method is illustrated by experiments. We further discuss XXX in Sec.~\ref{sec:discuss}.Finally, we conclude our work in Sec.~\ref{sec:conclusion}.
* ad-hoc - (for this purpose only)

# å¿ƒå¾— (ä»–äººç»éªŒ)

* é€»è¾‘é“¾å®Œå…¨ï¼Œè®©ğŸµéƒ½èƒ½çœ‹æ‡‚
* å¤šæŠ„åˆ«äººå¥å­
* å…ˆå•°å—¦ï¼Œä¸è¦çœä¸»è¯­
* æ¯ä¸€æ®µçš„ç¬¬ä¸€å¥è¯è¦æ€»ï¼ˆæœ€å¥½æ˜¯æ€»åˆ†æ€»ï¼‰
* methodä¸­éœ€è¦ç»™å‡ºchallenge
* ä¸è¦å†™è¿›å»èŠ‚å¤–ç”Ÿæçš„
* å¯¹åº”ï¼Œå›¾é‡Œæœ‰çš„ä¸€å®šè¦å¯¹åº”åˆ°æ–‡å­—
* æ¯ä¸ªæ®µè¿˜æœ‰ä¸€ä¸ªå•è¯ï¼Œç¼©å¥ç¼©æ’ç‰ˆ
* ç”¨è¿å­—ç¬¦æ¥æŠŠæ¯ä¸€è¡Œå¡«æ»¡
* éœ€è¦æ£€æŸ¥çš„
  * åè¯ä¸æ˜¯å¤æ•°å°±è¦åŠ aæˆ–è€…the
  * å¤§å°å†™ï¼
* å›¾è¦æ”¾åœ¨æœ€ä¸Šé¢
* æˆªç¨¿å‰æ³¨æ„å¤‡ä»½overleafï¼ˆæ´»ç”¨overleafçš„historyåŠŸèƒ½ï¼‰
* åœ¨æœ€åç»Ÿä¸€checkä¸€äº›è¡¨è¾¾
* "x"ç”¨```$\times$``` dollarå·ä¸èƒ½å¿˜ä¸ç„¶ä¼šæ ¼å¼å´©å
* æŠŠé‡è¦çš„ä¸œè¥¿å¾€å‰æ”¾ï¼Œç„¶åå†è¯´ä¸ºä»€ä¹ˆ
* æœ‰äº›æ–‡ç« ä¼šç”¨ä¸€ä¸ªå•ç‹¬çš„sectionæ¥è§£é‡Šåè¯(å¯èƒ½å½“é¢†åŸŸæ¯”è¾ƒæ··æ­çš„æ—¶å€™)
  * ä»¥åŠä¸€äº›æ–‡ç« åœ¨expéƒ¨åˆ†ä¼šä¼˜å…ˆå°†å„ä¸ªbaselineä»‹ç»ä¸€å“ˆ

---

### 2020-03-06 ECCV å¿ƒå¾—

* ç°åœ¨æ˜¯2020-03-06-03:25 æ–‡ç« äº¤äº†ï¼Œæ€»ç»“ä¸€ä¸‹å‡ ç‚¹å¿ƒå¾—å§
  * ç°åœ¨è‡ªå·±å†™æ–‡ç« çŸ¥é“è¿™å¥è¯åº”è¯¥å†™ä»€ä¹ˆä½†æ˜¯è¿˜æ˜¯è¡¨è¾¾ä¸å¥½â€¦
  * grammarly check
    * ä¸€èˆ¬æœ€ågrammarlyæŸ¥å‡ºæ¥çš„ï¼š 1. ä¸‰å• 2. the/a 3. æå°‘çš„æ­é…
    * the/a åŠ äº†æ²¡æœ‰check
    * åœ¨æäº¤ä¹‹å‰ç»Ÿä¸€ä¸€ä¸‹ä¸€äº›ä¸œè¥¿çš„ç”¨æ³•
  * marginæ˜¯ä¸ªå°bitchï¼Œæˆ‘è¿˜æ˜¯ä¸ä¼šè°ƒ
  * ç›¸å¯¹minorçš„ç‚¹å¦‚æœä¼šå¼•èµ·è¯¯è§£å»ºè®®çœå»ï¼Œè¿˜ä¸å¤§ä¼šå‡ºç°æ–‡ç« å†™ä¸æ»¡ï¼Œåªæœ‰å‹ä¸ä¸‹æ¥
  * æ—©ç‚¹æ‰¾**æ²¡æœ‰å‚ä¸è¿‡å·¥ä½œçš„ï¼Œæ²¡æœ‰é¢†åŸŸèƒŒæ™¯çš„**åŒå¿—çœ‹æ–‡ç« ï¼Œä¼šæœ‰æ–°çš„å‘ç°(å•Šä½ methodçœ‹ä¸æ‡‚å•Š)
  * ä¸è¦åå•¬å¼ºè°ƒï¼Œæ–‡ç« çš„é‡ç‚¹å°±paraphraseåå¤çš„è¯´
  * æ”¹å›¾çš„ä¸€äº›å¿ƒå¾—
    * å¯¹ç§°æ˜¯å¥½æ–‡æ˜ï¼Œä¸ä»…æ˜¯ä½ç½®å¯¹ç§°ï¼Œè¿˜æœ‰å­—ä½“å­—å·çš„å¯¹åº”
    * é…è‰²ï¼Œè¿™ä¸ªå€’æ˜¯å¯ä»¥å±•å¼€è®²ï¼Œç›®å‰æˆ‘çš„ä½“ä¼šæ˜¯ï¼Œé¥±å’Œåº¦æ‹‰ä½ï¼Œè‰²ç³»å€’æ˜¯å¯ä»¥ç¨å¾®è·¨è¶Šä¸€äº›
    * å­—ä½“å¤§å°æ”¾å¤§ï¼
    * å®é™…æ”¾åˆ°æ–‡ä¸­çš„å›¾é‡Œçš„å­—å¤§å°ï¼Œå’Œppté‡Œå­—å¤§å°ä¸å®Œå…¨ç›¸å…³è”ï¼Œæœ‰æ—¶å€™éœ€è¦æŠŠå›¾å‹æ‰æˆ–è€…æ‹‰é•¿æ‰æ˜¯çœŸæ­£å…³é”®çš„ï¼Œå› ä¸ºè®ºæ–‡æ ¼å¼é‡Œé¡µå®½å°±è¿™ä¹ˆå¤š


### 2020-05-01 æŒ‚arxiv

* æ³¨æ„arxivéœ€è¦çš„æ˜¯latex source code
* åœ¨æ–‡ä»¶çš„ä¸€å¼€å§‹éœ€è¦æŒ‡å®š```\pdfoutput=1```
	* å³ä½¿æ˜¯åœ¨texç¼–è¯‘çš„æ—¶å€™æ˜¾ç¤ºerrorä¹Ÿæ˜¯æŒ‰ç…§è¿™ä¸ªè§„åˆ™æ¥åš
* æ‰“åŒ…æˆä¸€ä¸ªå‹ç¼©åŒ…åŒ…å«æ‰€æœ‰source codeï¼Œéœ€è¦ä¸å«subfolderï¼Œé‡Œé¢ç›´æ¥æ˜¯å†…å®¹
	* ä¸»texæ–‡ä»¶æ˜¯è¦æ˜¯```ms.tex```
* æ³¨æ„arxivå¹¶ä¸ä¼šç»™ä½ è·‘bibtex
	* æ‰€ä»¥æäº¤çš„æ—¶å€™ä¸€å®šè¦å°†bblæ–‡ä»¶ç»™åŒ…å«äº†ï¼Œå¦åˆ™ä¼šæ²¡æœ‰å¼•ç”¨ä¿¡æ¯ï¼

### 2020-04-20 Rebuttal

* å¯¹äºç»™é«˜åˆ†çš„ï¼Œå…ˆèˆ”ï¼Œè¡¨ç¤ºXXå¾ˆé‡è¦ï¼Œç„¶åè¯´æˆ‘ä»¬è€ƒè™‘äº†XXï¼Œæ²¡åšä»€ä¹ˆæ˜¯ä¸ºä»€ä¹ˆï¼Œè¿˜ä¼šåšä»€ä¹ˆ

### 2020-05-26

* é˜…è¯»äº†[æµ…è°ˆRebuttal](https://zhuanlan.zhihu.com/p/104298923)
* ä½“ä¼šå®¡ç¨¿äººçš„æ„å›¾ï¼Œä»ä»–çš„è§’åº¦å‡ºå‘
  * å®¡ç¨¿äººå–·åªæ˜¯incremental/å‚è€ƒï¼Œè¦æ¾„æ¸…æŸéƒ¨åˆ†çš„ä»·å€¼
  * å¦‚æœæŸäº›è´±äººåªæ‹¿Noveltyæœ‰é™è¯´äº‹ï¼Œè¯´æ˜å®ƒæ‰¾ä¸åˆ°ç¡¬ä¼¤
    * è¿˜æ˜¯åªèƒ½è¯´æ˜åˆ›æ–°ç‚¹åœ¨å“ªé‡Œ,é›†ä¸­è¯´æ˜æˆ‘ä»¬çš„åˆ›æ–°ç‚¹
  * å–·ã€Œfactual errorã€ï¼šå®¡ç¨¿äººä¸€æ—¦æ‰¾å‡ºæ–‡ç« çš„äº‹å®æ€§é”™è¯¯ï¼Œä½œè€…ä¸å¦¨å¤§æ–¹æ‰¿è®¤ï¼Œå¹¶è¡¨ç¤ºæ„Ÿè°¢ï¼ŒåŒæ—¶è¡¨ç¤ºä¼šåœ¨final versionä¸­æ›´æ­£é”™è¯¯ï¼›å¦ä¸€ç§æƒ…å†µæ˜¯ï¼Œå¯èƒ½å°±æ˜¯å› ä¸ºä½œè€…è‡ªå·±æ²¡å†™æ˜ç™½ï¼Œæ‰ä½¿å¾—å®¡ç¨¿äººé”™è¯¯ç†è§£ï¼Œå¦‚æ­¤ï¼Œä¹Ÿå¯å¤§æ–¹æ‰¿è®¤ï¼Œè¯´â€œæˆ‘ä»¬å·²ç»ä¿®æ”¹äº†è¿™éƒ¨åˆ†æè¿°ï¼Œå®é™…ä¸Šæ˜¯è¿™æ ·åšçš„ï¼Œå¹¶ä¸æ˜¯ä½ ç†è§£çš„é‚£æ ·ï¼Œblabla
  * Rebuttalæ—¶è‹¥å‘ç°å®¡ç¨¿äººçš„factual errorï¼Œå¦‚taæå‡ºçš„æŸä¸ªè§‚ç‚¹æœ‰æ˜¾ç„¶é”™è¯¯ã€æå‡ºéœ€è¦å¯¹æ¯”çš„æ•°æ®é›†æ˜¾ç„¶ä¸æ˜¯è¯¥é¢†åŸŸå¸¸ç”¨çš„æ•°æ®ç­‰ï¼Œä½œè€…å¯åœ¨rebuttalå›åº”æ­¤äººæ—¶é¦–å…ˆæŒ‡å‡ºå…¶é”™è¯¯
  * rebuttalæ—¶ä¸è¦æ¼ç‚¹ï¼Œè¦é€ç‚¹å›åº”åšåˆ°æœ‰é—®å¿…ç­”ã€‚è‹¥å› ç¯‡å¹…æœ‰é™ï¼Œå¯å°†ç±»ä¼¼çš„æ„è§åˆæˆä¸€ç‚¹ï¼Œä¸‡ä¸å¯å› ç¯‡å¹…æœ‰é™æ“…è‡ªåˆ é™¤ä¸€äº›è¦ç‚¹æˆ–é—æ¼è¦ç‚¹ï¼Œä»¥å…é€ æˆå«ç³Šä¸æ¸…ã€æµ‘æ°´æ‘¸é±¼ä¹‹å«Œ
* **è¦æœ‰é—®å¿…ç­”**

### 2020-06-20

* Review Trans Paper
* æœ‰ä»€ä¹ˆä¸æ‡‚çš„å°±åŠ Question
  * ç»™Major Revision / Minor Revision 
* Template

```
Decision: Major revision

Comments:
The authors argue that imposing uniform regularizations on filters during structured pruning is problematic. And they propose to modify the existing regularization-based structured pruning methods, by weighting the regularization of different filters based on the saliency scores. The saliency score is calculated as ||grad x scale||^2 / FLOPs reduction.

Strength:
The accuracy results on CIFAR-10 and ImageNet are impressive. 

Weakness:
You should analysis the computational efficiency of the overall pruning process. In you abstract, you state that the overhead of calculating the saliency is minimum, however, there is an inner iteration to calculate the saliency scores every epoch during SASL, which might be time-consuming. Also, the post-processing pruning and fine-tuning may be time-consuming. I'm aware that you utilize a hard-mining technique to accelerate the pruning process, and in Ablation 4), you said â€œhard sample mining strategy can not only reduce the complexity overhead for the saliency estimationâ€œ. I would like to see a quantitive study on the computational efficiency of your proposed framework.

The description of the method and experiment settings is not very clear, here are some queries about the method and experimental settings:
- How do you â€œclassifyâ€ the filters into different â€œ hierarchyâ€? Define thresholds, or just uniform distribute them according to the ranking?
- How many iterations are there in the iterative pruning and fine-tuning process? In each iteration, how many filters are discarded?
- In the ablation study 2), how do you change the base regularization value according to the number of classes? Did you experiment with class number larger than 5?
- In SASL* (a.k.a, the aggressive scheme), what hyper-parameters are tuned? The base regularization value in SASL, or only the post-processing iterative pruning process?
- How do you handle skip connections?


Other comments:
- Page 5 line 24, Page 8 line 30 (right column): wrong quotation marks
- I donâ€™t think hierarchy is a very proper terminology for describing different saliency levels.


From my perspective, i suppose using the product of the gradient magnitude and the weight/activation magnitude as the saliency signal is common in early studies, e.g., [1][2]. Thus, the main contribution of this paper is to propose to use the quotient of the commonly used saliency score and the FLOPs reduction as the new saliency score, and then, the saliency scores are used to determine the weighting value for the regularizer.

[1] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, Pruning convolutional neural networks for resource efficient inference, ICLR 2017.
[2] Michael Figurnov , Aijan Ibraimova, Dmitry Vetrov, and Pushmeet Kohli, PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions, NIPS 2016.
```


```
This paper proposes several techniques to improve dense video caption performance, including an event-level feature fusion, a scene-level topic predictor, an NMS procedure that takes both temporal and linguistic similarity into consideration. By combining these techniques, this paper achieves very good results. Also, this paper conducts comprehensive ablation studies on the proposed techniques.

Basically, I think this is a good paper. Although the event-level feature fusion method is simple, the ablation studies have demonstrated its effectiveness. And, I like the idea of using topic model to discover latent topics, and then using the topic distribution of each video as the scene-level training signal. And itâ€™s good to see that the ablation study demonstrates its effectiveness on the evaluation metrics. Is this idea original? 

I have some queries:
- In equation (2), why do you use asymmetric linear embedding layers, W_Q, and W_K, this doesnâ€™t seem like a reasonable distance metric.
- Page3 line39 column right, you say that â€œthe low-dim vector P may not be easily separableâ€. I think separable is not a proper word here, since we are not solving a classification problem.
```

```
TCSVT review: multi-feature adaptive late fusion image retrieval based on cross-entropy normalization


Review confidence level: 4
Technical content:
novelty: 1
Tutorial: 1
Technical correctness: 1
Technical depth: 1
Application: 1

Presentation:
clarity: 1
organization: 1
conciseness: 1
english: 1
references: 2

Decision: Reject

Comments:
This paper proposes to select reference curves for each query image by minimizing the â€œcross-entropyâ€, and then use the calibrated similarity curves to fuse multiple features.

Basically, I think there is a lot to be improved on this paper. 

(1) Writing and Clarity
This paper is poorly written. There are many grammar mistakes, many sentences cannot convey the points, and many terminologies are not used correctly. The authors should check the grammar, and polish the presentation.

(2) Organization and conciseness
      Related work: The summary of the related work is not well organized, the logic chain between subsections is weak.
      Method: The organization of the method section is not good, itâ€™s hard to follow your method description and capture the crucial points, and I must read for the second time to capture what you mean. For an example, the title of III.C â€œEffect Analysis of Cross-Entropy Normalizationâ€ does not fit with the content.

(3) Technical correctness and depth
     I have some doubts about the method. The description in the paper seems to indicates that the S curve for each feature is normalized, and this paper aims to find a reference curve that can calibrate each featureâ€™s curve by fitting its tail. However, It seems from Eq.4 that S_i(u:v) is normalized across different features, maybe the notation is wrong that the summation is not across the index â€œiâ€?  If so, the summation of S_i(u, v) across the u-v segment should be less than 1 instead of equal 1.
     
     This method is purely heuristic and empirical, I donâ€™t think the derivation about the positiveness of the â€œrelative entropyâ€ is suitable in a paper, since itâ€™s a fact that is well known in textbooks. Thus, I would recommend that you remove the two theorems in your paper.

      Why do you use â€œarea under curveâ€ to calculate the fusion weights. The only place that mentions this intuition is the II.D section where you discuss that [17] utilized this intuition. Can you provide more theoretical or intuitive background , or at least, can you experiment with other alternatives to demonstrate the rationality of this weighting scheme. 

     The word â€œcomplementarityâ€ is repeatedly used in the paper, you emphasize that your method of selecting the reference curve can â€œoptimizeâ€ the complementarity. However,  I fail to capture the logic behind this statement. According to your description in III.D, it seems like you think minimizing the â€œcomplementarityâ€ of the reference curve and the current featureâ€™s similarity curve is a way to â€œoptimizeâ€ the complementarity? I suppose that utilizing feature complementarity means that you should choose features that maximally complement each other, and this is not related to the proposed method.

(4) Experiments, Application of the method
     This method might not be so useful in application. The proposed method is an incremental one, and brings extra memory footprints and query latency overhead. 

     In IV.G, you said that â€œweâ€™re not going to make a direct comparison on the query timeâ€. I donâ€™t think the differences in the computerâ€™s performance or the feature number is the reason for not doing the quantitive latency comparison, since all these variables can be controlled. So, you should compare the query time with more baselines.
```


### ECCV Camera Ready

> å¼€å§‹å‡†å¤‡eccv2020çš„camera readyäº†

* éœ€è¦æäº¤çš„å†…å®¹æœ‰
	* source code(latex) + camera readyçš„pdf
	* copyright pdf - éœ€è¦ç­¾å
	* supplmentary material


* gates
	* baselineæ–‡å­—çš„å›¾ç‰‡å’Œæ–‡å­—æ”¹ä¸€ä¸‹


### CVPR2021 -  2020-11-28

* ç¬¬äºŒæ¬¡å®Œæ•´çš„å†™æ–‡ç« ï¼Œæµæ°´è´¦ä¸€ä¸‹
* é¦–å…ˆæ˜¯åœ¨CMTä¸Šæ³¨å†Œè·å¾—paper-id,å å‘
	- è‡ªå·±å†™çš„æ—¶å€™è¯­æ³•é”™è¯¯å¥½å¤šï¼Œè¿˜éœ€è¦é”»ç‚¼ï¼ŒåŒ…æ‹¬æ–‡ç« çš„è¡¨è¾¾æ–¹å¼ï¼Œä»¥åè¯»æ–‡ç« çš„æ—¶å€™è¿˜éœ€è¦å¤šç§¯ç´¯
	- å¼€ç€grammarlyå†™æŠŠ
* å…³äºçœ‹ddlå¯ä»¥åŠ ä¸ªç¾¤æˆ–è€…ç”¨[AI Deadlines](https://aideadlin.es/) - å¯ä»¥ä¸ç”¨æå¤æ‚çš„æ—¶åŒºæ¢ç®—äº†


# Resources

## Docs

* çŸ³å¢¨
* è…¾è®¯
* Google Docs
* Notion

## Writing

* Grammarly
* [Liggle](https://linggle.com/)
* [Synonym](https://www.synonym.com/synonyms/)
* [New Better Synonym](https://www.thesaurus.com/browse/optimize?s=t)
* æœ‰é“ç¿»è¯‘


## Bib
* [ResearchGate](https://www.researchgate.net/?_sg=9D8JkWoEvbx5lKepanzgM0bx2GcheNWitXm5LIwKovHl43ewI72-pQS0vPCyRAmRJA37PppxLEoD)
* [Semantic Scholar](https://www.semanticscholar.org/)


## Plot

* [Color Hex](https://www.color-hex.com/)
* [Matplotlib Doc](https://matplotlib.org/contents.html)
  * [Cmap](https://matplotlib.org/3.1.1/tutorials/colors/colormaps.html)
  * [Marker](https://matplotlib.org/3.1.1/api/markers_api.html#module-matplotlib.markers)
* [Matplotlib Tutorial(Not Official)](https://riptutorial.com/matplotlib/)
* [Mathematcia Doc](https://reference.wolfram.com/language/)
* [è¿™ç¯‡æ–‡ç« å›¾ä¸é”™](https://arxiv.org/pdf/2006.05467.pdf)
