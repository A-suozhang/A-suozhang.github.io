---
layout:     post                    # 使用的布局（不需要改）
title:      Darts开发笔记          # 标题 
subtitle:   Divin into Darts method       #副标题
date:       2048-08-04           # 时间
author:     tianchen                      # 作者
header-img:  img/7_1/njroad.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - private---

# Papers

## []()




# 开发Darts代码的一些细节

### search space

* 8 ops
	* 3x3 5x5 separable conv
	* 3x3 5x5 dilated conv
	* 3x3 max/avergae pooling
	* zero / shortcut
* N=7 nodes
	* output node as depthwise concat
	* cell k's 1st and 2nd node are equal to cell k-1, k-2's output (insert 1x1 conv for channel align)
* Relu-conv-bn 
* separable-conv applied twice
* 1/3 & 2/3 depth of arch is reduction cell
	* stride=2 op nearest to input

### trainer

* split the CIFAR-10 set - 50% train / 50% val
* For weights - SGD - 0.025/0.9/3.e-4
* zero init \alpha
* For alpha - 3.e-4/(0.5,0.999)/1.e-3

# Xie's Talk - Full space NAS

* Darts的问题所在
	* 主要在Bi-level Optimizatoin
		* 很难找到当前架构上的最优参数，假设不成立
	* Discrete Loss会让母网络的精度大量下降
* 替代Bilevel optimization
	* alpha与weight的数量差距很大，jointly training的话会导致优化不平衡(所以需要采用bilevel来分离)
	* 在单阶段中加入正则化去拉 
	* 加入gradual pruning，防止超网络去疯狂生长，让所有的参数都留下来,加一个flops约束来
* 删除不合理的
	* 每条边上只能保持一个操作
	* 每个Cell只能和之前的两个Cell链接
	* Normal以及Reduction Cell必须一样

