---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      An Intro to RL              # æ ‡é¢˜ 
subtitle:   Think Like An Agent    #å‰¯æ ‡é¢˜
date:       2019-09-26                # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/bg-street.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - DL
    - ç»¼è¿°
---
# Intro To RL
* ä¹‹å‰å¯¹RLçš„ç†è§£ä¸€ç›´æ˜¯åªè¨€ç‰‡è¯­ï¼Œæ²¡æœ‰â€œç³»ç»Ÿåœ°å…¥ä¸ªé—¨â€ï¼ˆé›¾ï¼‰ï¼Œä¹‹åæŠŠRLç›¸å…³çš„ä¸€äº›ç¢ç‰‡çŸ¥è¯†éƒ½æ•´ç†åˆ°è¿™ä¸ªposté‡Œé¢æŠŠ

## æ ¸å¿ƒå››å…ƒç»“æ„
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908082919.png)
* å¯¹Agentï¼ˆæ™ºèƒ½ä½“ï¼‰ä»ç¯å¢ƒä¸­å­¦ä¹ çš„è¿‡ç¨‹å»ºæ¨¡    

## åˆ†ç±»è¾¨æ A Simple Taxonomy
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908083740.png)
* Model Free & Model Based ç†ä¸ç†è§£æ‰€å¤„çš„ç¯å¢ƒ(æˆ–è€…è¯´æ˜¯agentæœ‰æ²¡æœ‰access to)
    * ä¸å»å¯¹ç¯å¢ƒå»ºæ¨¡ï¼Œç›´æ¥æ¥å—çœŸå®ä¸–ç•Œçš„åé¦ˆ - **Model-Free**
        * *Q-Learning,Sarsa,Policy Gradients*
    * æ™ºèƒ½ä½“è‡ªå·±å»ºç«‹ä¸€ä¸ªæ¨¡å‹å»æ¨¡æ‹ŸçœŸå®ä¸–ç•Œå¯¹è‡ªå·±è¡Œä¸ºçš„åé¦ˆ **Model-Based**
        * ç›¸æ¯”äºModel-Freeå¤šäº†ä¸€ä¸ªå»ºæ¨¡çš„å·¥å…·ï¼Œå¤šäº†ä¸€ä¸ªæ²™ç›’æ¨¡æ‹Ÿçš„è¿‡ç¨‹
        * ç›¸æ¯”Model-Freeçš„æ–¹æ³•ï¼Œå¤šäº†â€œæƒ³è±¡åŠ›â€å’Œâ€œæ¨æµ‹â€ï¼ŒModel-Freeåªèƒ½ç­‰å¾…çœŸå®ä¸–ç•Œçš„åé¦ˆï¼Œä¸€æ­¥æ­¥è¿›è¡Œï¼›è€ŒModel-Basedçš„æ–¹æ³•å¯ä»¥é¢„æµ‹å‡ºæœªæ¥å‡ æ­¥ï¼Œå¹¶ä¸”é€‰æ‹©ä¸€ä¸ªæ›´åŠ pleasantçš„é¢„æµ‹æ¥æ‰§è¡Œï¼ˆæ€è€ƒä¸€ä¸‹å›´æ£‹ï¼Œçœ‹å‡ æ­¥ï¼‰
    * Model-Based**ä¼˜åŠ¿**ä¸ºç»™ä¸äº†agentæ ¹æ®environmentè€Œå…·æœ‰**Think Ahea**çš„èƒ½åŠ›ï¼ŒğŸ¤”(æ„Ÿè§‰ä½“é‡éƒ½å¾ˆå¤§) -Example*AlphaZero*
    * Model-Based**åŠ£åŠ¿**åœ¨äºè¿™ä¸ªmodelå¾ˆéš¾è®­ç»ƒï¼Œéœ€è¦å®Œå…¨é€šè¿‡experienceæ¥æ¨æµ‹environment
    * äº‹å®è¯æ˜åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹model-freeçš„æ–¹æ³•éƒ½å¯ä»¥ä¸model-basedå¯¹ç­‰çš„æ•ˆæœï¼Œæ‰€ä»¥æ›´ä¸ºæµè¡Œ 
* **Policy-Based & Value-Based** ï¼ˆWhat To Learnï¼‰
    * Policy-Based åŸºäºæ¦‚ç‡çš„æ–¹æ³•ï¼šé€šè¿‡åˆ†ææ‰€å¤„çš„ç¯å¢ƒï¼Œå¯¹æ‰€æœ‰actionï¼ˆå†³ç­–ï¼‰ç»™ä¸€ä¸ªå‘ç”Ÿæ¦‚ç‡
        * **ç›´æ¥å¯¹Policyè¿›è¡Œå‚æ•°åŒ–æè¿°(æ˜¯ä¸€ç§ç‰¹æ®Šçš„æŠ½è±¡)ç›´æ¥å»æ‹Ÿåˆä¼˜åŒ–ä»–**
    * åŸºæœ¬éƒ½æ˜¯On-Policyï¼ˆåªåŸºäºmost recent policyï¼‰
        * Policy Gradient
        * A2Cã€A3C
        * PPO
        * *Learn An Approximator Of $$ V^\pi(s) $$*
    * Value-Based åŸºäºä»·å€¼çš„æ–¹æ³•ï¼Œè¾“å‡ºæ‰€æœ‰actionçš„ä»·å€¼ç„¶åé€‰æ‹©ä¸€ä¸ªæœ€å¥½çš„    
        * Q-Learning,Sarsa
        * *Learn An Approxiamtion Of $$ Q_\theta(s,a) $$*
        * åŸºæœ¬æ˜¯Off-Policyçš„(æ¯ä¸€æ¬¡Updateå¯ä»¥ç”¨åˆ°æ‰€æœ‰data)
    * ä¸¤è€…ç»“åˆ
        * Actor-Critic: ActoråŸºäºPolicyï¼ŒåŸºäºæ¦‚ç‡ç»™å‡ºåŠ¨ä½œï¼Œç„¶åCriticå¯¹æ‰€ä½œå‡ºçš„åŠ¨ä½œç»™å‡ºä»·å€¼ï¼ˆç›¸æ¯”äºå•çº¯çš„Policy GradientåŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼‰
        * DDPG  
    * â“: æœ‰ç‚¹åƒHard/SoftMax?
    * *ä¸¤è€…è¾¨æ*
        1. Policy Bssedæ›´æœ‰ç†è®ºåŸºç¡€ï¼Œå› ä¸ºç›´æ¥ä¼˜åŒ–çš„æ˜¯æˆ‘ä»¬éœ€è¦çš„ä¸œè¥¿ï¼Œè€ŒValueBasedç”±Valueè¿™ä¸€é—´æ¥ä¼˜åŒ–ï¼Œæ‰€ä»¥å¯èƒ½ä¼šè®­ç»ƒå‡ºæ¥åçš„æ¨¡å‹
        2. ä¸¤è€…åœ¨å¾ˆå¤šæ—¶å€™å…¶å®æ˜¯ç­‰ä»·çš„ğŸ˜²åœ¨ä¸¤è€…ä¹‹é—´çš„ä¸€äº›ç®—æ³•å¯ä»¥æ¯”è¾ƒå¥½çš„Trade Offè¿™ä¸¤ç§æ€æƒ³
* å›åˆæ›´æ–°/å•æ­¥æ›´æ–°
    * ä¼ ç»Ÿçš„æ¯”å¦‚Monte-Carlo Leaningï¼ŒPolicy Gradientï¼ˆOLDï¼‰ï¼Œä¸€æ•´ä¸ªå›åˆç»“æŸæ‰ä¼šæ€»ç»“æ›´æ–°
    * Q-Learingï¼ŒSarsaï¼ŒPolicy Gradientï¼ˆNEWï¼‰ï¼Œæ¯ä¸€æ­¥éƒ½æ›´æ–°ï¼Œ*æ•ˆç‡æ›´é«˜*ï¼Œç°åœ¨å¤§éƒ¨åˆ†çš„éƒ½æ˜¯
* On-Policy/Off-Policy
    * åœ¨çº¿ï¼š Sarsa
    * ç¦»çº¿ï¼š QLearning


            

## å…³é”®å®šä¹‰
* RLçš„**æ ¸å¿ƒ**ï¼š
    1. å¦‚ä½•å®šä¹‰é‡‡å–åŠ¨ä½œä¹‹åçš„Rewardçš„è¯„ä»·æ ‡å‡†ï¼ˆrewardçš„è®¾è®¡ä¸ä»…è¦ç¬¦åˆæœ€åæˆ‘ä»¬æ‰€éœ€è¦è¾¾åˆ°çš„ç›®çš„ï¼Œè¿˜éœ€è¦æ³¨æ„åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¸åŒactionçš„æ¯”è¾ƒï¼‰
        * æœ¬è´¨æ˜¯éœ€è¦è®©agentå­¦åˆ°æœ€å¥½çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ
    2. æˆ‘ä»¬å¦‚ä½•æ›´æ–°æ“ä½œ
    > Q-Learningä¸­ï¼Œä¼šæœ‰ä¸€å¼ å¾ˆå¤§çš„Tableï¼Œè®°å½•äº†æ‰€æœ‰stateå¯¹åº”çš„æ‰€æœ‰actionçš„å€¼ï¼Œæˆ‘ä»¬æŒ‰æ ·æœ¬å­¦ä¹ æ¯ä¸ªå•å…ƒæ ¼ï¼Œä½¿æœ€åçš„Rewardæœ€å¤§
    * åŸºäºValueçš„æ–¹æ³• - DQN
        * ä½†æ˜¯ç”±äºè¡¨è¿‡åº¦å¤§ï¼ŒDQNç”¨ä¸€ä¸ªNNæ¥æŸ¥è¡¨
        * DQNé€šè¿‡experience reply(è§£å†³æœºå™¨å­¦ä¹ ç‹¬ç«‹åŒåˆ†å¸ƒçš„é—®é¢˜) â”
        * ç”¨ä¸€ä¸ªç½‘ç»œä¼°è®¡Qå’Œç°å®Qä¹‹å·®ï¼Œä½œä¸ºloss
    * åŸºäºPolicyçš„æ–¹æ³•
        * ç›´æ¥ä¼˜åŒ–Policyç®€å•æ˜å¿«ï¼ˆNNç›´æ¥è¾“å‡ºPolicyï¼‰ï¼Œé—®é¢˜è¢«è½¬åŒ–ä¸ºå¦‚ä½•æ›´æ–°NNå‚æ•°
            * æœ‰stochastic / deterministic
                * Stochastic éœ€è¦é‡‡æ ·ï¼Œè®¡ç®—å¤§  - PPO
                * Deterministic å·²ç»è¯æ˜è¿‡äº†ç­–ç•¥æ¢¯åº¦å…¬å¼ - DPG/DDPG
    * èåˆValueä¸Policy - Actor-criticæ–¹æ³• -> A3C
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190915141712.png)
* Stateï¼šå¯¹äºç°å®ä¸–ç•Œæƒ…å†µçš„å®Œå…¨æè¿°ï¼ˆå¾€å¾€æ˜¯æ¯”è¾ƒæŠ½è±¡çš„å»ºæ¨¡ï¼‰
    * ç”¨ä¸€ä¸ªTensoræ¥ä½œä¸º*Representation*
* Observationä¸Stateç›¸å¯¹åº”ï¼Œæ˜¯å½“å‰ä¸–ç•Œä¸å®Œå…¨çš„æè¿°ï¼Œä½†æ˜¯ä¸€èˆ¬ä¼šæ¯”è¾ƒå…·ä½“  ~~æˆ‘ä¸åˆ°å•Šæˆ‘ççŒœçš„~~
* Action State-å†³ç­–ç©ºé—´
    * å¯èƒ½è¿ç»­ï¼ˆæœºå™¨äººåœ¨å®é™…åœºæ™¯ä¸­ï¼‰å¯èƒ½ç¦»æ•£AlphaGO
* Policy - a rule used by agent to decide which action to take ~~æˆ‘å¥½åƒæƒ³ä¸åˆ°ä¸€ä¸ªå¤ªå¥½çš„ä¸­æ–‡ç¿»è¯‘ï¼Œå†³ç­–æ–¹å¼ï¼Ÿ~~
    * ç‰©ç†æ„ä¹‰æ˜¯*çŠ¶æ€åŠ¨ä½œä»·å€¼å‡½æ•°Q(State,Action)å–æœ€å¤§å€¼æ—¶å€™çš„Action*
    * å¯ä»¥æ˜¯Deterministicï¼ˆHardï¼‰ *Max*
    * ä¹Ÿå¯ä»¥æ˜¯Stochastic ï¼ˆSoftï¼‰ *Soft*
        * Categorical Policies - Used In Discrete
            * å°±ç±»ä¼¼å»ºç«‹ä¸€ä¸ªå¯¹ç¦»æ•£Actionçš„Classifier
            * ğŸ¤”å·²çŸ¥äº†å„ä¸ªActionçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¯¹è¾“å‡ºéœ€è¦åšä¸€ä¸ª*Sampling*
        * Diagonal Gaussion Policies - Used In Continuous
            * ä¸€ä¸ªé«˜ç»´çš„é«˜æ–¯åˆ†å¸ƒï¼Œå¯ä»¥ç”±ä¸€ä¸ªå‡å€¼Vector&åæ–¹å·®çŸ©é˜µæ¥æè¿°
                * Diagnol Gaussian Distributionæ˜¯åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’é˜µçš„æ—¶å€™,è¿™ä¸ªæ—¶å€™åæ–¹å·®çŸ©é˜µä¹Ÿå¯ä»¥æ‹¿ä¸€ä¸ªVectoræ¥è¡¨ç¤º
            * ä¸€èˆ¬è¿™ç§æ–¹å¼é€šè¿‡ä¸€ä¸ªNNæŠŠè¾“å…¥çš„Observationæ˜ å°„ä¸ºMean Vectorä»¥åŠCovariance Matrixçš„ä¸€ä¸ªä»£è¡¨ï¼Œå¯ä»¥æ˜¯
                * ç›´æ¥æ˜¯Vector Of Log Standard Variation ï¼ˆæ ‡å‡†å·®çš„Logçš„å‘é‡ï¼‰
                * é€šè¿‡ä¸€ä¸ªNNå»ºç«‹ä¸€ä¸ªstateåˆ°log(Ïƒ)çš„æ˜ å°„ï¼ˆæˆ–è®¸ä¼šä¸è¾“å‡ºmeançš„ç½‘ç»œshare paramï¼‰
                * *ç”¨Logçš„åŸå› æ˜¯å€¼åŸŸè¦†ç›–æ•´ä¸ªå®æ•°åŸŸï¼Œè€Œæ ‡å‡†å·®åœ¨(0,1),è€Œä¸”æ²¡æœ‰ä»€ä¹ˆç²¾åº¦æŸå¤±*
            * Samplingå°±æ˜¯ä¸€ä¸ªé‡‡æ ·ï¼Œå·²çŸ¥ä¸€ä¸ªåˆ†å¸ƒä¹‹å Mean + Variation*Noise
* Trajectory (episodes / rollouts)
    * A Sequence T={s0,a0,s1,a1...}
    * The 1st State Is ramdom sample of state-state-distribution
    * çŠ¶æ€ä¹‹é—´çš„è½¬æ¢æ˜¯ç”±Environmentæ‰€å†³å®šçš„
    * ä¸ºäº†ç®—æ³•ç ”ç©¶ï¼Œä¸€èˆ¬å…·æœ‰é©¬å°”ç§‘å¤«æ€§ï¼Œå³åªä¾èµ–äºå‰ä¸€çŠ¶æ€
* **Reward**  
    * æœ‰æ—¶å€™ä¹Ÿå«åš*åŸºäºç­–ç•¥çš„çŠ¶æ€ä»·å€¼å‡½æ•° V(s)=E[G_t|S_t=s]*
    * rt = R{st,at,st+1}
    * ç»å¸¸è¢«ç®€åŒ–ä¸ºR(st)æˆ–è€…æ˜¯R(st,at)
    * The Goal Of a Agent is to get *Maxium cumulative reward*
    * Final Reward(return) 
        * Infinite with discount R = sum((discount_factor)*rt)
            * discount factor (0,1) The Earlier Than Now,The Less
            * *Used For Better Convergence*
        * Finite Without Discount

* **The Main Model(Optimization View)**

$$ P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1}{P(s_{t+1}|s_t,a_t)}\pi(a_t|s_t) $$

* è¯¥ï¼°è¡¨ç¤ºäº†é‡‡ç”¨è¯¥Trajectoryå‘ç”Ÿçš„æ¦‚ç‡(Under The Case Of A Certain Policy pi)

$$ J(\pi) = \int_{\tau}{P(\tau|\pi)R(\tau)} = E_{\tau \sim \pi}[R(\tau)]$$

* Jè¡¨ç¤ºäº†Return,æ˜¯Rewardå‡½æ•°çš„ä¸€ä¸ªæœŸæœ›
* è€ŒRLæœ€æ ¸å¿ƒçš„é—®é¢˜æ˜¯

$$ \pi^* = \argmax_\pi{J(\pi)}$$

* **Value Function**
* ~~æƒå½“é”»ç‚¼ä¸€ä¸‹æ‰‹æ‰“å…¬å¼çš„èƒ½åŠ›~~
* æ„ä¹‰æ˜¯Rewardå‡½æ•°åœ¨æŸç§æƒ…å†µä¸‹çš„æœŸæœ›ï¼Œä¸€èˆ¬ä¼šç”¨åˆ°ä»¥ä¸‹å‡ ç§
1. On-Policy Value Function: Given An Expecated *Policy* and Initial *State*

$$ V^(\pi)(s) = E_{\tau \sim \pi}[R(\tau)|s_0=s]$$

* Its Maximum(Optimal)

$$ V^*(s) = \max_{\pi}{E_{\tau \sim \pi}[R(\tau)|s_0=s]} $$

2. On Policy Action-Value Function: Take An Arbitary Action (which may not from policy)

$$ Q^\pi(s,a) = E_{\tau\sim\pi}[R(\tau)|s_0=s,a_0=a] $$

* Its Optimal

$$ Q^*(s,a) = \max_\pi{E_{\tau\sim\pi}[R(\tau)|s=s_0,a=a_0]} $$

* Their Connection

$$ V^*(\pi) = \max_{a}{Q^*(s,a)}$$

* **The Bellman Function**
* ä¸*åŠ¨æ€è§„åˆ’*ç´§å¯†è”ç³»,Bellman Equationæ˜¯ä¸€ä¸ªé€’å½’çš„å½¢å¼ã€‚
    * åŠ¨æ€è§„åˆ’çš„æœ¬è´¨ï¼š *å°†é—®é¢˜çš„æ±‚è§£è½¬åŒ–ä¸ºæ±‚è§£å­é—®é¢˜æ¥ä¸æ–­è§£å†³*è€ŒMarkovæ€§ä¸åŠ¨æ€è§„åˆ’å¯¹åº”
    * å¯ä»¥è®¤ä¸ºå½“å‰çš„Q(å€¼)å‡½æ•°å­˜å‚¨äº†ä¸Šä¸€æ­¥çš„å†³ç­–çš„ä¿¡æ¯ï¼Œå› è€Œå¤„ç†ä¸‹ä¸€æ­¥(ä¸‹ä¸€ä¸ªå­é—®é¢˜)çš„æ—¶å€™ä¸ç”¨é‡æ–°è®¡ç®—ï¼Œè€Œæ˜¯å¯ä»¥ç›´æ¥å–ä¸Šä¸€æ¬¡çš„å€¼æ¥ä½¿ç”¨
* ğŸ¤”The Core Is *The Value Of Your Starting Point Is The Reward You Expect To Get Being There + Wherever You Land Next*
  * The [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation) is a concept used In Dynamic Programming


* **The Main Model(Markov Model View)**
* Could be described as a MDP(Markov Decision Process) <S,A,R,P,$$ \rho_0 $$>
  * S - All States
  * A - All Valid Actions
  * R - reward Function $$r =  R(s_t,a_t,s_{t+1})$$
  * P - Transition Probaliliy Function
  * $$ \rho_0 $$ - The Starting State
* **MDP(Markov Decision Process)**
    * Markovå†³ç­–è¿‡ç¨‹,æ¯”ä¸€åŠçš„Markovè¿‡ç¨‹,åœ¨æ¯ä¸ªçŠ¶æ€è½¬ç§»çš„è¿‡ç¨‹ä¸­,å®šä¹‰ä¸€ä¸ªå›æŠ¥å‡½æ•°(ä¸€èˆ¬åœ¨æ¯ä¸€æ­¥ä¸ŠåŠ ä¸€ä¸ªæŠ˜æ‰£å› å­)
    * å†³ç­–ä¼˜åŒ–çš„ç›®æ ‡æ˜¯å¯»æ‰¾åˆ°ä¸€æ¡ si->sj->sk è¿™æ ·çš„**è·¯å¾„**,è®©å›æŠ¥å‡½æ•°æœ€å¤§åŒ–
    
## Mc & TD
* æ˜¯DLä¹‹å¤–çš„å¦å¤–åˆ†æ”¯,é¢å‘æŸäº›ç‰¹åˆ«çš„Scenario 
* Mc: **Monte-Carlo**é€‚ç”¨äº"æƒ…èŠ‚å‹ä»»åŠ¡"(ä¸ä¹‹å¯¹åº”çš„æ˜¯è¿ç»­å‹)
* TD: **Time Difference**ç»“åˆäº†MCå’ŒDP(Dynamic Programming)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190915141540.png)

## Q-Learning
* åŸºäºè¡¨æ ¼çš„æ–¹æ³•ï¼ˆè¡¨æ ¼çš„XYè½´åˆ†åˆ«æ˜¯å½“å‰stateå’Œå½“å‰æ—¶åˆ»æ‰€ä½œå‡ºçš„å†³ç­–ï¼›é‡Œé¢çš„å€¼æ˜¯æ½œåœ¨å¥–åŠ±-Qå€¼ï¼‰
    * è¿™ä¸ªè¡¨æ ¼*Q-table*å°±åƒæ˜¯ä¸ªçŠ¶æ€è½¬ç§»çŸ©é˜µï¼šå†³å®šäº†æ¯ä¸€ä¸ªçŠ¶æ€éœ€è¦é‡‡å–ä»€ä¹ˆæ ·çš„ç­–ç•¥
* é‚£ä¹ˆQå€¼æ˜¯æ€ä¹ˆå®šä¹‰å’Œæ›´æ–°å‘¢ï¼Ÿ
    * å¦‚æœå‡ºç°äº†æ–°çš„stateï¼ˆæ’å…¥å…¨0æˆ–è€…ç‰¹å®šåˆå§‹åŒ–ï¼‰æ‹“å±•QTable
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908104247.png)
* æ¯æ¬¡æ›´æ–°éƒ½ä¼šç”¨åˆ°Qçš„çœŸå®å€¼å’Œé¢„æµ‹å€¼
* alphaæ˜¯å­¦ä¹ ç‡ï¼Œè¡¨ç¤ºå­¦ä¹ é€Ÿåº¦ / gammaæ˜¯å¯¹æœªæ¥rewardçš„è¡°å‡å€¼ï¼ˆæ·±è°‹è¿œè™‘çš„ç¨‹åº¦ï¼‰
* Q-Learningæ˜¯Off-Policyçš„ï¼ˆQ-Tableå­˜å‚¨äº†ä¹‹å‰çš„ç»éªŒï¼Œå…¶æ›´æ–°å¯ä»¥ä¸åŸºäºæ­£åœ¨ç»å†çš„ç»éªŒï¼‰
    * ä½†æ˜¯æˆ‘ä»¬å¹¶æ²¡æœ‰åˆ©ç”¨åˆ°Q-Learning OFF-policyçš„ç‰¹æ€§ï¼Œåœ¨DQNä¸­ä¼šæœ‰ç”¨åˆ°
* **ProblemğŸ˜£**:ä¸€èˆ¬æ¥è¯´,Q Valueå¯¹ä¸åŒActionçš„æ–¹å·®å°,éš¾æ”¶æ•›(è€Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹,Q-Valueå§‹ç»ˆä¸ºæ­£))
  * Solution:å¼•å…¥Advantage Function

## Sarsa(State-Action-Reward-State-Action-è¿™ä¸ªåå­—ä¹Ÿå¤ªé­”æ€§äº†...) 
* å‰å‘æ¨æ–­æ–¹å¼ç±»ä¼¼äºQ-Learningï¼ˆéƒ½æ˜¯ä»ä¸€ä¸ªQTableä¸­é€‰å–æœ€å¤§Qå€¼çš„ç­–ç•¥ï¼‰
* æ›´æ–°æ–¹å¼ä¸åŒï¼š
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908135538.png)
* å¯ä»¥ç†è§£ä¸ºå¯¹å·®è·çš„å®šä¹‰ä¸ä¸€æ ·ï¼š
    * Q-Learningæ˜¯ â€œä¸‹ä¸€çŠ¶æ€çš„æœ€ä½³Qâ€ - â€œå½“å‰çŠ¶æ€Qâ€ ï¼ˆå¯ä»¥çœ‹åšè§‚å¯Ÿäº†ä¸‹ä¸€çŠ¶æ€ä¼šæ€ä¹ˆæ ·ï¼Œå†åšåˆ¤æ–­ï¼ˆè¿˜æœ‰è½¬æœºï¼‰ï¼‰
    * è€ŒSarsaç›´æ¥å¯¹ â€œä¸‹ä¸€çŠ¶æ€çš„Qâ€ - â€œå½“å‰çŠ¶æ€çš„Qâ€ ï¼ˆå¯ä»¥çœ‹åšå·²ç»åšå‡ºäº†å†³å®šåˆ°äº†ä¸‹ä¸€çŠ¶æ€ï¼Œä¸å†æœ‰å›æ—‹ä½™åœ°ï¼‰
    * Q-Learningåœ¨å½“å‰æ—¶åˆ»åªä¼šå®šå½“å‰æ—¶åˆ»çš„Actionï¼ˆå†³å®šäº†ä¸‹ä¸€æ—¶åˆ»çš„Stateï¼‰ä½†æ˜¯ä¸ä¼šç¡®å®šä¸‹ä¸€æ—¶åˆ»çš„Actionï¼ˆéœ€è¦çœ‹MaxQ(s',a')ï¼‰
    * Sarsaçš„æ¯ä¸€æ—¶åˆ»éƒ½å†³å®šäº†å½“å‰æ—¶åˆ»çš„Actionä»¥åŠä¸‹ä¸€æ—¶åˆ»çš„Action
        * æ¯ä¸€æ—¶åˆ»å‚ä¸å­¦ä¹ çš„å‚æ•°æœ‰(State-Action-Reward-State'-Action')
* è¿™ä¹ˆçœ‹Q-Learningæ›´åŠ **è´ªå©ª**ï¼ˆä½“ç°åœ¨æœ€å¤§åŒ–maxQï¼‰ä¸€äº›ï¼Œå¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½è¡¨ç°å¾—å¥½çš„è¯ï¼ŒQ-Learningèƒ½æ‰¾åˆ°æœ€è¿‘çš„è·¯çº¿ï¼Œè€ŒSarsaåˆ™ä¼šè¡¨ç°çš„ç›¸å¯¹**ä¿å®ˆ**ï¼ˆå› ä¸ºä¸ä¿å®ˆçš„éƒ½æ­»äº†ï¼‰
* Sarsaå±äº*å•æ­¥æ›´æ–°*çš„ç®—æ³•

## Improved-Sarsa -> Sarsa(lambda)
* åŠ é€Ÿçš„Sarsaï¼Œä¸»è¦å°±æ˜¯æŠŠ*å•æ­¥æ›´æ–°*æ”¹æˆäº†*Næ­¥æ›´æ–°*ï¼Œèƒ½å¤Ÿé¿å…â€œåŸåœ°æ‰“è½¬â€çš„æƒ…å†µ
    * lambdaåœ¨[0,1] - 0å°±é€€åŒ–ä¸ºç®€å•Sarsaï¼Œ1å°±å˜æˆç›´åˆ°è·å–äº†rewardæ‰æ›´æ–°

## DQN - Deep Q Network
* å½“é—®é¢˜å¤æ‚ä¹‹åï¼ŒQ-Tableä¼šå˜å¾—éå¸¸å¤§ï¼Œ*å°†ä»¥ä¸€ä¸ªè¡¨æ ¼å­˜å‚¨Qå€¼ï¼Œè½¬ä¸ºç”±ä¸€ä¸ªNNæ¥ç”ŸæˆQå€¼*ğŸ¤”ï¼ˆç”¨ä¸€ä¸ªNNæ¥å»ºæ¨¡QTableçš„è§„å¾‹ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ç§å‹ç¼©æ„ŸçŸ¥ï¼Ÿï¼‰
* DQN Workçš„æ ¸å¿ƒ(ç”±äºDQNä½¿ç”¨NN,ä½†æ˜¯ä¸æ»¡è¶³*iid*(ç‹¬ç«‹åŒåˆ†å¸ƒ),ç†è®ºä¸Šåœ¨è¿™ä¸ªç¯èŠ‚ä¸Šä¸æ˜¯ç‰¹åˆ«ä¸¥è°¨,æ‰€ä»¥éœ€è¦ä¸€äº›é¢å¤–çš„è®¾è®¡æ¥ä¿è¯ç½‘ç»œçš„æ”¶æ•›)ï¼š 
    1. Experience Replayï¼š ç”±äºQ-Learning Off-Policyï¼ˆç¦»çº¿å­¦ä¹ ï¼Œèƒ½å­¦ä¹ è¿‡å»ä»¥åŠç°åœ¨çš„Experienceï¼Œä¹Ÿå¯ä»¥æ˜¯åˆ«çš„agentçš„ï¼‰
        * éšæœºæŠ½å–è¿‡å»çš„ç»å†è¿›è¡Œå­¦ä¹ ï¼Œè€Œæ‰“ä¹±ç»å†ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œé¿å…å­¦åˆ°æˆ‘ä»¬ä¸éœ€è¦çš„(æˆ–è€…è¯´æ˜¯ä¸€ä¸ªéšæœºé‡‡æ ·,æ¥ä¿è¯éšæœºæ€§ä»¥å°½é‡è¾¾æˆåˆ†å¸ƒçš„**ç‹¬ç«‹æ€§**)
    2. Fixed-Q-Target
        * ä½¿ç”¨ä¸¤ä¸ªç»“æ„ä¸€æ ·çš„NN(Target/Estimation Network)ï¼Œå®é™…è¿›è¡ŒQ-Estimationçš„ç½‘ç»œå…·å¤‡æ–°çš„å‚æ•°ï¼Œè€Œé¢„æµ‹$$\hat{Q}$$çš„NNç”¨çš„æ˜¯æ¯”è¾ƒä¹…ä¹‹å‰çš„
        * æ¯Cè½®è¿­ä»£ä¹‹åè®©target<-estimation,ç›®çš„iæ˜¯æ›´å¥½çš„ç¨³å®šè®­ç»ƒ    
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190909150807.png)

## DDPG (Deterministic Policy Gradient)
* DQNèƒ½å¤Ÿè§£å†³é«˜ç»´çš„ç¦»æ•£é—®é¢˜,ä½†æ˜¯å¯¹äºè¿ç»­ç©ºé—´æˆ–æ˜¯å˜åŒ–å¾ˆå¤§çš„Scenarioä¸å¤ªé€‚ç”¨
* é‡‡ç”¨äº†Actor-Criticè§£å†³è¿ç»­ç©ºé—´çš„é—®é¢˜
  * via å¼•å…¥Actorè¾“å‡ºè¿ç»­çš„åŠ¨ä½œ(æˆ–è€…æ˜¯ç¦»æ•£çš„),Criticå¯¹[s,a]æ‰“åˆ†
* ç±»ä¼¼äºDQN,ä¹Ÿæ˜¯Dual-Network,Criticå’ŒActorå„æœ‰ä¸¤ä¸ªNN(Target/Estimation)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190915134806.png)
* DDPGä¸æ˜¯ç›´æ¥å°†Estimationä»˜ç»™Target,è€Œæ˜¯é‡‡ç”¨äº†soft target update,è®©å‚æ•°*ç¼“æ…¢æ›´æ–°*
* æœ‰Trickæ˜¯å¯¹Critcæˆ–è€…æ˜¯Actorç©ºé—´åŠ å…¥noise,ä¼šé¼“åŠ±Actorè¿›è¡ŒExploration,æœ€åæ•ˆæœä¼šæ›´å¥½

##ã€€TRPO(Trust Region Policy Optimization)
* è§£å†³DDPGç½‘è·¯å‚æ•°æ›´æ–°çš„æ­¥é•¿ä¸ç¡®å®šé—®é¢˜
* UCB,æ ¸å¿ƒåœ¨äºå­¦ä¹ çš„å¯ä¿¡åº¦
* å¥½æ•°å­¦...

## PPO (Proximal Policy Optimization)

## A3C - Asynchronous Advantage Actor-Critic
* åšåˆ°äº†å¼‚æ­¥,é€‚ç”¨äºåˆ†å¸ƒå¼çš„å¹³å°,å¤šä¸ªactor-criticå¯¹,æ¯ä¸ªå¯¹åº”äº†ä¸åŒçš„æ¢ç´¢ç­–ç•¥(å¤šä¸ªå°é˜Ÿå…±åŒæ¢ç´¢)
  * ç”±äºå¼‚æ­¥å„å¯¹ä¹‹é—´ä½ç›¸å…³,ä¸éœ€è¦ä½¿ç”¨DQNä¸­çš„Experience Replayæœºåˆ¶æ¥è®­ç»ƒ
* æ•´ä½“æ¶æ„å’ŒDDPGç±»ä¼¼,æ¯”DDPGå¤šäº†: **é‡‡ç”¨äº†Max(Advantage)è€Œä¸æ˜¯Max(Q)**
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190915142259.png)


## Multi-agent

> 2019.09.26 ç»„ä¼šæ·»åŠ çš„å†…å®¹

* å•æœºåˆ°å¤šæœºå¸¦æ¥çš„æ–°éº»çƒ¦
    * éå¹³ç¨³
        * å¦‚æœé‡‡ç”¨å¯¹å•ä¸€Agentè¿›è¡Œè®­ç»ƒï¼Œè€Œå°†å…¶ä»–Agentå½“åšç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼›å¤šä¸ªAgentåŒæ—¶å¯¹ç¯å¢ƒè¿›è¡Œä½œç”¨ï¼Œå¯¼è‡´ä¸§å¤±å¹³ç¨³æ€§ï¼Œç†è®ºä¸èƒ½ä¿è¯æœ€ä¼˜è§£
        * **é›†ä¸­å­¦ä¹ ï¼Œåˆ†å¸ƒæ‰§è¡Œ**
    * æ›´å¤§çš„çŠ¶æ€ç©ºé—´
        * å•çº¯å°±æ˜¯ç»´åº¦ä¼šå˜å¤§
        * **å‚æ•°å…±äº«**
    * è§‚æµ‹é‡æ›´åŠ åˆ†æ•£ï¼Œæ›´éš¾æ•´åˆåˆ©ç”¨ä¿¡æ¯
    * å½“ä¸åŒAgentå¼‚è´¨çš„æ—¶å€™å°¤ä¸ºéš¾æ•´

## Ref
* [OpenAI SpinningUp](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#id20)
* [Morvan's Blog](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-1-A-q-learning/) 