---
layout:     post                    # 使用的布局（不需要改）
title:      3D related            # 标题 
subtitle:   View things from another dimension        #副标题
date:       2020-11-29            # 时间
author:     tianchen                      # 作者
header-img:  img/2020/street1.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - survey
---


## Codebase

* [Minkowski by Nvidia](https://github.com/NVIDIA/MinkowskiEngine)
	* [SpatialTemporal](https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/datasets/scannet.py) contains scannet preprocess

* [torch-point3d](https://github.com/nicolas-chaulet/torch-points3d)

* [E3D - MIT HAN Lab](https://github.com/mit-han-lab/e3d)
	* torchsparse
	* PVCNN
	* SPVNAS

### related code/packages

* [*o3d](http://www.open3d.org/docs/release/getting_started.html)

### Data-formats

* ```pcd = o3d.io.read_point_cloud("1.ply")```
	* type `Geometry.PointCloud`
	* 一般point cloud直接`np.floor(coords / voxel_size)` - 就可以获得quantize之后的coords
		* 这样相当于直接把对应的点移动到了空间的mesh上(Sub-Nearest)，还有多种quantize的方法
*  `Mesh`
	* with vertices`


## Dataset

* [ModelNet]()
	* 大概相当于CIFAR10
* [ShapeNet]()
	* CIFAR-100

* [ScanNet](http://kaldir.vc.in.tum.de/scannet_benchmark/)
	* 室内Seg数据集
	* pointnet-v2 process code [here](https://github.com/daveredrum/Pointnet2.ScanNet)
	* also at [PointCNN](https://github.com/yangyanli/PointCNN)
* [ScanNet-V2]() - updated on 2018
	* processed on [Google Drive*](https://drive.google.com/drive/folders/1xz59bKaIZbf0BU3oKSTs3qyV3gRf7aDW?usp=sharing)
	* [pointnet-v2-process](https://github.com/charlesq34/pointnet2/tree/master/scannet)
	* [pointcnn-process](https://github.com/yangyanli/PointCNN)
		* [data-conversion](https://github.com/yangyanli/PointCNN/blob/master/data_conversions/README.md)
* Maybe Usefule - [SpatialTemporal-process](https://github.com/chrischoy/SpatioTemporalSegmentation-ScanNet)

* [SemanticKitti]()
	* [PVCNN's Preparation](https://github.com/mit-han-lab/pvcnn/tree/db13331a46f672e74e7b5bde60e7bf30d445cd2d#data-preparation)

* [KP-Conv](https://github.com/HuguesTHOMAS/KPConv-PyTorch) provide some dataset sources

### ScanNet Dataset Preparation

1. Download
通过官方发Email获得邮件下载script
下载了ply文件 ``` python download-scannet.py -o ./ --type _vh_clean_2.ply```(clean_2是downsample之后的ply),还有要下载(_vh_clean_2.labels.ply,_vh_clean_2.0.010000.segs.json)
文件夹结构如下:
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205092449.png)

2. Preprocess

* 参考了[SpatialTemporal-process](https://github.com/chrischoy/SpatioTemporalSegmentation-ScanNet)的处理方式
2-1. 修改```SpatioTemporalSegmentation/lib/datasets/preprocessing/scannet.py``` 中的

2-2. 从[ScanNet官方git-repo]()下载对应的filelist,放到对应目录
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205163418.png)

2-3. 对该repository还需要修改读取文件后缀
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205163641.png)

2-4. 采用train命令测试,将scannet_process/train/作为datasetpath输入


---

* tasks - Semantic Scene Understanding(SSU)
	* fundamental vision tasks: det, seg, pose estimation (High-level understanding, instance-level)
	* base tasks: object registeration(low-level, point-level)

---

* forms of data & conversion
1. RGB-D: color and depth are complementary modal, how to apply feature fuse is key
	*2.5-D solution: 2-D image of [R,G,B,D]
	* both RGB and depth to form colorized point cloud(6-channel point cloud)RGBXYZ

## MinkowskiEngine Doc

### Features & Terminology

* Sparse Tensor(Point Cloud dara are sparse) 

* also support negative coordinates(?)
* generalized convolution
	* since the in/out is sparse, we must know how the non-zero elment in input maps to the output, **kernel_map**
* sparse tensor
	* use  COOrdinate list format to represent a sparse tensor
	* 一个大的coord matrix C，以及feature vector F
* tensor stride
* coordinate manager
	* generate new set of output coordinates with different order(conv pooling)
	* 在`SparseTensor`中的`coords_man`
		* 当进行inplace运算的时候,参与计算的各个tensor需要共享coords_manager,可以直接丢coord_key就可以不用输入coords了
	* coordeinate key - the hash to index the unordered coordinate manager(share the same memory)
	* caches the kernel_map and cur coordinates
		* reuses the coordinate instead of recomputing the order for series of convs
* kernel_map
	* [[I],[O]] 

* SparseTensor initial requires coord with additional batch indice
	* use `ME.utils.sparse_collate` / `ME.utils.batched_cordinates`
	* 上面的函数的作用就只是给一个batch内的所有的coords加上一个新的dim，BATCH，值为batch_idx
		* 从原本的[num_points, num_dim] -> [num_points, num_dim+1(batch_idx)]
	* 经过这样打包之后的coord和feature才能去initialize SparseTensor

```
coords0, feats0 = to_sparse_coo(data_batch_0)
coords1, feats1 = to_sparse_coo(data_batch_1)
coords, feats = ME.utils.sparse_collate(
    coords=[coords0, coords1], feats=[feats0, feats1])
```

* discretize the continous coordinates

```
sinput = ME.SparseTensor(
    feats=torch.from_numpy(colors), # Convert to a tensor
	    coords=ME.utils.batched_coordinates([coordinates / voxel_size]),  # coordinates must be defined in a integer grid. If the scale
		    quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE  # when used with continuous coordinates, average features in the same coordinate
			)
logits = model(sinput).slice(sinput)
```

* quantize the coords in dataset
	* use `ME.utils.sparse_quantize`, could use `return_index=True`

* defining dataloader
	* define the `colate_fn` to convert the input to proper output
	* `collate_fn = ME.utils.batch_sparse_collate / SparseCollation()`
		* collation - 整理校对

> in examples/training & examples/sparse_tensor_basics

* data of getitem() of the dataset, before using the `ME.utils.batch_sparse_collate`
	- discrete-coords  - [num_point, dim_coord]
	- out_feature      - [num_point, dim_feature]
	- label			   - [num_point] 
		* contains [N_CLASS] choices of elements

* process of the `ME.MinkowskiConvolution`
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205092049.png)


## Survey

#### Deep Learning for 3D Point Clouds: A Survey

* 3 tasks: Shape Classification, Object Det & Tracking, Semantic Segmentation
* Data source: LiDAR, RGBD-camera, 3D-Scanners
* Data form: depth images, point cloud. meshes, volumeetric grids
	* point cloud: no discretization(Better representation)
* Challenges: 1. small scale dataset; 2. high-dimension; 3. unstructured natureA
* Available Datasets: 
	- ModelNet
	- ScanObjectNN
	- ShapeNet
	- ParNet
	- S3DIS
	- ScanNet
	- Semantic3D
	- ApooloCar3D
	- KITTI
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201129200502.png)
* Evaluation Metrics:
	- classification: Overall Acc(Acc of all test instances), meanAcc*(mAcc - acc for all classes)
	- Det: AP(Average Precision): area-under precision-recall curve
	- Track: precision & success
	- Segmentation: meanIOU(meanIntersectionOverUnion), meanClassAccuracy

* 3D Classification
	- Multi-view - convert unstructured point cloud to 2d images
		* project into multi 2d views, then fuse these features
		* Key: how to aggregate the multi-view feature
		* MVCNN: maxpool multiview feature, which causes information loss
		* MHBN: harmonized bilinear pooling
		* Relation network to discover relations between group of views
		* View-GCN: directed graph, graph nodes as multi-views
	- Volumetric - convert into 3d volumetric form
		* Key: 1st voxelize into 3d grids, then apply a 3DCNN
			* Preprocess + CNN: how to find good preprocess
		* Problems: unable to scale well to dense 3D-Data
			* OctTree is sometimes introduced
		* VoxNet: volumetric occupacy network
		* Deep Belief 3D ShapeNets: 
		* OctNet: use hierarchical octree to gen a bit-string representation
		* PointGrid: integrate point and grid representatio
		* 3DmFV: 3D grids further processed with 3D modified Fisher Vector
	- Point-based - directly without voxelization / projection
		* no explicit loss, more popular
		* category: 1. point-wise MLP 2. CNN-based 3. graph-based 4. hier-data structure 5. others
		* MLP: 
			* have permutation invariance with symmetric function
			* PointNet: 
			* DeepSets: summing all representation than transformation
			* PointNet++: hierarchical network to capture geometric from neighbourhood - (sampling/grouping/PointNet-based)
			* MoNet: PointNet-like
			* PAT(Point Attention Transform): represent point with its abs position and relative position with neighbours, then group shuffle attention used for get relations, then gumbel-set-sample used for learn hier fetature. 
			* PointWeb: improve feature by Adaptive feature adjustment
			* SRN(Structural Relation Net): learn structure feature 
			* SRINet: project point cloud to find rotation invariant features, then use pointnet backbone, then graph-based aggregation
			* PointASNL: adaptive sampling - furthertest point sampling methods + local-nonlocal module
		* Conv-based:
			* Continuous Conv:
				* the weights for neighboring points are related to the spatial distribution with respect to the center point
					* on spherical harmonic
				* conv could be viewed as a wieghted sum over a given subset
				* RS-CNN
				* DensePoint
				* KPConv
				* ConvPoint
				* PointConv
				* MCCNN
			* Discrete Conv:
				* the weights for neighbouring points are related to offsets with respeqct to center poin
		* Graph-based - each point is a vertex, the directed edge represents the neighbourhood
			* On Space-division
				* conv is MLP over spatial neighbours, pooling is adopted produce coarse graph
			* On Spectral-division
				* conv as spetral filtering, applying mult on graph laplacian matrix eigenvectors
		* Hier Data Structure 
			* apply on Hier-data structure(kd-tree & octTree)
			* feature aggregation from leaf to root node
	- Summary:
		* pointwise MLP often serve as a basic building-block
		* CNN & GNN are promising directions
			* how to handle irregular data structure is key
		* Efficiency is often a problem


* 3D Detection: - output the 3D Bounding Boxes
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130154412.png) 
	* Taxnomy: 
		* 1. Region-Proposal based(2-Stage)
			* Mult-view based methods
			* Segmentation-based methods(Use segmentation to remove background points, then raise high-quality proposals)
			* Frustem-based: generate 2-d proposal, then transform to frustem(几何体) 3d ones
		* 2. Single-Shot
			* BEV-based (Bird-View)
			* Discreticized: for voxels
			* Point-based: diirectly on raw 3d pointcloud
	* Problems:
		* how to efficiently fuse multi-modality feature
		* extract robust representation
		* long-range detection poor
		* how to exploit texture information
	* Summary:
		* currentlly 2-stage outperform single by a large margin
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130161206.png)

* 3D Tracking - give the location of the obkject at the 1st frame, estimate its state in subsequent frames
	* use the rich geometric information to overcome drawbacks of of image tracking like: occlusion, illumination, variation
	* 3D version of Siamese

* 3D Scnen Flow Estimation
	* Given 2 point cloud, measure its movement, the 3d version of optical flow
	* point-based most rich information, however no explicit neighbourhood contains in the point representation
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130163319.png)

---

* representations forms
	* Projection/Discretization based could leverage the 2-d network architecture, dealing with structured data form. hoewver, information loss
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130164056.png)

* Current Problems &  Future Directions
	1. imbalanced segmentation
	2. Dense point clouds
	3. spatial-temporal information for dynamic point clouds


---

> MingYuan的Survey

* tasks:
	* 3D Shape Classification
	* Segmantions (Part/Semantic)
* Methods:
	* Volumetric-based - 体素化
		* 转变为voxel，会损失一定的信息
		* 细的时候计算复杂度很高
		* 体素化的example，找一个大1x1立方体，切成小块
	* Multiview - 3D to 2D
	* Point-based: per point [x,y,z+N(feature)]
* PointNet(pointwise-MLP) - directly apply deep learning on points)
	* point properties: unordered & invariant to geometric transform 	* facing orderless: symmetric computing  - 具有对称性的运算
	* T-Net: 学习出一个线性变换，为了摆正，目的是做对齐
		* T-Net + MLP
	* 没有点之间的联系
* PointNet++ 
	* 在pointnet之前加上了一个sampling和grouping(非常关键)
			* sample - 最远点采样
			* grouping - K-近邻 
			* 相当于降采样
	* seg任务中是一个插值
	* sample的可能改进方向
		* non-uniform sampling
		* 会有hier的分支
* KPConv 
	* 一个⚪的卷积核，里面固定的位置有一些点，实际卷的时候将对应的点放在圆心，按照距离吧其他的weight加权求和生成一个新的weight与x乘
* PointCNN
	* Concat(MLP(position),color)x(MLP(x) as transform)
* PVCNN
	* Point + Voxel
		* 2条支路,voxel本身也可以看成是一个sample，voxel先体素化然后卷积，然后再反体素化
	* Voxel-based有很大冗余
	* 本身没有降采样
* Grid-CNN
	 




