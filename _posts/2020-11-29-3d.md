---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      3D related            # æ ‡é¢˜ 
subtitle:   View things from another dimension        #å‰¯æ ‡é¢˜
date:       2020-11-29            # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/2020/street1.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - survey
---


## Codebase

* [Minkowski by Nvidia](https://github.com/NVIDIA/MinkowskiEngine)
	* [SpatialTemporal](https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/datasets/scannet.py) contains scannet preprocess

* [torch-point3d](https://github.com/nicolas-chaulet/torch-points3d)

* [E3D - MIT HAN Lab](https://github.com/mit-han-lab/e3d)
	* torchsparse
	* PVCNN
	* SPVNAS

### related code/packages

* [*o3d](http://www.open3d.org/docs/release/getting_started.html)

### Data-formats

* ```pcd = o3d.io.read_point_cloud("1.ply")```
	* type `Geometry.PointCloud`
	* ä¸€èˆ¬point cloudç›´æ¥`np.floor(coords / voxel_size)` - å°±å¯ä»¥è·å¾—quantizeä¹‹åçš„coords
		* è¿™æ ·ç›¸å½“äºç›´æ¥æŠŠå¯¹åº”çš„ç‚¹ç§»åŠ¨åˆ°äº†ç©ºé—´çš„meshä¸Š(Sub-Nearest)ï¼Œè¿˜æœ‰å¤šç§quantizeçš„æ–¹æ³•
*  `Mesh`
	* with vertices`
	* ?: differenece between "mesh" "grid" & "voxel"


### Terminology

* Couple of data formats: 
	1. Lattice/Mesh: graphicså½“ä¸­å¸¸ç”¨çš„ä¸€ç§3dæ•°æ®å­˜å‚¨æ ¼å¼ï¼Œå®è´¨ä¸Šæ˜¯ä¸€ä¸ªgraphï¼›å«äº†vertexå’Œedgeï¼Œä¸¤è€…ä¸Šé¢éƒ½å¯ä»¥æœ‰feature
	2. Voxel: ç©ºé—´ä¸­çš„å°æ–¹æ ¼ï¼Œä½†æ˜¯å…¶å®å¯ä»¥å†…åµŒç‚¹äº‘æˆ–è€…æ˜¯Feature
	3. Gridæœ¬èº«å°±æ˜¯2D imageçš„å½¢å¼ï¼Œæ¯ä¸ªpixelä¸€ä¸ªvalue

* canonicalization:
	* pointnetä¸­çš„T-Net(ç„¶è€Œåœ¨åç»­è¢«å‘ç°æ²¡æœ‰å¤šå°‘ç”¨)æ˜¯åšè¿™ä¸ªï¼Œç›®çš„æ˜¯è®©feature/inputç©ºé—´å˜å¾—è§„æ•´ï¼Œæœ‰ç‚¹ç±»ä¼¼äºBN
	* ç±»ä¼¼äºnormalization,ä½†æ˜¯ä¸ä¼šscalingï¼Œä¼šæœ‰translationä»¥åŠrotation

## Dataset

> 3-d model dataset

* [ModelNet40]()
	* Generated
	* (9843/2468)
	* Subset ModelNet10 - (3991/908)
	* å¤§æ¦‚ç›¸å½“äºCIFAR10
* [ShapeNet]()
	* CIFAR-100
	* 220k 3,135 classes
	* ShapeNetCore - 51,300  - 55 class
	* ShapeNetAug - extend from ShapeNetCore - 26k models - 573k labels - 24 classes
	* ShapeNetSeg - 12k - 270 class

> 3d-indoor

* [ScanNet](http://kaldir.vc.in.tum.de/scannet_benchmark/)
	* å®¤å†…Segæ•°æ®é›†
	* pointnet-v2 process code [here](https://github.com/daveredrum/Pointnet2.ScanNet)
	* also at [PointCNN](https://github.com/yangyanli/PointCNN)
* [ScanNet-V2]() - updated on 2018
	* processed on [Google Drive*](https://drive.google.com/drive/folders/1xz59bKaIZbf0BU3oKSTs3qyV3gRf7aDW?usp=sharing)
	* [pointnet-v2-process](https://github.com/charlesq34/pointnet2/tree/master/scannet)
	* [pointcnn-process](https://github.com/yangyanli/PointCNN)
		* [data-conversion](https://github.com/yangyanli/PointCNN/blob/master/data_conversions/README.md)
	* Maybe Usefule - [SpatialTemporal-process](https://github.com/chrischoy/SpatioTemporalSegmentation-ScanNet)
* [NYUDv2]()
* [ScnenNN]()
* [S3DIE]()

> 3-d outdoor

* [SemanticKitti]()
	* [PVCNN's Preparation](https://github.com/mit-han-lab/pvcnn/tree/db13331a46f672e74e7b5bde60e7bf30d445cd2d#data-preparation)
	* [KP-Conv](https://github.com/HuguesTHOMAS/KPConv-PyTorch) provide some dataset sources
* [Semantic3D]()
* [DBNet]()
* [Appollo]()
* [BLVD]()


### ScanNet Dataset Preparation

1. Download
é€šè¿‡å®˜æ–¹å‘Emailè·å¾—é‚®ä»¶ä¸‹è½½script
ä¸‹è½½äº†plyæ–‡ä»¶ ``` python download-scannet.py -o ./ --type _vh_clean_2.ply```(clean_2æ˜¯downsampleä¹‹åçš„ply),è¿˜æœ‰è¦ä¸‹è½½(_vh_clean_2.labels.ply,_vh_clean_2.0.010000.segs.json)
æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹:
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205092449.png)

2. Preprocess

> å‚è€ƒäº†[SpatialTemporal-process](https://github.com/chrischoy/SpatioTemporalSegmentation-ScanNet)çš„å¤„ç†æ–¹å¼

* ä¿®æ”¹```SpatioTemporalSegmentation/lib/datasets/preprocessing/scannet.py``` ä¸­çš„
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205163418.png)

* ä»[ScanNetå®˜æ–¹git-repo](https://github.com/ScanNet/ScanNet/tree/master/Tasks/Benchmark)ä¸‹è½½å¯¹åº”çš„filelist,æ”¾åˆ°å¯¹åº”ç›®å½•(train/testç›®å½•ä¹Ÿæ”¾ä¸€ä»½)
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205165038.png)

* å¯¹è¯¥repositoryè¿˜éœ€è¦ä¿®æ”¹è¯»å–æ–‡ä»¶åç¼€
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205163641.png)

* é‡‡ç”¨trainå‘½ä»¤æµ‹è¯•,å°†scannet_process/train/ä½œä¸ºdatasetpathè¾“å…¥


---

* tasks - Semantic Scene Understanding(SSU)
	* fundamental vision tasks: det, seg, pose estimation (High-level understanding, instance-level)
	* base tasks: object registeration(low-level, point-level)

---

* forms of data & conversion
1. RGB-D: color and depth are complementary modal, how to apply feature fuse is key
	*2.5-D solution: 2-D image of [R,G,B,D]
	* both RGB and depth to form colorized point cloud(6-channel point cloud)RGBXYZ

## MinkowskiEngine Doc

### Features & Terminology

* Sparse Tensor(Point Cloud dara are sparse) 

* also support negative coordinates(?)
* generalized convolution
	* since the in/out is sparse, we must know how the non-zero elment in input maps to the output, **kernel_map**
* sparse tensor
	* use  COOrdinate list format to represent a sparse tensor
	* ä¸€ä¸ªå¤§çš„coord matrix Cï¼Œä»¥åŠfeature vector F
* tensor stride
* coordinate manager
	* generate new set of output coordinates with different order(conv pooling)
	* åœ¨`SparseTensor`ä¸­çš„`coords_man`
		* å½“è¿›è¡Œinplaceè¿ç®—çš„æ—¶å€™,å‚ä¸è®¡ç®—çš„å„ä¸ªtensoréœ€è¦å…±äº«coords_manager,å¯ä»¥ç›´æ¥ä¸¢coord_keyå°±å¯ä»¥ä¸ç”¨è¾“å…¥coordsäº†
	* coordeinate key - the hash to index the unordered coordinate manager(share the same memory)
	* caches the kernel_map and cur coordinates
		* reuses the coordinate instead of recomputing the order for series of convs
* kernel_map
	* [[I],[O]] 

* SparseTensor initial requires coord with additional batch indice
	* use `ME.utils.sparse_collate` / `ME.utils.batched_cordinates`
	* ä¸Šé¢çš„å‡½æ•°çš„ä½œç”¨å°±åªæ˜¯ç»™ä¸€ä¸ªbatchå†…çš„æ‰€æœ‰çš„coordsåŠ ä¸Šä¸€ä¸ªæ–°çš„dimï¼ŒBATCHï¼Œå€¼ä¸ºbatch_idx
		* ä»åŸæœ¬çš„[num_points, num_dim] -> [num_points, num_dim+1(batch_idx)]
	* ç»è¿‡è¿™æ ·æ‰“åŒ…ä¹‹åçš„coordå’Œfeatureæ‰èƒ½å»initialize SparseTensor

```
coords0, feats0 = to_sparse_coo(data_batch_0)
coords1, feats1 = to_sparse_coo(data_batch_1)
coords, feats = ME.utils.sparse_collate(
    coords=[coords0, coords1], feats=[feats0, feats1])
```

* discretize the continous coordinates

```
sinput = ME.SparseTensor(
    feats=torch.from_numpy(colors), # Convert to a tensor
	    coords=ME.utils.batched_coordinates([coordinates / voxel_size]),  # coordinates must be defined in a integer grid. If the scale
		    quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE  # when used with continuous coordinates, average features in the same coordinate
			)
logits = model(sinput).slice(sinput)
```

* quantize the coords in dataset
	* use `ME.utils.sparse_quantize`, could use `return_index=True`

* defining dataloader
	* define the `colate_fn` to convert the input to proper output
	* `collate_fn = ME.utils.batch_sparse_collate / SparseCollation()`
		* collation - æ•´ç†æ ¡å¯¹

> in examples/training & examples/sparse_tensor_basics

* data of getitem() of the dataset, before using the `ME.utils.batch_sparse_collate`
	- discrete-coords  - [num_point, dim_coord]
	- out_feature      - [num_point, dim_feature]
	- label			   - [num_point] 
		* contains [N_CLASS] choices of elements

* process of the `ME.MinkowskiConvolution`
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201205092049.png)


## Survey

#### Deep Learning for 3D Point Clouds: A Survey

* 3 tasks: Shape Classification, Object Det & Tracking, Semantic Segmentation
* Data source: LiDAR, RGBD-camera, 3D-Scanners
* Data form: depth images, point cloud. meshes, volumeetric grids
	* point cloud: no discretization(Better representation)
* Challenges: 1. small scale dataset; 2. high-dimension; 3. unstructured natureA
* Available Datasets: 
	- ModelNet
	- ScanObjectNN
	- ShapeNet
	- ParNet
	- S3DIS
	- ScanNet
	- Semantic3D
	- ApooloCar3D
	- KITTI
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201129200502.png)
* Evaluation Metrics:
	- classification: Overall Acc(Acc of all test instances), meanAcc*(mAcc - acc for all classes)
	- Det: AP(Average Precision): area-under precision-recall curve
	- Track: precision & success
	- Segmentation: meanIOU(meanIntersectionOverUnion), meanClassAccuracy

* 3D Classification
	- Multi-view - convert unstructured point cloud to 2d images
		* project into multi 2d views, then fuse these features
		* Key: how to aggregate the multi-view feature
		* MVCNN: maxpool multiview feature, which causes information loss
		* MHBN: harmonized bilinear pooling
		* Relation network to discover relations between group of views
		* View-GCN: directed graph, graph nodes as multi-views
	- Volumetric - convert into 3d volumetric form
		* Key: 1st voxelize into 3d grids, then apply a 3DCNN
			* Preprocess + CNN: how to find good preprocess
		* Problems: unable to scale well to dense 3D-Data
			* OctTree is sometimes introduced
		* VoxNet: volumetric occupacy network
		* Deep Belief 3D ShapeNets: 
		* OctNet: use hierarchical octree to gen a bit-string representation
		* PointGrid: integrate point and grid representatio
		* 3DmFV: 3D grids further processed with 3D modified Fisher Vector
	- Point-based - directly without voxelization / projection
		* no explicit loss, more popular
		* category: 1. point-wise MLP 2. CNN-based 3. graph-based 4. hier-data structure 5. others
		* MLP: 
			* have permutation invariance with symmetric function
			* PointNet: 
			* DeepSets: summing all representation than transformation
			* PointNet++: hierarchical network to capture geometric from neighbourhood - (sampling/grouping/PointNet-based)
			* MoNet: PointNet-like
			* PAT(Point Attention Transform): represent point with its abs position and relative position with neighbours, then group shuffle attention used for get relations, then gumbel-set-sample used for learn hier fetature. 
			* PointWeb: improve feature by Adaptive feature adjustment
			* SRN(Structural Relation Net): learn structure feature 
			* SRINet: project point cloud to find rotation invariant features, then use pointnet backbone, then graph-based aggregation
			* PointASNL: adaptive sampling - furthertest point sampling methods + local-nonlocal module
		* Conv-based:
			* Continuous Conv:
				* the weights for neighboring points are related to the spatial distribution with respect to the center point
					* on spherical harmonic
				* conv could be viewed as a wieghted sum over a given subset
				* RS-CNN
				* DensePoint
				* KPConv
				* ConvPoint
				* PointConv
				* MCCNN
			* Discrete Conv:
				* the weights for neighbouring points are related to offsets with respeqct to center poin
		* Graph-based - each point is a vertex, the directed edge represents the neighbourhood
			* On Space-division
				* conv is MLP over spatial neighbours, pooling is adopted produce coarse graph
			* On Spectral-division
				* conv as spetral filtering, applying mult on graph laplacian matrix eigenvectors
		* Hier Data Structure 
			* apply on Hier-data structure(kd-tree & octTree)
			* feature aggregation from leaf to root node
	- Summary:
		* pointwise MLP often serve as a basic building-block
		* CNN & GNN are promising directions
			* how to handle irregular data structure is key
		* Efficiency is often a problem


* 3D Detection: - output the 3D Bounding Boxes
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130154412.png) 
	* Taxnomy: 
		* 1. Region-Proposal based(2-Stage)
			* Mult-view based methods
			* Segmentation-based methods(Use segmentation to remove background points, then raise high-quality proposals)
			* Frustem-based: generate 2-d proposal, then transform to frustem(å‡ ä½•ä½“) 3d ones
		* 2. Single-Shot
			* BEV-based (Bird-View)
			* Discreticized: for voxels
			* Point-based: diirectly on raw 3d pointcloud
	* Problems:
		* how to efficiently fuse multi-modality feature
		* extract robust representation
		* long-range detection poor
		* how to exploit texture information
	* Summary:
		* currentlly 2-stage outperform single by a large margin
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130161206.png)

* 3D Tracking - give the location of the obkject at the 1st frame, estimate its state in subsequent frames
	* use the rich geometric information to overcome drawbacks of of image tracking like: occlusion, illumination, variation
	* 3D version of Siamese

* 3D Scnen Flow Estimation
	* Given 2 point cloud, measure its movement, the 3d version of optical flow
	* point-based most rich information, however no explicit neighbourhood contains in the point representation
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130163319.png)

---

* representations forms
	* Projection/Discretization based could leverage the 2-d network architecture, dealing with structured data form. hoewver, information loss
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201130164056.png)

* Current Problems &  Future Directions
	1. imbalanced segmentation
	2. Dense point clouds
	3. spatial-temporal information for dynamic point clouds


---

> MingYuançš„Survey

* tasks:
	* 3D Shape Classification
	* Segmantions (Part/Semantic)
* Methods:
	* Volumetric-based - ä½“ç´ åŒ–
		* è½¬å˜ä¸ºvoxelï¼Œä¼šæŸå¤±ä¸€å®šçš„ä¿¡æ¯
		* ç»†çš„æ—¶å€™è®¡ç®—å¤æ‚åº¦å¾ˆé«˜
		* ä½“ç´ åŒ–çš„exampleï¼Œæ‰¾ä¸€ä¸ªå¤§1x1ç«‹æ–¹ä½“ï¼Œåˆ‡æˆå°å—
	* Multiview - 3D to 2D
	* Point-based: per point [x,y,z+N(feature)]
* PointNet(pointwise-MLP) - directly apply deep learning on points)
	* point properties: unordered & invariant to geometric transform 	* facing orderless: symmetric computing  - å…·æœ‰å¯¹ç§°æ€§çš„è¿ç®—
	* T-Net: å­¦ä¹ å‡ºä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œä¸ºäº†æ‘†æ­£ï¼Œç›®çš„æ˜¯åšå¯¹é½
		* T-Net + MLP
	* æ²¡æœ‰ç‚¹ä¹‹é—´çš„è”ç³»
* PointNet++ 
	* åœ¨pointnetä¹‹å‰åŠ ä¸Šäº†ä¸€ä¸ªsamplingå’Œgrouping(éå¸¸å…³é”®)
			* sample - æœ€è¿œç‚¹é‡‡æ ·
			* grouping - K-è¿‘é‚» 
			* ç›¸å½“äºé™é‡‡æ ·
	* segä»»åŠ¡ä¸­æ˜¯ä¸€ä¸ªæ’å€¼
	* sampleçš„å¯èƒ½æ”¹è¿›æ–¹å‘
		* non-uniform sampling
		* ä¼šæœ‰hierçš„åˆ†æ”¯
* KPConv 
	* ä¸€ä¸ªâšªçš„å·ç§¯æ ¸ï¼Œé‡Œé¢å›ºå®šçš„ä½ç½®æœ‰ä¸€äº›ç‚¹ï¼Œå®é™…å·çš„æ—¶å€™å°†å¯¹åº”çš„ç‚¹æ”¾åœ¨åœ†å¿ƒï¼ŒæŒ‰ç…§è·ç¦»å§å…¶ä»–çš„weightåŠ æƒæ±‚å’Œç”Ÿæˆä¸€ä¸ªæ–°çš„weightä¸xä¹˜
* PointCNN
	* Concat(MLP(position),color)x(MLP(x) as transform)
* PVCNN
	* Point + Voxel
		* 2æ¡æ”¯è·¯,voxelæœ¬èº«ä¹Ÿå¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªsampleï¼Œvoxelå…ˆä½“ç´ åŒ–ç„¶åå·ç§¯ï¼Œç„¶åå†åä½“ç´ åŒ–
	* Voxel-basedæœ‰å¾ˆå¤§å†—ä½™
	* æœ¬èº«æ²¡æœ‰é™é‡‡æ ·
* Grid-CNN
	 

---

> [Review: deep learning on 3D point clouds](http://arxiv.org/abs/2001.06280)

* mainly focus on directly taking in the point cloud 
* Challenges:
	1. Irregualr: dense and sparse regions
	2. Unordered: within the same set, no order
	3. Unstructured: No grid, each point independent
* Structured Grid-based Learning
	* analogy from Conv on 2-d images
	1. Voxel-based: discretize into binary grids(has point or not)
		* high memory consumption due to sparsity
	2. Multiview-based: squash the 3-d model into many 2-d grids(as images)
	3. Lattices-based
* Directly operate on pc
* pointnet:
	* plain MLP, no local information
	* simple max-pooling
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201206225052.png)
* how2 get local information: sample/group & mapping - often approximated by a MLP
	* sample: random sample, K-farthest sampling(sample M times, each points is the farthest points of former), uniform sampling, gumbel sampling
		* FIND THE CENTROID
	* group: K-nearest sampling group them into a local branch
		* GROUP NEARBY POINTS
	* mapping: mlp+max-pool to retain symmetric for orderless
		* MERGE THEIR INFORMATION
* donot extract local info
	* pointnet++: hier applu pointnet
	* VoxelNet: sample T points for each voxel, use their location mean(centroid) into FCN
	* Pointwise Convolution
* explore local info
	* PointCNN: start from pointnet++, apply X-transform on K-nearest points, permute inputs
	* GeoCNN: weighted feature aggregation based on their distance from centroids
	* PAT: gumbel sampling for sample, pointnet for computing + multi-head attention
		* permutation invariant & robust ot outliers
* Some dont follow MLP: thinks it neglects the prior of geometry of PC & large param size:
	* Step func + Taylor expansion
* graph-based: input graph {V,E} V points, E as a {V,V} represent edges

> YiLi: 3D Deep Geometrical Process

* Sensing to perception:
	* segmentation -> assign attribute
* 3-d datasets for objects
	* Rich attributes* - semantic category, pose, part, optical material
	* ShapeNet - CAD models - >3M models; 4k Object classes
	* ShapeNetCore - spatially aligned and with pose label
	* PartNet
* 3-d dataste for scne undestanding
	* SceneNet
	* ScanNet
	* Waymo / Kitti / Apollo
* 3-d dataset representations:
	* multiview-image
	* depth map
	* volumetric occupacy
	* point cloud (*)
	* polygon mesh (*)
* GeoNet - CVPR2019
	* capture the surface topology representation - geodesic-aware
	* (task of point cloud upsampling?complete)
	* geodestic distance? 
		* shortest path along the 2d surface
	* a very good example of how to acquire the low-level features of the point cloud(i didnt see in many pointCNN)

* Learning on graph/mesh:
* SyncSpecCNN: 
	* Previous work: SpectralCNN - mult in the spectrum, apply IFFT to get the convolution in time division
		* fourier bases corressbond to local features
	* this paper: sync the basis
	* follow-up work: TextureNet
		* orientation - æ–¹å‘ï¼Ÿ
		* ä¹‹å‰ä¸å¸¦æ–¹å‘æ€§(ç”±äºIFFT)åœ¨ç©ºåŸŸä¸Šä¸€èˆ¬æ˜¯æ²¡æœ‰æ–¹å‘æ€§
		* kernel-weights orientational symmetric

* Tasks:
* 3-d instance detection & segmentation:

* GSPN: 
	* RPN: region proposal, 3-d region proposal fails
		* 3-d box may not be suitable, free-form shape proposal
	* Objects of the same class has true physical scales, less afftected by lighting/acculusion
	* VAE - generate the object conditioned at the input scene
* Differnet between indoor / outdoor
	* indoor: dense / outdoor: sparse Lidar
	* outdoor: has domain gap
		* view it as a geometrical completion task

* More finer tasks into parts:
* Primitive Fitting:
	* ç”¨ä¸€äº›åŸºç¡€ä»¶å»è´´åˆä¸€ä¸ª3d model
	* SPFM
		* make differentiable Pattern for base elements
* Motion Segmentation - articulated - é“¾æ¥å¼çš„ç»“æ„(æ¯”å¦‚âœ‚)
	* how points change from the 1st state to 2nd state
	* category / domain invariance?
	* hard to annotate
	* discover correspondences between 
	* (Pose Estimation?)

* Future Dicrections:
	* 2-d surface manifold in 3d (Non-encludian)
	* various representations - explicit representation(?)
		* potentially combine themï¼Ÿ
	* Multi-modal - in autonomous driving
	* 3-d generative model
	* Embedded AI
	



## Papers

- [PointNet++](http://arxiv.org/abs/1706.02413)

* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201207151936.png)
* MultiscaleGrouping - 
	* dropout inputs points for each instance 
	* generate differnet extend of sparsity for input
	* during training, using all points
 * Multi-resolution grouping:(computationaly efficient)
	* åœ¨å¤„ç†ä¹‹å‰å’Œä¹‹åçš„embeddingä¸Šåˆ†åˆ«åšgroupingï¼Œå¹¶ä¸”å°†ä¸¤ä¸ªè¾“å‡ºconcatèµ·æ¥
 ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201207162501.png)

---

- [KPConv](http://arxiv.org/abs/1904.08889)

* Point cloud - Sparse & Unordered
	* similar but also different from grid: (same) - spatially localized
* Methods towards the point cloud:
	1. Grid-based project sparse 3D data on regular shape(voxelize?), so conv could be defined more easily
		- voxelization is projection in eculidean space
	2. Directly Apply MLP on the points
		- original pointnet dont model the local representation
		- other hierachical methods use MLP as conv, author argue hard to converge
	3. Graph Conv
		- conv on graphh is equivalent to mul on its spectral representation
		- represent edge connections instead of edge relative positions
		- combines feature on local patches, however, invariant to deformations(å˜å½¢-bad)
	4. Other Point Conv 
		- PointwiseCNN: kernel weights at voxel bin(more like grid-based methdos)
		- SpiderCNN: family of polynominal function applied weighting for each neighbour, spatially inconsistent
		- FlexCNN: linear functions
		- PCNN: also use point to carry kernel weights
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201208224803.png)
* Also Propose the deformabel version of this conv
	- rigid / deformable
	- with learnabel local shift
	- extra regularization is applied to avoid empty set
* Favors radius neighbour instead of KNN during grouping
	* consistent spherical domain
* reception field is a ball
	* inside K points cntains kernel weights
* Aside from weight, a correlation is also applied:
	* use linear correlation max(0, 1-||y_i - x_k||/\sigma)A
	* didnt use gaussian correlation for simplicity
* "fitting reg" - penalize the kernel point and the cloest neighbour
* "reulsive reg" - penalize points have overlap region
* Deformable
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201208235239.png)

* Point cloud registration task?
	- ç‚¹äº‘é…å‡†ï¼Œå¯¹é½ä¸¤ä¸ªç‚¹äº‘ç©ºé—´çš„å˜æ¢ï¼ˆæ„Ÿè§‰æœ‰ç‚¹pose estimationçš„æ„å‘³ï¼‰

---

- [PointCNN]()

* X-Conv
	* ```F^* = [MLP(p_{center} - p_i), feature]```
	* ```X = MLP(p_center - p_i)``` of dim [k,K]
	* ```Y = X*F^*```
	* nearby points `projected/aggregated` into representative point P(sampled)
* Architectural resemble the grdid-based CNN
	* KxK local patched -> K neighbouring points around representative points
	* X-conv instead of conv 

---

- [PointwiseCNN]()

* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201209141254.png)
	* conv kernel centered at each point
	* radius value
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201209141845.png)


- [SpiderCNN]()

* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201209141029.png)
	* weighted distance gap

---

- [PVCNN]()

* Voxel-based methods - regular and good memory locality / need high reso to not lose information -> big memory consumption
* Point-based methods - irregular memory access / huge dynamic overhead
* Combine both - PVConv(point-voxel conv)
	* disentangle fine-grained feature transformation / coarse-grained neighbourhood aggergation(low reso voxel grids)
* Method:
	1. normalize the coords 1st before voxelization
	2. Voxelization: average all the features falls into the grid box
	3. Feature aggregation: 3-d volumetric conv
	4. Devoxelization: (plain) - nearest-neighbour inerpolation, will result in all points in 1 voxel share the same value
		* so tri-linear interpolation
	5. Also leverage a MLP to extract point-based feature
 
---

- [PointContrast]()

- é¦–å…ˆæœ‰ä¸€ä¸ªcase studyï¼Œå‚ç…§åˆ«äººç”¨fully-supervised(ç”¨å…¨labelæ˜¯ä¸ºäº†è¡¨ç¤ºtransferçš„upper bound) ShapeNet(å•ä¸ªç±»åˆ«çš„ç”Ÿæˆçš„3dæ¨¡å‹)åšé¢„è®­ç»ƒï¼Œç„¶åè¿ç§»åˆ°S3DIS segmentation

- Dataset - downstream tasks
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201216164014.png)

- FCGF-based

FullyConvNetwork Arch(U-Net full-resolution output,åŒæ—¶è¾“å…¥ä¸ºæ•´ä¸ªpcï¼Œä¸éœ€è¦cropæˆå¤šä¸ªå—ï¼Œä¿ç•™äº†ä¿¡æ¯)
å­¦ä¹ æ–¹å¼æ˜¯Point-levelçš„Metric Learning
taskæ˜¯domain-specificçš„pointcloud registration task

rigid transform - rotation, translation, scaling

- Loss

PointInfoNCE
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201216205853.png)

Pæ˜¯æ‰€æœ‰positive match pairsï¼Œåªç”¨åˆ°äº†positiveçš„pairï¼Œè€Œæ²¡æœ‰ç”¨åˆ°épositiveçš„pairï¼›ç”¨çš„dictionary learningçš„æ€æƒ³ï¼Œkeyå¯¹åº”ä¸Šçš„æ˜¯positiveï¼Œå¯¹åº”ä¸ä¸Šçš„æ˜¯negã€‚å¤–é¢å¥—äº†ä¸€ä¸ªsoftmaxï¼Œä¼šæ›´åŠ stable
 
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201216114508.png)

- PointContrastå®Œæ•´çš„è¯´æ˜äº†Unsupervised Pre-trainingçš„ä½œç”¨ï¼Œ

æ­¤å¤–ï¼Œå¯¹ScanNetæœ¬èº«ä¹Ÿæœ‰accçš„å¢ç›Šã€‚å³ä½¿ä¸trasnfer Unsupervised Pretrainingä¹Ÿæœ‰ä½œç”¨(ScanNetv2 to æœ¬èº«4)ï¼Œä»¥åŠtransferçš„Unsupervisedå¯¹æ¯”supervisedä¸å·®å¤ªå¤š(ScanNetV2->S3DIS)

- Architecture

Sparse Residue U-Net(SR U-Net) - Res34

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201216211620.png)
?: pointcontrastçš„preprocessï¼Œå¦‚ä½•è·å¾—point pair
å¯¹æŸä¸ªscene xï¼Œä»ä¸¤ä¸ªviewè·å–x1ï¼Œx2ï¼Œsubsample every 25 frames, å°†ä¸¤ä¸ªframe alignåˆ°åŒä¸€ä¸ªä¸–ç•Œåæ ‡(collect 2 point clouds in a pair of at least 30% overlap?)

- Exp

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201216212535.png)
å¯¹äºShapeNetåˆ†ç±»ï¼Œåªéœ€è¦1%çš„labelç›´æ¥è®­å°±å¯ä»¥æœ‰66%ï¼Ÿ

Sun-RGBD Detection task - modify archietcture to VoteNet

- Ideas:

use holistic scene is not working, ä¸å†ä»ä¸¤ä¸ªviewåˆ†åˆ«åštransformï¼Œè€Œç›´æ¥ç”¨reconstructed point cloudå»applyä¸¤ä¸ªtransformï¼Œä¼šæ˜æ˜¾æ‰ç‚¹ã€‚

---

- [FCGF-Fully Convolutional Geometric Features]()


* ğŸ”‘ Key:         
	* uses metric learning loss + fullyConv Backbone
* ğŸ“ Source:      
* ğŸŒ± Motivation: 
	* Towards the low-level geometric feature extraction problem - task as registration/reconstruction & tracking 
	* earlier works use limited reception field - 3DCNN
* ğŸ’Š Methodology: 
	* Metric Learning:
		* the hardest contrastive - hardest triplets
		* 2 Constraints: similar features should be close and dissimilar ones should be a margin away
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201216214619.png)
		* use hard negative mining
* ğŸ“ Exps:
	* Datesets: 3D match on KITTI, find GPS gt is noisy, use ICP to refine the alignment
	* Generate point pairs, then sample the positive/negative pair
* ğŸ’¡ Ideas:  
	 
---

- [Searching E3Dcient 3D Architectures with Sparse Point-Voxel Convolution](http://arxiv.org/abs/2007.16100)

* ğŸ”‘ Key:         

1. Prpose efficient 3d block: SPVConv(Sparse-Point-Voxel Convolution)
2. Design automl flow(search space)

* ğŸ“ Source: MIT HAN

* ğŸŒ± Motivation: 

* ğŸ’Š Methodology: 

1. SPVConv

Revisit Point-Voxel Conv(Proposed in the PVCNN): Point-Voxel Convolution:(Coarse Voxelization) - originally proposed to reduce memory consumption(reduce the irregular memory access / improve locality)
However, some small instance occupy very few voxels, cant be learnt well(could use fewer piece sliding window but huge cost)

Revisit Sparse Conv(Minkowski Net): its core idea: skip the non-activated region(1st finding the active map between input/output points)
However, have to agressively downsample to gain good reception field, then the reso will be too coarse

These methods sacrifeices the resolution to gain efficiency

use point-voxel conv: 
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201223141619.png)

keep the sparse voxelized tensor(only store non-zero voxels), and the full point cloud, transform between them with voxelization/devoxelization
Voxelization: 3d coordinates - floor(x/v) (v is the voxel size); the feature are the means of all points fallen into em
feature aggregation with sparse-voxel-conv with residue SparseConvolution, then transform(devoxelize) back to point representation: interpolate em with 8 neighbour grids

2. NAS Design Space: fine-grained channel num & Elastic network depth

not coarsed grained channel size(expansion ratio in ResNet) O(n) channel space: crop the first c channels sampled
since the 3d model is more memory-bounded, search for depth, sampled depth N, only n layers(the deeper layers may be poorly trained, applied progressive shrinking to avoid)
small kernel matters, but not supported yet, keep all to be 3. 

Evolutionary searching. 


* ğŸ“ Exps:

task: Semantic KIITI Seg, then transfer to Det


* ğŸ’¡ Ideas:  

1. since point cloud has sparse nature, using dense volumetric methods are inefficient(some efforts already made: octree to reduce memory footprint, MinkowskiNet proposes Sparse Convolution)
2. 3d medical data understaning resembles more with 2-d image, but not like 3d scene unserstanding on point cloud

---

- [4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks](http://arxiv.org/abs/1904.08755)

* ğŸ”‘ Key:         

mainly focus on using temporal information 3d + video 
however true contribution - Minkowski Engine & MinkowskiNet - Sparse Voxel Conv

* ğŸ“ Source: Christopher Choy - StanfordVL

* ğŸŒ± Motivation: 
* ğŸ’Š Methodology: 

- Generalized SparseConv

* ğŸ“ Exps:

Seg on ScanNet & S3DIS

improvement much

* ğŸ’¡ Ideas:  

---

* ğŸ”‘ Key:         
* ğŸ“ Source: 
* ğŸŒ± Motivation: 
* ğŸ’Š Methodology: 
* ğŸ“ Exps:
* ğŸ’¡ Ideas:  


