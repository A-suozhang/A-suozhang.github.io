---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      Contrastive Learning            # æ ‡é¢˜ 
subtitle:   æ€ä¹ˆæ„Ÿè§‰å¥½åƒæ›¾ç»å†™è¿‡â€¦ 
date:       2020-12-12            # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/2020/street3.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - survey
---

# Contrastive learning

> ref the [Blog-ContrastiveSelfSupervisedLearning](https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html)è¿™ä¸ªpostå†™çš„éå¸¸å¥½â€¦æœ‰æœºä¼šè¦å‘ä»–å­¦ä¹ ï¼Œç›®å‰è‡ªå·±çš„postç¡®å®éƒ½æ˜¯éšæ‰‹è®°çš„

- The Gelato's Bet

unsupervised method could beat R-CNN on PASCAL by 2015 (finally recently MoCo achieve that)

- Self-Supervised Learning

1. use the data itself as the supervised signal
	* the underlying data distribution has richer information than sparse labels
	* for high-dimensional problems(such as RL), require label is harder
2. Main flow: Generative / Predictive / Contrastive
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201212185232.png)
traditional generative methods, focusing on the reconstruction error on pixel space, not abstract latent space
3. Cur self-supvesied method on unlabeled imagenet + linear classifier could outperform AlexNet(Data-efficient CPC 2019) * Contrastive training on ImageNet beats pre-training from ImageNet

- Contrastive Learning 

Key: Score(f(x),f(x^+)) >> Score(f(x),f(x^-));
x is the `anchor point`
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201212190040.png)
the denominator(åˆ†æ¯) contains 1 positive & N-1 negative values, dot product as score function, N-way softmax classifier
called `infoNCE Loss`, `multi-class n-pair loss` or `ranking-based NCE`

* Scheme

4 Main Component: Data aug -> encoder -> Pretext Task -> Contrastive Learning

Mocoå°†å…¶æŠ½è±¡ä¸ºäº†ä¸€ä¸ªdictionary lookupçš„é—®é¢˜,å½’ç±»äº†å‡ ç§Scheme
1. ä¸¤ä¸ªencoderï¼Œåˆ†åˆ«äº§ç”ŸPositiveå’ŒNegativeçš„æ ·æœ¬;gradåŒæ—¶å›æµä¸¤ä¸ªencoder
2. å•ç‹¬encoderå¤„ç†posæ ·æœ¬ï¼Œnegæ ·æœ¬ä»ä¸€ä¸ªmemory bankä¸­å–å¾—
3. ä¸€ä¸ªencoderå’Œå¦å¤–ä¸€ä¸ªmomentum encoderå¯¹encoderåštemporal ensembleæ¥å¤„ç†neg sample

* Ideas:
1. ä¸å»pixel-levelçš„é‡æ„æ•°æ®ï¼Œæ˜¯å› ä¸ºå®é™…çš„featureå¯èƒ½æ›´åŠ é«˜å±‚ã€‚
2. InfoNCEå…¶å®æ˜¯åœ¨ç”¨äº’ä¿¡æ¯é‡æ¥è¡¡é‡ç‰¹å¾çš„å¥½å
3. MoCoçš„æƒ³æ³•æ˜¯å¢åŠ è´Ÿæ ·æœ¬çš„ä¸ªæ•°ï¼ŒåŒæ—¶ä¸ä¸€å‘³å¢åŠ memory bank

### Papers

- [DIM-Deep InfoMax-2018]()

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201212190419.png)
the contrastive task: classify wheter a pair of global and local feature are from the same image or not 
f(x) as anchor - global feature: output of the decoder of CNN representation
f(x+), f(x-) local feature: intermediate output of the CNN encoder, if from the same image, f(x+)
many follow-up works on graph & RL, also augment multi-scale

- [CPC - Contrastive Predictive Coding]()

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201212190904.png)
could apply on any form of data(view it as a seq)
focus on learning the `slow feature`, which don't change quickly along time, while discarding the `local representation`
contrastive task: {x1,...,xn} a seq of data, xt is the `anchor point`, x{t+k} is the postive sample(usually k>1 as the fig shows), x{t\*} randomly sampled from the seq is the neg sample 
an auto-regressive encoder network is applied to encode the historical content

- [Learning Invariances with Contrastive Coding]()

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201212192146.png)

focus on using contrastive learning to improve invariance in feature space(which is advantage of the CL), if we want features that are invariant to transformation T(x), for anchor data x, T(x) is positive sample & T(x') is negative sample, AMDIM & CMC focus on this scheme

- [Moco]()

> Sclaing the number of the negative sample

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201212192801.png)

(note that grad donot flow back through the momentum encoder of MoCo)
Insights: CL works better with more negative samples, usual form of CL, the grad flow back through encoders of both positve/negative samples, so the negative sample is restricted to size of batch size.
Moco(Momentum Contrast) solve this by maintaining a queue of negative samples, no using the grad to update the negative encoder, preiodically update the negative encoder with momentum update.
(the postive sample forward through the encoder, and the neg sample forward throught the momentum encoder, generating the loss, the grad backward through the encoder. then the momentum encoder is weighted sun of the encoder) 

---

* V2

å‚è€ƒSimCLRï¼Œæ›´æ–°äº†éçº¿æ€§å±‚ä»¥åŠä¼˜åŒ–äº†æ•°æ®å¢å¼ºæ–¹æ³•

- [SimCLR - NIPS2020]()

æ›´å¤§çš„batch-size
æå‡ºåœ¨encoderååŠ ä¸€ä¸ªé¢å¤–çš„éçº¿æ€§æ˜ å°„ W(ReLU(Whi)),å‘ç°encoderçš„ç»“æœä¼šä¿ç•™ä¸æ•°æ®å¢å¼ºç›¸å…³çš„ä¿¡æ¯ï¼Œç”¨ä¸€ä¸ªéçº¿æ€§å±‚å»æ‰è¿™äº›ä¿¡æ¯(è¯¥å±‚åªåœ¨æ— ç›‘ç£è®­ç»ƒä¸­ä½¿ç”¨)
æ¢ç´¢å¹¶ä¼˜åŒ–äº†æ•°æ®å¢å¼ºæ–¹æ³•çš„ç»„åˆæ–¹å¼

* V2

ä¹Ÿå‚è€ƒmocoåŠ ä¸Šäº†momentumï¼Œä»¥åŠæ¢arch

- [SwAV - NIPS2020 - (FAIR)]()

motivation: å¯¹æ¯”å­¦ä¹ éœ€è¦neg sampleè¿›è¡Œæ¯”è¾ƒï¼Œæ¶ˆè€—è®¡ç®—æ—¶é—´ä»¥åŠå†…å­˜
key: é¦–å…ˆå¯¹å„ç±»æ ·æœ¬è¿›è¡Œèšç±»ï¼Œç„¶ååŒºåˆ†æ¯ç±»çš„ç°‡
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201213110456.png)
å¯¹äºä¸€å¼ å›¾ç‰‡ï¼Œencoderäº§ç”Ÿäº†embedding z,å®šä¹‰äº†Kä¸ªèšç±»ä¸­å¿ƒCï¼ŒZä¸Cç»“åˆäº§ç”Ÿæœ€ç»ˆçš„embedding Qï¼ŒQçš„å€¼è¡¨ç¤ºç¬¬Kä¸ªèšç±»ä¸­å¿ƒä¸ç¬¬bä¸ªæ ·æœ¬çš„ç›¸ä¼¼åº¦(Lossä¹Ÿæ˜¯å–çš„å†…ç§¯)
æ­¤å¤–æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œå¯¹ä¸åŒåˆ†è¾¨ç‡çš„viewè¿›è¡Œmix


- [BYOL - DeepMind]()

Motivation: ä¸ä½¿ç”¨å¯¹æ¯”è¿™ä¸€scheme,Representation Learning è§£æ„ä¸ºç”¨ä¸€ç§viewå»é¢„æµ‹å›¾å½¢çš„å…¶ä»–viewï¼Œè¿™æ ·å°±å¯èƒ½ä¼š`collapse`,å…¨éƒ¨æ”¶æ•›åˆ°æŸä¸€ç§è¡¨ç¤ºï¼Œè‡ªå·±å»é¢„æµ‹è‡ªå·±,Contrastive Learningå°†é¢„æµ‹çš„é—®é¢˜è½¬åŒ–ä¸ºäº†å¯¹æ¯”çš„é—®é¢˜ï¼Œå¼ºè¿«æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œé¿å…äº†åç¼©,BYOLå°è¯•ä¸ç”¨å¯¹æ¯”(Neg Sample),å…¬ç”¨encoderï¼Œå°†MSEä½œä¸ºæŸå¤±ï¼Œç¼©å°ä¸åŒviewä¹‹é—´çš„è·ç¦»ï¼Œä¼šå¼•å‘åç¼©ã€‚ä½†æ˜¯å¦‚æœå°†å…¶ä¸­ä¸€ä¸ªencoderå›ºå®šä¸‹ï¼Œå¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šé¿å…åç¼©ã€‚åç»­å‚è€ƒmomentumçš„æ€æƒ³æ”¹è¿›

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201213112341.png)

ä¸Šä¾§Onlineï¼Œå«æœ‰æ¢¯åº¦æ›´æ–°ï¼Œä¸‹ä¾§æ˜¯Targetï¼Œä¸å»æ›´æ–°æ¢¯åº¦ã€‚ä¼˜åŒ–çš„ç›®æ ‡æ˜¯ç”¨Onlineå»é¢„æµ‹targetçš„è¡¨å¾(MSE Loss)ã€‚Targetæ˜¯onlineéƒ¨åˆ†encoderçš„æ»‘åŠ¨å¹³å‡,æœ€åé¢„æµ‹çš„æ—¶å€™åªç”¨onlineéƒ¨åˆ†(å¥½åƒMeanTeacherçš„æ¶æ„)
åç»­çš„å·¥ä½œå‘ç°è¿™ä¸ªMLPä¸­çš„BNéå¸¸å…³é”®ï¼ŒBNåœ¨æ˜¾ç¤ºçš„åšå¯¹æ¯”ï¼Œé™¤å»batchå†…ç›¸åŒçš„éƒ¨åˆ†
ä¸ä¾èµ–è´Ÿæ ·æœ¬ä¹‹åï¼Œå…¶å¯¹æ•°æ®å¢å¼ºå°±æ›´åŠ é²æ£’äº†

- [SimSiam]()

> Kaimingé’ˆå¯¹å­ªç”Ÿç½‘ç»œçš„follow-up work

followäº†BYOLçš„æ€è·¯ï¼Œæ‰¾åˆ°äº†collapseçš„é¿å…æ–¹æ³•æ˜¯stop-gradient
å·¦ä¾§çš„encoderäº§ç”Ÿz1ï¼Œç»è¿‡MLPä¹‹åè¾“å‡ºp1ï¼Œå³ä¾§ç”Ÿæˆz2ï¼Œè®¡ç®—p1ä¸z2çš„Cosine Similarityï¼Œå·¦å³è°ƒæ¢ï¼Œè®¡ç®—p2ä¸z1çš„Cosine Similarityï¼Œå¹¶ä¸”æœ€å¤§åŒ–ä»¥ä¸Šä¸¤ä¸ªä½™å¼¦è·ç¦»çš„å’Œï¼Œå³ä¾§encoderä¸€ç›´ä¸å›ä¼ æ¢¯åº¦
ä¸ºä»€ä¹ˆä¼šworkï¼Œå‚è€ƒ[Andy's Blog](http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/-Vtl_8nND7WCPLdL5bNlMw)

---

- [SimSiam](http://arxiv.org/abs/2011.10566)

* ğŸ”‘ Key:         

new flow in self-supervised learning, simply using the siamese flow(max the similarity of a image between 2 augmentations)
without relying on: 1. neg pair 2. large bs 3. momentum encoder

* ğŸ“ Source: 
* ğŸŒ± Motivation: 
* ğŸ’Š Methodology: 

simple scheme: same encoder network takes in 2 augmented image, one follows a MLP, the other uses the stop-grad, then maximize their similarity
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201226085919.png)
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201226091728.png)


Stop-gradient is critical, as formula above, the encoder's weights are not updated through grad of z1,z2 but from p1, p2

Min their negative cosine similarity
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201226091547.png)

* ğŸ“ Exps:
* ğŸ’¡ Ideas:  

Vanilla Simamese network(serve for comparing entities) have a trivial case of all outputs falling into a constant - `collapse`(contrastive learning such as SimCLR aims to solve it)
tricks like momentum encoder / use neg pairs / online clustering(SwAV)


Comparison with other Siamese methods
![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20201226094617.png)



