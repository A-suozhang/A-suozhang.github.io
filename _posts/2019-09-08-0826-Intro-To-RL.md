---
layout:     post                    # 使用的布局（不需要改）
title:      An Intro to RL              # 标题 
subtitle:   Think Like An Agent    #副标题
date:       2019-09-08              # 时间
author:     tianchen                      # 作者
header-img:     #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
    - 综述
---
# Intro To RL
* 之前对RL的理解一直是只言片语，没有“系统地入个门”（雾），之后把RL相关的一些碎片知识都整理到这个post里面把

## 核心四元结构
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908082919.png)
* 对Agent（智能体）从环境中学习的过程建模    

## 分类辨析 A Simple Taxonomy
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908083740.png)
* Model Free & Model Based 理不理解所处的环境
    * 不去对环境建模，直接接受真实世界的反馈 - **Model-Free**
        * *Q-Learning,Sarsa,Policy Gradients*
    * 智能体自己建立一个模型去模拟真实世界对自己行为的反馈 **Model-Based**
        * 相比于Model-Free多了一个建模的工具，多了一个沙盒模拟的过程
        * 相比Model-Free的方法，多了“想象力”和“推测”，Model-Free只能等待真实世界的反馈，一步步进行；而Model-Based的方法可以预测出未来几步，并且选择一个更加pleasant的预测来执行（思考一下围棋，看几步）
* Policy-Based & Value-Based
    * Policy-Based 基于概率的方法：通过分析所处的环境，对所有action（决策）给一个发生概率
        * Policy Gradient
    * Value-Based 基于价值的方法，输出所有action的价值然后选择一个最好的    
        * Q-Learning,Sarsa
    * 两者结合
        * Actor-Critic: Actor基于Policy，基于概率给出动作，然后Critic对所作出的动作给出价值（相比于单纯的Policy Gradient加速了学习过程）
    * ❓: 有点像Hard/SoftMax?
* 回合更新/单步更新
    * 传统的比如Monte-Carlo Leaning，Policy Gradient（OLD），一整个回合结束才会总结更新
    * Q-Learing，Sarsa，Policy Gradient（NEW），每一步都更新，*效率更高*，现在大部分的都是
* On-Policy/Off-Policy
    * 在线： Sarsa
    * 离线： QLearning


            

## 关键定义
* RL的**核心**：
    1. 如何定义采取动作之后的Reward的评价标准（reward的设计不仅要符合最后我们所需要达到的目的，还需要注意在学习过程中不同action的比较）
        * 本质是需要让agent学到最好的状态转移矩阵
    2. 我们如何更新操作
    > Q-Learning中，会有一张很大的Table，记录了所有state对应的所有action的值，我们按样本学习每个单元格，使最后的Reward最大
    * 基于Value的方法 - DQN
        * 但是由于表过度大，DQN用一个NN来查表
        * DQN通过experience reply(解决机器学习独立同分布的问题) ❔
        * 用一个网络估计Q和现实Q之差，作为loss
    * 基于Policy的方法
        * 直接优化Policy简单明快（NN直接输出Policy），问题被转化为如何更新NN参数
            * 有stochastic / deterministic
                * Stochastic 需要采样，计算大  - PPO
                * Deterministic 已经证明过了策略梯度公式 - DPG/DDPG
    * 融合Value与Policy - Actor-critic方法 -> A3C
* State：对于现实世界情况的完全描述（往往是比较抽象的建模）
    * 用一个Tensor来作为*Representation*
* Observation与State相对应，是当前世界不完全的描述，但是一般会比较具体  ~~我不到啊我瞎猜的~~
* Action State-决策空间
    * 可能连续（机器人在实际场景中）可能离散AlphaGO
* Policy - a rule used by agent to decide which action to take ~~我好像想不到一个太好的中文翻译，决策方式？~~
    * 可以是Deterministic（Hard） *Max*
    * 也可以是Stochastic （Soft） *Soft*
        * Categorical Policies
        * Diagonal Gaussion Policies

    


## Q-Learning
* 基于表格的方法（表格的XY轴分别是当前state和当前时刻所作出的决策；里面的值是潜在奖励-Q值）
    * 这个表格*Q-table*就像是个状态转移矩阵：决定了每一个状态需要采取什么样的策略
* 那么Q值是怎么定义和更新呢？
    * 如果出现了新的state（插入全0或者特定初始化）拓展QTable
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908104247.png)
* 每次更新都会用到Q的真实值和预测值
* alpha是学习率，表示学习速度 / gamma是对未来reward的衰减值（深谋远虑的程度）
* Q-Learning是Off-Policy的（Q-Table存储了之前的经验，其更新可以不基于正在经历的经验）
    * 但是我们并没有利用到Q-Learning OFF-policy的特性，在DQN中会有用到

## Sarsa(State-Action-Reward-State-Action-这个名字也太魔性了...) 
* 前向推断方式类似于Q-Learning（都是从一个QTable中选取最大Q值的策略）
* 更新方式不同：
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908135538.png)
* 可以理解为对差距的定义不一样：
    * Q-Learning是 “下一状态的最佳Q” - “当前状态Q” （可以看做观察了下一状态会怎么样，再做判断（还有转机））
    * 而Sarsa直接对 “下一状态的Q” - “当前状态的Q” （可以看做已经做出了决定到了下一状态，不再有回旋余地）
    * Q-Learning在当前时刻只会定当前时刻的Action（决定了下一时刻的State）但是不会确定下一时刻的Action（需要看MaxQ(s',a')）
    * Sarsa的每一时刻都决定了当前时刻的Action以及下一时刻的Action
        * 每一时刻参与学习的参数有(State-Action-Reward-State'-Action')
* 这么看Q-Learning更加**贪婪**（体现在最大化maxQ）一些，如果两个模型都表现得好的话，Q-Learning能找到最近的路线，而Sarsa则会表现的相对**保守**（因为不保守的都死了）
* Sarsa属于*单步更新*的算法

## Improved-Sarsa -> Sarsa(lambda)
* 加速的Sarsa，主要就是把*单步更新*改成了*N步更新*，能够避免“原地打转”的情况
    * lambda在[0,1] - 0就退化为简单Sarsa，1就变成直到获取了reward才更新
* 






## Ref
* [OpenAI SpinningUp](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#id20)
* [Morvan's Blog](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-1-A-q-learning/) 