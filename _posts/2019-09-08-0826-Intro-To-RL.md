---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      An Intro to RL              # æ ‡é¢˜ 
subtitle:   Think Like An Agent    #å‰¯æ ‡é¢˜
date:       2019-09-08              # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/bg-street.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - DL
    - ç»¼è¿°
---
# Intro To RL
* ä¹‹å‰å¯¹RLçš„ç†è§£ä¸€ç›´æ˜¯åªè¨€ç‰‡è¯­ï¼Œæ²¡æœ‰â€œç³»ç»Ÿåœ°å…¥ä¸ªé—¨â€ï¼ˆé›¾ï¼‰ï¼Œä¹‹åæŠŠRLç›¸å…³çš„ä¸€äº›ç¢ç‰‡çŸ¥è¯†éƒ½æ•´ç†åˆ°è¿™ä¸ªposté‡Œé¢æŠŠ

## æ ¸å¿ƒå››å…ƒç»“æ„
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908082919.png)
* å¯¹Agentï¼ˆæ™ºèƒ½ä½“ï¼‰ä»ç¯å¢ƒä¸­å­¦ä¹ çš„è¿‡ç¨‹å»ºæ¨¡    

## åˆ†ç±»è¾¨æ A Simple Taxonomy
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908083740.png)
* Model Free & Model Based ç†ä¸ç†è§£æ‰€å¤„çš„ç¯å¢ƒ
    * ä¸å»å¯¹ç¯å¢ƒå»ºæ¨¡ï¼Œç›´æ¥æ¥å—çœŸå®ä¸–ç•Œçš„åé¦ˆ - **Model-Free**
        * *Q-Learning,Sarsa,Policy Gradients*
    * æ™ºèƒ½ä½“è‡ªå·±å»ºç«‹ä¸€ä¸ªæ¨¡å‹å»æ¨¡æ‹ŸçœŸå®ä¸–ç•Œå¯¹è‡ªå·±è¡Œä¸ºçš„åé¦ˆ **Model-Based**
        * ç›¸æ¯”äºModel-Freeå¤šäº†ä¸€ä¸ªå»ºæ¨¡çš„å·¥å…·ï¼Œå¤šäº†ä¸€ä¸ªæ²™ç›’æ¨¡æ‹Ÿçš„è¿‡ç¨‹
        * ç›¸æ¯”Model-Freeçš„æ–¹æ³•ï¼Œå¤šäº†â€œæƒ³è±¡åŠ›â€å’Œâ€œæ¨æµ‹â€ï¼ŒModel-Freeåªèƒ½ç­‰å¾…çœŸå®ä¸–ç•Œçš„åé¦ˆï¼Œä¸€æ­¥æ­¥è¿›è¡Œï¼›è€ŒModel-Basedçš„æ–¹æ³•å¯ä»¥é¢„æµ‹å‡ºæœªæ¥å‡ æ­¥ï¼Œå¹¶ä¸”é€‰æ‹©ä¸€ä¸ªæ›´åŠ pleasantçš„é¢„æµ‹æ¥æ‰§è¡Œï¼ˆæ€è€ƒä¸€ä¸‹å›´æ£‹ï¼Œçœ‹å‡ æ­¥ï¼‰
* Policy-Based & Value-Based
    * Policy-Based åŸºäºæ¦‚ç‡çš„æ–¹æ³•ï¼šé€šè¿‡åˆ†ææ‰€å¤„çš„ç¯å¢ƒï¼Œå¯¹æ‰€æœ‰actionï¼ˆå†³ç­–ï¼‰ç»™ä¸€ä¸ªå‘ç”Ÿæ¦‚ç‡
        * Policy Gradient
    * Value-Based åŸºäºä»·å€¼çš„æ–¹æ³•ï¼Œè¾“å‡ºæ‰€æœ‰actionçš„ä»·å€¼ç„¶åé€‰æ‹©ä¸€ä¸ªæœ€å¥½çš„    
        * Q-Learning,Sarsa
    * ä¸¤è€…ç»“åˆ
        * Actor-Critic: ActoråŸºäºPolicyï¼ŒåŸºäºæ¦‚ç‡ç»™å‡ºåŠ¨ä½œï¼Œç„¶åCriticå¯¹æ‰€ä½œå‡ºçš„åŠ¨ä½œç»™å‡ºä»·å€¼ï¼ˆç›¸æ¯”äºå•çº¯çš„Policy GradientåŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼‰
    * â“: æœ‰ç‚¹åƒHard/SoftMax?
* å›åˆæ›´æ–°/å•æ­¥æ›´æ–°
    * ä¼ ç»Ÿçš„æ¯”å¦‚Monte-Carlo Leaningï¼ŒPolicy Gradientï¼ˆOLDï¼‰ï¼Œä¸€æ•´ä¸ªå›åˆç»“æŸæ‰ä¼šæ€»ç»“æ›´æ–°
    * Q-Learingï¼ŒSarsaï¼ŒPolicy Gradientï¼ˆNEWï¼‰ï¼Œæ¯ä¸€æ­¥éƒ½æ›´æ–°ï¼Œ*æ•ˆç‡æ›´é«˜*ï¼Œç°åœ¨å¤§éƒ¨åˆ†çš„éƒ½æ˜¯
* On-Policy/Off-Policy
    * åœ¨çº¿ï¼š Sarsa
    * ç¦»çº¿ï¼š QLearning


            

## å…³é”®å®šä¹‰
* RLçš„**æ ¸å¿ƒ**ï¼š
    1. å¦‚ä½•å®šä¹‰é‡‡å–åŠ¨ä½œä¹‹åçš„Rewardçš„è¯„ä»·æ ‡å‡†ï¼ˆrewardçš„è®¾è®¡ä¸ä»…è¦ç¬¦åˆæœ€åæˆ‘ä»¬æ‰€éœ€è¦è¾¾åˆ°çš„ç›®çš„ï¼Œè¿˜éœ€è¦æ³¨æ„åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¸åŒactionçš„æ¯”è¾ƒï¼‰
        * æœ¬è´¨æ˜¯éœ€è¦è®©agentå­¦åˆ°æœ€å¥½çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ
    2. æˆ‘ä»¬å¦‚ä½•æ›´æ–°æ“ä½œ
    > Q-Learningä¸­ï¼Œä¼šæœ‰ä¸€å¼ å¾ˆå¤§çš„Tableï¼Œè®°å½•äº†æ‰€æœ‰stateå¯¹åº”çš„æ‰€æœ‰actionçš„å€¼ï¼Œæˆ‘ä»¬æŒ‰æ ·æœ¬å­¦ä¹ æ¯ä¸ªå•å…ƒæ ¼ï¼Œä½¿æœ€åçš„Rewardæœ€å¤§
    * åŸºäºValueçš„æ–¹æ³• - DQN
        * ä½†æ˜¯ç”±äºè¡¨è¿‡åº¦å¤§ï¼ŒDQNç”¨ä¸€ä¸ªNNæ¥æŸ¥è¡¨
        * DQNé€šè¿‡experience reply(è§£å†³æœºå™¨å­¦ä¹ ç‹¬ç«‹åŒåˆ†å¸ƒçš„é—®é¢˜) â”
        * ç”¨ä¸€ä¸ªç½‘ç»œä¼°è®¡Qå’Œç°å®Qä¹‹å·®ï¼Œä½œä¸ºloss
    * åŸºäºPolicyçš„æ–¹æ³•
        * ç›´æ¥ä¼˜åŒ–Policyç®€å•æ˜å¿«ï¼ˆNNç›´æ¥è¾“å‡ºPolicyï¼‰ï¼Œé—®é¢˜è¢«è½¬åŒ–ä¸ºå¦‚ä½•æ›´æ–°NNå‚æ•°
            * æœ‰stochastic / deterministic
                * Stochastic éœ€è¦é‡‡æ ·ï¼Œè®¡ç®—å¤§  - PPO
                * Deterministic å·²ç»è¯æ˜è¿‡äº†ç­–ç•¥æ¢¯åº¦å…¬å¼ - DPG/DDPG
    * èåˆValueä¸Policy - Actor-criticæ–¹æ³• -> A3C
* Stateï¼šå¯¹äºç°å®ä¸–ç•Œæƒ…å†µçš„å®Œå…¨æè¿°ï¼ˆå¾€å¾€æ˜¯æ¯”è¾ƒæŠ½è±¡çš„å»ºæ¨¡ï¼‰
    * ç”¨ä¸€ä¸ªTensoræ¥ä½œä¸º*Representation*
* Observationä¸Stateç›¸å¯¹åº”ï¼Œæ˜¯å½“å‰ä¸–ç•Œä¸å®Œå…¨çš„æè¿°ï¼Œä½†æ˜¯ä¸€èˆ¬ä¼šæ¯”è¾ƒå…·ä½“  ~~æˆ‘ä¸åˆ°å•Šæˆ‘ççŒœçš„~~
* Action State-å†³ç­–ç©ºé—´
    * å¯èƒ½è¿ç»­ï¼ˆæœºå™¨äººåœ¨å®é™…åœºæ™¯ä¸­ï¼‰å¯èƒ½ç¦»æ•£AlphaGO
* Policy - a rule used by agent to decide which action to take ~~æˆ‘å¥½åƒæƒ³ä¸åˆ°ä¸€ä¸ªå¤ªå¥½çš„ä¸­æ–‡ç¿»è¯‘ï¼Œå†³ç­–æ–¹å¼ï¼Ÿ~~
    * å¯ä»¥æ˜¯Deterministicï¼ˆHardï¼‰ *Max*
    * ä¹Ÿå¯ä»¥æ˜¯Stochastic ï¼ˆSoftï¼‰ *Soft*
        * Categorical Policies - Used In Discrete
            * å°±ç±»ä¼¼å»ºç«‹ä¸€ä¸ªå¯¹ç¦»æ•£Actionçš„Classifier
            * ğŸ¤”å·²çŸ¥äº†å„ä¸ªActionçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¯¹è¾“å‡ºéœ€è¦åšä¸€ä¸ª*Sampling*
        * Diagonal Gaussion Policies - Used In Continuous
            * ä¸€ä¸ªé«˜ç»´çš„é«˜æ–¯åˆ†å¸ƒï¼Œå¯ä»¥ç”±ä¸€ä¸ªå‡å€¼Vector&åæ–¹å·®çŸ©é˜µæ¥æè¿°
                * Diagnol Gaussian Distributionæ˜¯åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’é˜µçš„æ—¶å€™,è¿™ä¸ªæ—¶å€™åæ–¹å·®çŸ©é˜µä¹Ÿå¯ä»¥æ‹¿ä¸€ä¸ªVectoræ¥è¡¨ç¤º
            * ä¸€èˆ¬è¿™ç§æ–¹å¼é€šè¿‡ä¸€ä¸ªNNæŠŠè¾“å…¥çš„Observationæ˜ å°„ä¸ºMean Vectorä»¥åŠCovariance Matrixçš„ä¸€ä¸ªä»£è¡¨ï¼Œå¯ä»¥æ˜¯
                * ç›´æ¥æ˜¯Vector Of Log Standard Variation ï¼ˆæ ‡å‡†å·®çš„Logçš„å‘é‡ï¼‰
                * é€šè¿‡ä¸€ä¸ªNNå»ºç«‹ä¸€ä¸ªstateåˆ°log(Ïƒ)çš„æ˜ å°„ï¼ˆæˆ–è®¸ä¼šä¸è¾“å‡ºmeançš„ç½‘ç»œshare paramï¼‰
                * *ç”¨Logçš„åŸå› æ˜¯å€¼åŸŸè¦†ç›–æ•´ä¸ªå®æ•°åŸŸï¼Œè€Œæ ‡å‡†å·®åœ¨(0,1),è€Œä¸”æ²¡æœ‰ä»€ä¹ˆç²¾åº¦æŸå¤±*
            * Samplingå°±æ˜¯ä¸€ä¸ªé‡‡æ ·ï¼Œå·²çŸ¥ä¸€ä¸ªåˆ†å¸ƒä¹‹å Mean + Variation*Noise
* Trajectory (episodes / rollouts)
    * A Sequence T={s0,a0,s1,a1...}
    * The 1st State Is ramdom sample of state-state-distribution
    * çŠ¶æ€ä¹‹é—´çš„è½¬æ¢æ˜¯ç”±Environmentæ‰€å†³å®šçš„
    * ä¸ºäº†ç®—æ³•ç ”ç©¶ï¼Œä¸€èˆ¬å…·æœ‰é©¬å°”ç§‘å¤«æ€§ï¼Œå³åªä¾èµ–äºå‰ä¸€çŠ¶æ€
* **Reward**  
    * rt = R{st,at,st+1}
    * ç»å¸¸è¢«ç®€åŒ–ä¸ºR(st)æˆ–è€…æ˜¯R(st,at)
    * The Goal Of a Agent is to get *Maxium cumulative reward*
    * Final Reward(return) 
        * Infinite with discount R = sum((discount_factor)*rt)
            * discount factor (0,1) The Earlier Than Now,The Less
            * *Used For Better Convergence*
        * Finite Without Discount

* **The Main Model(Optimization View)**

$$ P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1}{P(s_{t+1}|s_t,a_t)}\pi(a_t|s_t) $$

* è¯¥ï¼°è¡¨ç¤ºäº†é‡‡ç”¨è¯¥Trajectoryå‘ç”Ÿçš„æ¦‚ç‡(Under The Case Of A Certain Policy pi)

$$ J(\pi) = \int_{\tau}{P(\tau|\pi)R(\tau)} = E_{\tau \sim \pi}[R(\tau)]$$

* Jè¡¨ç¤ºäº†Return,æ˜¯Rewardå‡½æ•°çš„ä¸€ä¸ªæœŸæœ›
* è€ŒRLæœ€æ ¸å¿ƒçš„é—®é¢˜æ˜¯

$$ \pi^* = \argmax_\pi{J(\pi)}$$

* **Value Function**
* ~~æƒå½“é”»ç‚¼ä¸€ä¸‹æ‰‹æ‰“å…¬å¼çš„èƒ½åŠ›~~
* æ„ä¹‰æ˜¯Rewardå‡½æ•°åœ¨æŸç§æƒ…å†µä¸‹çš„æœŸæœ›ï¼Œä¸€èˆ¬ä¼šç”¨åˆ°ä»¥ä¸‹å‡ ç§
1. On-Policy Value Function: Given An Expecated *Policy* and Initial *State*

$$ V^(\pi)(s) = E_{\tau \sim \pi}[R(\tau)|s_0=s]$$

* Its Maximum(Optimal)

$$ V^*(s) = \max_{\pi}{E_{\tau \sim \pi}[R(\tau)|s_0=s]} $$

2. On Policy Action-Value Function: Take An Arbitary Action (which may not from policy)

$$ Q^\pi(s,a) = E_{\tau\sim\pi}[R(\tau)|s_0=s,a_0=a] $$

* Its Optimal

$$ Q^*(s,a) = \max_\pi{E_{\tau\sim\pi}[R(\tau)|s=s_0,a=a_0]} $$

* Their Connection

$$ V^*(\pi) = \max_{a}{Q^*(s,a)}$$

* **The Bellman Function**
* ğŸ¤”The Core Is *The Value Of Your Starting Point Is The Reward You Expect To Get Being There + Wherever You Land Next*
  * The [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation) is a concept used In Dynamic Programming


* **The Main Model(Markov Model View)**
* Could be described as a MDP(Markov Decision Process) <S,A,R,P,$$ \rho_0 $$>
  * S - All States
  * A - All Valid Actions
  * R - reward Function $$r =  R(s_t,a_t,s_{t+1})$$
  * P - Transition Probaliliy Function
  * $$ \rho_0 $$ - The Starting State
    


## Q-Learning
* åŸºäºè¡¨æ ¼çš„æ–¹æ³•ï¼ˆè¡¨æ ¼çš„XYè½´åˆ†åˆ«æ˜¯å½“å‰stateå’Œå½“å‰æ—¶åˆ»æ‰€ä½œå‡ºçš„å†³ç­–ï¼›é‡Œé¢çš„å€¼æ˜¯æ½œåœ¨å¥–åŠ±-Qå€¼ï¼‰
    * è¿™ä¸ªè¡¨æ ¼*Q-table*å°±åƒæ˜¯ä¸ªçŠ¶æ€è½¬ç§»çŸ©é˜µï¼šå†³å®šäº†æ¯ä¸€ä¸ªçŠ¶æ€éœ€è¦é‡‡å–ä»€ä¹ˆæ ·çš„ç­–ç•¥
* é‚£ä¹ˆQå€¼æ˜¯æ€ä¹ˆå®šä¹‰å’Œæ›´æ–°å‘¢ï¼Ÿ
    * å¦‚æœå‡ºç°äº†æ–°çš„stateï¼ˆæ’å…¥å…¨0æˆ–è€…ç‰¹å®šåˆå§‹åŒ–ï¼‰æ‹“å±•QTable
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908104247.png)
* æ¯æ¬¡æ›´æ–°éƒ½ä¼šç”¨åˆ°Qçš„çœŸå®å€¼å’Œé¢„æµ‹å€¼
* alphaæ˜¯å­¦ä¹ ç‡ï¼Œè¡¨ç¤ºå­¦ä¹ é€Ÿåº¦ / gammaæ˜¯å¯¹æœªæ¥rewardçš„è¡°å‡å€¼ï¼ˆæ·±è°‹è¿œè™‘çš„ç¨‹åº¦ï¼‰
* Q-Learningæ˜¯Off-Policyçš„ï¼ˆQ-Tableå­˜å‚¨äº†ä¹‹å‰çš„ç»éªŒï¼Œå…¶æ›´æ–°å¯ä»¥ä¸åŸºäºæ­£åœ¨ç»å†çš„ç»éªŒï¼‰
    * ä½†æ˜¯æˆ‘ä»¬å¹¶æ²¡æœ‰åˆ©ç”¨åˆ°Q-Learning OFF-policyçš„ç‰¹æ€§ï¼Œåœ¨DQNä¸­ä¼šæœ‰ç”¨åˆ°

## Sarsa(State-Action-Reward-State-Action-è¿™ä¸ªåå­—ä¹Ÿå¤ªé­”æ€§äº†...) 
* å‰å‘æ¨æ–­æ–¹å¼ç±»ä¼¼äºQ-Learningï¼ˆéƒ½æ˜¯ä»ä¸€ä¸ªQTableä¸­é€‰å–æœ€å¤§Qå€¼çš„ç­–ç•¥ï¼‰
* æ›´æ–°æ–¹å¼ä¸åŒï¼š
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190908135538.png)
* å¯ä»¥ç†è§£ä¸ºå¯¹å·®è·çš„å®šä¹‰ä¸ä¸€æ ·ï¼š
    * Q-Learningæ˜¯ â€œä¸‹ä¸€çŠ¶æ€çš„æœ€ä½³Qâ€ - â€œå½“å‰çŠ¶æ€Qâ€ ï¼ˆå¯ä»¥çœ‹åšè§‚å¯Ÿäº†ä¸‹ä¸€çŠ¶æ€ä¼šæ€ä¹ˆæ ·ï¼Œå†åšåˆ¤æ–­ï¼ˆè¿˜æœ‰è½¬æœºï¼‰ï¼‰
    * è€ŒSarsaç›´æ¥å¯¹ â€œä¸‹ä¸€çŠ¶æ€çš„Qâ€ - â€œå½“å‰çŠ¶æ€çš„Qâ€ ï¼ˆå¯ä»¥çœ‹åšå·²ç»åšå‡ºäº†å†³å®šåˆ°äº†ä¸‹ä¸€çŠ¶æ€ï¼Œä¸å†æœ‰å›æ—‹ä½™åœ°ï¼‰
    * Q-Learningåœ¨å½“å‰æ—¶åˆ»åªä¼šå®šå½“å‰æ—¶åˆ»çš„Actionï¼ˆå†³å®šäº†ä¸‹ä¸€æ—¶åˆ»çš„Stateï¼‰ä½†æ˜¯ä¸ä¼šç¡®å®šä¸‹ä¸€æ—¶åˆ»çš„Actionï¼ˆéœ€è¦çœ‹MaxQ(s',a')ï¼‰
    * Sarsaçš„æ¯ä¸€æ—¶åˆ»éƒ½å†³å®šäº†å½“å‰æ—¶åˆ»çš„Actionä»¥åŠä¸‹ä¸€æ—¶åˆ»çš„Action
        * æ¯ä¸€æ—¶åˆ»å‚ä¸å­¦ä¹ çš„å‚æ•°æœ‰(State-Action-Reward-State'-Action')
* è¿™ä¹ˆçœ‹Q-Learningæ›´åŠ **è´ªå©ª**ï¼ˆä½“ç°åœ¨æœ€å¤§åŒ–maxQï¼‰ä¸€äº›ï¼Œå¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½è¡¨ç°å¾—å¥½çš„è¯ï¼ŒQ-Learningèƒ½æ‰¾åˆ°æœ€è¿‘çš„è·¯çº¿ï¼Œè€ŒSarsaåˆ™ä¼šè¡¨ç°çš„ç›¸å¯¹**ä¿å®ˆ**ï¼ˆå› ä¸ºä¸ä¿å®ˆçš„éƒ½æ­»äº†ï¼‰
* Sarsaå±äº*å•æ­¥æ›´æ–°*çš„ç®—æ³•

## Improved-Sarsa -> Sarsa(lambda)
* åŠ é€Ÿçš„Sarsaï¼Œä¸»è¦å°±æ˜¯æŠŠ*å•æ­¥æ›´æ–°*æ”¹æˆäº†*Næ­¥æ›´æ–°*ï¼Œèƒ½å¤Ÿé¿å…â€œåŸåœ°æ‰“è½¬â€çš„æƒ…å†µ
    * lambdaåœ¨[0,1] - 0å°±é€€åŒ–ä¸ºç®€å•Sarsaï¼Œ1å°±å˜æˆç›´åˆ°è·å–äº†rewardæ‰æ›´æ–°
* 






## Ref
* [OpenAI SpinningUp](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#id20)
* [Morvan's Blog](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-1-A-q-learning/) 