## [Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget](http://proceedings.mlr.press/v32/korattikara14.pdf)

ğŸ”‘ Key:         

* æå‡Bayesian Posteriorçš„ MCMCçš„samplingæ•ˆç‡
* é»˜è®¤çš„MHç®—æ³•ä¸­ä¾æ®Nä¸ªdata pointæ¥è®¡ç®—likelyhood functionå»è®¡ç®—ä¸€ä¸ªbinary decisionè¿‡äºcomputationally inefficientäº†
* æå‡ºäº†ä¸€ç§approximateçš„MH rule (based on sequential hypoyhesis test)
  * likelyhood could be computed from mini-batches of data
  * åªå¼•å…¥äº†æ¸è¿›çš„è¯¯å·®

ğŸ“ Source: 

ğŸŒ± Motivation: 

ğŸ’Š Methodology: 

ğŸ“ Exps:

ğŸ’¡ Ideas:  

* æˆ‘ä¹Ÿçœ‹ä¸å‡ºåšäº†ä¸ªå•¥â€¦æ€»ä¹‹å°±æ˜¯è¯æ˜äº†å¯ä»¥ç”¨ä¸€ä¸ªsubsetçš„æ•°æ®æ¥è¿‘ä¼¼MHç®—æ³•ä¸­è·å¾—likelyhoodçš„è¿‡ç¨‹



## [Beyond Backprop: Online Alternating Minimization with Auxiliary Variables](http://proceedings.mlr.press/v97/choromanska19a/choromanska19a.pdf)

ğŸ”‘ Key:         

* åŸæ¥çš„ä¸€äº›auxiliary-variableçš„methods(break the complex nested objective function into local subproblems)åŸºäºbatch(éœ€è¦full dataset at each iter)ï¼Œä¸æ˜¯å¾ˆèƒ½å¤Ÿé€‚é…extremely large datasets, æˆ–è€…æ˜¯continual/Reinforcement Learningçš„åœºæ™¯
* æœ¬æ–‡æå‡ºäº†ä¸€ç§novelçš„Onlineçš„Alternating Optimizationæ–¹æ³•(ä¸”æœ‰theoretical convergenceçš„guarantee in the stochastic setting)

ğŸ“ Source: 

ğŸŒ± Motivation: 

* åŸºæœ¬å’Œã€ŠA Scalable ADMM Approachä¸€æ ·ã€‹ï¼Œç”¨grad-basedçš„æ–¹æ³•æœ‰ä¸€äº›é—®é¢˜ï¼š
  * Local Minimaï¼Œ Objective highly non-convex, non-differentiable nonlinearity, cant parallelize weight update across layers(åº”è¯¥æ˜¯å› ä¸ºbackpropçš„æ—¶å€™chain ruleäº†æ‰€ä»¥ç›¸äº’ä¾å­˜?)

ğŸ’Š Methodology: 

* å¹¶æ²¡æœ‰ç”¨Langrangeä¹˜å­ï¼Œonly use one set of auxliary variable per layer -> Memory-Efficient as the standard SGD(store the activation value for grad computation)
* ?: æ²¡æœ‰å¤ªææ¸…æ¥šå®ƒå’Œå…¶ä»–AMçš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ“ Exps:

ğŸ’¡ Ideas:  



## [Training Neural Networks Without Gradients: A Scalable ADMM Approach](http://arxiv.org/abs/1605.02026)

ğŸ”‘ Key:       

* Using ADMM to optimize NN instead of Gradient-based methods

ğŸ“ Source: 

ğŸŒ± Motivation: 

* SGDè®­ç»ƒæ–¹å¼æ˜¯å¾ˆå¤šä¸ªsteps of cheap operation(on a mini batch), however,å¯¹äºCPUè¿™ç§multi-coreçš„ç»“æ„ï¼Œæ¯ä¸€ä¸ªsubtaskè¿‡äºçš„inexpensiveäº†(æ‰€ä»¥è¯´æ›´é€‚åˆGPU)å¯¹äºparallezing on CPU, éœ€è¦small number of expensive minimization steps. åœ¨æ¯ä¸€ä¸ªstepå»compute the exact gradient on each iteration. 
  * like conjugate gradients, BFGS, Hessian-Free
* å¦å¤–stressäº†grad-basedçš„å‡ ä¸ªé—®é¢˜:
  * vanishing gradient
  * stuck at local minimal and saddle points
  * not easy to parallelize across layers

ğŸ’Š Methodology: 

* separating the objective function at each layer into 2 terms:
  * Measuring the distance between weight & Activation ï¼ˆèƒ½å¤Ÿè®©weightçš„ä¼˜åŒ–ä¸å­˜åœ¨Vanishing gradï¼‰
  * Containing the nonlinear activation function(non-convex minimization problem that can be solved globally in closed-form)
* decouple the weights from the nonlinear link functions using a splitting technique.
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210228194418.png)

ğŸ“ Exps:

ğŸ’¡ Ideas:  



## [Natural Evolution Strategies](http://ieeexplore.ieee.org/document/4631255/)

ğŸ”‘ Key:         

ğŸ“ Source: 

ğŸŒ± Motivation: 

ğŸ’Š Methodology: 

ğŸ“ Exps:

ğŸ’¡ Ideas:  