## [Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss](http://arxiv.org/abs/1906.07413)

##  ğŸ”‘ Key:     

##      

##  ğŸ“ Source: 

* Stanford MaTengyu

##  ğŸŒ± Motivation: 

* Margin-based: encouraging a large margin as the regularizer

##  ğŸ’Š Methodology: 

* data dependent / label dependetn regularizer
* å¯¹per classå¯»æ‰¾å…¶uniform label error bound, æ¨boundä¸ºæ¯ç§Classè®¾è®¡äº†ä¸€ç§Bound
  * ![image-20210224222627127](C:\Users\A-suozhang\AppData\Roaming\Typora\typora-user-images\image-20210224222627127.png)

##  ğŸ“ Exps:

##  ğŸ’¡ Ideas:  

![](https://github.com/A-suoz

hang/MyPicBed/raw/master//img/20210224221141.png)

* existing soft margin loss, encourage the minority classes to have larger margins
* å…¶ä»–é¢å‘Imbalanceä»»åŠ¡çš„æ–¹æ³•
  * Re-weightï¼š Over-sample the minority classes
    * some even sample-level weight
    * empirically importance weighting does not have a significant effect when no regularization is applied
  * Re-sample
  * Margin-Loss: "max-min" - Hinge Loss
    * many uses class independent margin(this paper uses class-aware margin)
  * View it as "Label shift" in transfer learning


# [High-Performance Large-Scale Image Recognition Without Normalization](https://arxiv.org/abs/2102.06171)

##  ğŸ”‘ Key:         

* Boosting  CNN perf. without the batchnorm layer (NF-ResNets - batchnorm-free resnets)

##  ğŸ“ Source: 

* Deepmind

##  ğŸŒ± Motivation: 

* no BN: could not handle big LR / strong data aug

##  ğŸ’Š Methodology: 

* AGC (Adapative Gradient Clipping)
* design a family of Normalize-Free ResNets

##  ğŸ“ Exps:

* å®ç°æ–¹å¼æ˜¯å…ˆåœ¨ä¸€ä¸ª300Mçš„å¤§datasetä¸Šè¿›è¡Œpretrainå†åœ¨imgnetä¸Šè¿›è¡Œfinetune
* AGCçš„insightæ˜¯å¯¹grad clipçš„thresholdåº”è¯¥ä¾æ®weightæœ¬èº«çš„å¤§å°æ¥å†³å®šï¼š
  * è€Œé‡‡ç”¨element-wiseçš„æ¯”layer-wiseçš„è¦å¥½
  * å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§relaxation of normalized optimizers(LARS)



##  ğŸ’¡ Ideas:  

* Revisiting BatchNorm
  	1. å‡å°‘äº†residue(shortcut) branchä¸Šçš„activationçš„scale
   	2. è§£å†³äº†mean-shiftçš„é—®é¢˜(è¶Šæ·±è¶Šä¸¥é‡)
   	3. æœ¬èº«å¯ä»¥åšä¸ºregularizer
   	4. èƒ½å¤Ÿä½¿ç”¨æ›´å¤§çš„LRï¼Œå¢åŠ äº†è®­ç»ƒæ•ˆç‡(fewer steps of param updateï¼‰



## [GLOM: How to represent part-whole hierarchiesin a neural network](http://arxiv.org/abs/2102.12627)

ğŸ’¡ Ideas:  

* é‡ç‚¹å…³æ³¨çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨NNä¸­å»ºæ¨¡â€éƒ¨åˆ†/æ•´ä½“çš„å…³ç³»â€œ

  * æ¯”å¦‚è¯´å¯¹äºä¸€å¼ å›¾ç‰‡ï¼Œä¸åŒçš„å…ƒç´ å‡ºç°åœ¨ä¸åŒçš„ä½ç½®é¢ å€’ä¸€ä¸‹ï¼ˆäººè„¸çš„ä¾‹å­ï¼‰ï¼Œå…¶å®æ˜¯ä¸èƒ½å¤„ç†çš„
  * æ¯”è¾ƒåˆç†çš„æ–¹å¼æ˜¯å°†å„ä¸ªpartä»¥ä¸€ç§parse treeçš„å½¢å¼å»ºæ¨¡èµ·æ¥(æ„Ÿè§‰åƒä¸€ä¸ªgraph)ï¼›ä½†æ˜¯ç”±äºNNä¸èƒ½åŠ¨æ€çš„åˆ†é…ä¸€éƒ¨åˆ†èµ„æºå»æ„æˆè¿™ä¸ªâ€parse treeâ€œï¼š - Capsuleæœ¬èº«æ˜¯è§£å†³è¿™ç§é—®é¢˜çš„

* å¸Œæœ›è®¾è®¡çš„æ¨¡å‹GLOM

  * åŒ…å«äº†å¾ˆå¤šcolumnï¼Œä»–ä»¬çš„weightä¸€æ ·
  * æ¯ä¸ªcolumnåŒ…å«äº† a stack of autoencoder
    * æ¯ä¸ªautoencoder transformer the feature at some level to ä¸Šä¸€ä¸ªæˆ–è€…ä¸‹ä¸€ä¸ªlevel through a bottom-up encoder/top-down decoder
    * è¿™äº›levelè¡¨ç¤ºäº†ç‰¹å¾å±‚æ¬¡ï¼š æ¯”å¦‚é¼»å¤´-é¼»å­-è„¸-äººï¼›
    * åŒä¸€ä¸ªlevelçš„å¤šä¸ªcolumnæ˜¯ä¸ºäº†æ„å»ºå‡ºä¸€ä¸ªâ€islandâ€œ
      * proposes the idea of using islands of similar vectors to represent the parse tree of an image

  ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210304170657.png)

  * æŸä¸ªlevelçš„embeddingå—4ä¸ªæ–¹é¢å½±å“ï¼š
    1. bottom-up
    2. top-down
    3. embedding at previous time step
    4. attention-weighted average at the same level from nearby column
  * Capsuleçš„æ€æƒ³æ˜¯ï¼š
    * ä¸ºæ¯ä¸ªå¯èƒ½å‡ºç°çš„objectie/parté¢å¤–set asideä¸€ç³»åˆ—neuronï¼Œuse a routing algoå»dynamically connect small set of active capsules,æ„å»ºä¸€ä¸ªgraph