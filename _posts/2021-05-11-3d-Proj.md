layout:     post                    # 使用的布局（不需要改）
title:      3d Transformer项目记录           # 标题 
subtitle:  Updating.
date:       2021-05-11            # 时间
author:     tianchen                      # 作者
header-img:  img/2020/street3.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签



# Exp Running

## 2021-05-11

* 启动了一个MixedTransformer，固定npoint个点做downsample的实验(与固定点的PointTR Setting类似)

* 因为昨天的实验启动的时候用的是N=2，所以严格来说和最开始N=4 downsampling rate的PointTR有区别

* 发现了一个细节是TD层中有没有把xyz作为feature加入进去影响比较大

* 目前PointTR(discrete-input)与MinkTR相当: mIOU ~54%,同等情况下的MixedTR却更高(58%)，其与PointTR的唯一区别就是有没有对输入点做归化(一样Coord的Feature给downsample掉)

  * 区别在with/without KNN:用的是query&group,并没有严格的KNN
  * PointTR和VoxelInputTR的区别在，下采样的点数是固定为N/2个(对point-based的情况是固定的，因为输入都是4096点，而VoxelInput的则不定个 )

  1. PointTR: 53.4%
  2. VoxelTR: 54.6%
  3. PointTR using voxel input: 56.1%

# Code-base & Details

### Resume:

* Resume的用法:
  * 在train.sh里面输入export RESUME=True，就会load ckpt
  * 但是state里面存好了iter-size，如果iter-size到了就会自动进valid，所以需要改一下设置

### val_batch_size: 

如果在test的时候输入了config作为参数，一开始是会被load进来(via `main.py --batch_size`被main函数一开始的gen_config中得到)，但是由于后面又loading了test_config(train时候用的config)，所以就决定不了了，为了方便我们要么还是直接改config吧。(虽然不是很严谨)

* 改了个接口通过val_batch_size来喂进去了
* 修改了config.py中的默认为8

### KNN in PTBlock



# Algorithms

