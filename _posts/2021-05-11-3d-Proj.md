---
layout:     post                    # 使用的布局（不需要改）
title:      3d Transformer项目记录           # 标题 
subtitle:  Updating.
date:       2021-05-11            # 时间
author:     tianchen                      # 作者
header-img:  img/2020/street3.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
---



# Exp Running

> 立个flag，要好好记录实验场景和数据

## Running



## TODO：

**说明了Transformer没有学习到Pointnet-like的TransitionDown操作**，需要探索why？

1. TODO: 改TD只用一个FPS，不用group&projection

* 仍然还是需要一个Projection不然dim对不上，以及不能利用xyz信息
* 之前的cat_xyz是将neighbor的xyz都减去中心点的坐标*,目前新的xyz做不到加入这个
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210512221120.png)
* 只能先把xyz坐标给norm一下(除以mean把就)，然后一个linear-BN给映射到64-dim把
* `fps_only` on cardn 5   <font color='red'>46%</font> 

2. TODO: 是否是neighborhood的Group点范围

* 实际打印出来ck了一下，大概16个点的距离，浅层在1~2,后面能够到6~7(各个batch之间差距大，有的甚至能够到20)
  * 在sample&group的时候
  * `dat = x.C[new_indices[0,0].long()]; dat - dat[0]`
* `biggerStridedTD` on card 6 <font color=red>41.4%</font>

* 如果这几个不work的话，就是Transformer没有学到TD
  * 完全不加constraint的话，就很难学好 -> fixed part of the arch（类似我认为的引入一些anchor transformer）
  * 中间不一定要加pointnet的block，或许可以以其他形式引导transformer的优化

* 测试一下整scene上的VoxelTR与MinkResNet
  * TDLayer可能会出现问题： 
* Maybe 可以从整scene上FPS一定的voxel对比VoxelTR与MinkResNet (8192 points)
  * 写一个新的dataloader
  * 一定程度上可能会因为各个scene之间的sample间隔而有问题

## 2021-05-21

* TD层有一个巨大的影响
  * Transformer的引入貌似弥补了position信息的丢失
  * 原本(-10%)
  * 启动了一个MixedTrans的不带xyz的，在test，6卡上； 貌似最后区别不大，55% （-1%）
  * 启动了MInkTrans，最后的在51%左右(-3.5%)
* 是否kernel_size=1和TD就是不兼容（之前试过，确认一下）
  * 启动了MinkTrans下的`STrideConvTD` on card 5
  * 掉到了42%
* Voxel中的Conv中加回来
  * stem改成3x3
  * 生成qkv的改
  * linear-top/down改
  * middle-linear保持了1x1
  * OOM了…，重新启动之后  `mIoU: 54.2%`并没有变高，基本一致
* 加了一个vanilla的版本确定我的merge没有错
  * `MinkTrans/test` on card 7，看起来ok
* 加了一个只有TU换成voxel-based的形式:
  * `JustVoxelTU` 
  * 跑起来报错了，做不到，因为前面并不是stridedConv，导致内部的tensor_stride不是2，作罢

* 貌似目前的VoxelTR的Query&Group与KNN有一些区别
  * TODO: 也改成KNN?
  * :question:: 整scene上的架构preference会与当前的有区别。
  * 目前的KNN实现方式在整scene上不feasible…(是不是还是迁移到整scene上？)



> 完成之后的实验按照日期放在下面

## 2021-05-11

* 启动了一个MixedTransformer，固定npoint个点做downsample的实验(与固定点的PointTR Setting类似)

* 因为昨天的实验启动的时候用的是N=2，所以严格来说和最开始N=4 downsampling rate的PointTR有区别

* 发现了一个细节是TD层中有没有把xyz作为feature加入进去影响比较大

  * pointnet的一个legacy
  * 对于absolute的coord来说，加入dense的位置信息有help，

* 目前PointTR(discrete-input)与MinkTR相当: mIOU ~54%,同等情况下的MixedTR却更高(58%)，其与PointTR的唯一区别就是有没有对输入点做归化(一样Coord的Feature给downsample掉)

  * 区别在with/without KNN:用的是query&group,并没有严格的KNN

    * 应该就是2/3的唯一区别了，可能是因为r以及感受野的原因

  * PointTR和VoxelInputTR的区别在，下采样的点数是固定为N/2个(对point-based的情况是固定的，因为输入都是4096点，而VoxelInput的则不定个 )

    * 将VoxelInput的输入换成了固定的点数(与Point保持一致)变成了54.9%(一下子就掉下来了)

    

  1. PointTR(`pointr_base`): 53.4%
  2. VoxelTR(`minktr_base`): 54.6%
  3. PointTR using voxel input(`mixedtr_base`): 56.1%

# Code-base & Details

### Resume:

* Resume的用法:
  * 在train.sh里面输入export RESUME=True，就会load ckpt
  * 但是state里面存好了iter-size，如果iter-size到了就会自动进valid，所以需要改一下设置
  * 在train函数里面会检索 `if config.train`而我给他搞了一个默认的值是False，会触发这个，我先很dirty的用了判断是不是 == ‘True’，不是就置None（有那么一点蠢比）

### val_batch_size: 

如果在test的时候输入了config作为参数，一开始是会被load进来(via `main.py --batch_size`被main函数一开始的gen_config中得到)，但是由于后面又loading了test_config(train时候用的config)，所以就决定不了了，为了方便我们要么还是直接改config吧。(虽然不是很严谨)

* 改了个接口通过val_batch_size来喂进去了
* 修改了config.py中的默认为8

### KNN in PTBlock

1. `pct_utils.py`: 
   * 先做KNN(输入点数N/4，或者是输入npoint/2)
   * 然后依据KNN对每个点group K个(对phai和alpha)

2. `pct_voxel_utils.py`
   * 直接用的是QueryAndGroup函数，输入一个r一个k
     * `/home/zhaotianchen/project/point-transformer/pt-cls/pointnet2_ops_lib/pointnet2_ops/_ext-src/src/ball_query_gpu.cu`这个路径下
     * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210511145852.png)
     * 通过ball_query得到一系列idx再做grouping
     * 顺序的从random-shuffled的点中(循环点数k)找到nsample个fit in radius的点

### PTBlock

1. Linear-Top层: in_dim -> hidden_dim

   * 对MinkResNet-like的，可能in-dim和hidden-dim不一样，后续的qkv都是从hidden-dim来的

2. QKV的Mapping: hidden_dim -> out_dim(AttentionMap的尺度)

   * 就是一个1x1 Conv(Linear)
   * \phai是直接由input-feature过一个Linear，然后broadcast到psi(xK)
   * \psi是先过一个Linear，从neighbor feature中取k个
   * \alpha与\psi类似

3. Pose_encoding:(\delta) [3, out_dim] -> 因为要和\alpha(value)加在一起，所以也需要成out_dim： 

4. Attn_paer

   * (q-k+pose_encode)加在一起过一个MLP(gamma):  out_dim -> vector_dim

   * 这样之后的结果做一个Softmax就是attn_map了
   * 然后再乘上\alpha+pos_enc

5. LinearDown： [out_dim -> in_dim]

   * 最后revert回到in_dim来加shortcut

# Algorithms

* DDP: [Distributed data parallel training in Pytorch (yangkky.github.io)](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)

