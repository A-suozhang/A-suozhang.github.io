---
layout:     post                    # 使用的布局（不需要改）
title:      Bayesian & Non-Parametric   # 标题 
subtitle:   (...炼出一个丹方子)  #副标题
date:       2019-12-16           # 时间
author:     tianchen                      # 作者
header-img:  img/11_30/bg-ucentre.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 碎片知识
---

# Bayesian
  
*  Bayesian学派和频率学派(Frequentism)的对比
*  贝叶斯概率公式 - P(A|B) *{Posterior}*  =P(B|A) *{Prior}* xP(A) *{Likelyhood-We Want}* / P(B) *{Evidence}*

|Bayesian | Frequentism|
|--|--|
|利用先验分布来描述事件| / |
|将参数$\theta$看作一个服从某个后验分布(Posterior)随机变量(Stochastic Variable)|将起看作一个待求的常数|
| 给定样本X，求参数theta的条件分布-后验分布| 求常数theta的值 |
| 认为样本X是固定的 | 认为样本是随机的 |
| 最大后验估计(分布估计-所以要用交叉熵) | 最大似然估计(点估计) |


---

* 最小二乘回归(Least Square) - 等价于在高斯先验(Prior)下的最大似然估计
  * *高斯分布先验对应着二阶矩，原理同L2 Regularization等价于加上高斯先验*
* Ridge回归 - 等价于高斯下的最大后验估计（MAP）
* LASSO回归 - 等价于Laplace分布下的最大后验估计
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191218153125.jpg)
---

* Bayesian方法能给出不确定度的度量?
* NN中的BayesianNetwork（贝叶斯网络）是一个DAG(有向无环图)和Bayesian Neural Network不是一个东西

---

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191218162209.jpg)


# Variational Inference

* Variational - Variant (意指从一簇函数中寻找一个)
* 思想是用一个隐变量M(在我们的case中是Mask i)去推导到我们需要求解的变量X
	* 当实际X的分布untractable的时候用这种间接的方式来解决问题
	* 经典的例子是X-下雨概率 M-空气湿度
* 先验是我们希望隐变量M的分布具有的一些数学性质,并依照它选择这一簇函数组(对应我们的S型函数)
	* Posterior (P(M|X)) X与M具有什么样的关系
* Cast Inference as an Optimization Problem
	* When Probablity Distribution p is untractable(不能够对其采样的时候,p指代真实的后验分布)
	* Try to solve an optimization problem over a group of tractable Distribution Q, to find the most similar(通过在一簇函数族中优化参数的优化问题来解决)
		* Choose Between a Family Q
* Example: (Mean-Field Inference)	
	* (待填充,没有太看懂...)
* 大部分Probabilistic-Based模型的inference方法，都基于MCMC(Markov Chain Monte Carlo)
  * 本质上就是在MarkovChain上不停的走,依据一些采样原则(Sampling方法)会依据吸收概率等原则决定下一步往Chain上哪里走;理论只要经过一个足够长的时间一定会找到最优的地方
  * Example: Gibbs Sampling / Metropolis-Hastings  
  * Shortcomings: 虽然保证能够获得Optimal，但是不知道当前解距离最优解的距离有多少； 需要一个很好的Sampling
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191223215247.png)
* 感性认知可以认为是
	* 我已知我的后延是一簇含参数的函数组，需要通过Vi的训练来确定最优参数
	* VI的过程是,首先从一簇后验之中采样得到一个随机变量的值(获得一个Mask,相当于RL中的ActionSample)然后正常进行前向获得ClfLoss(相当于RL中的Reward)
	* 训练方式有两大种
		* EM: 抬杠子,交替进行当前weight下的最好Param估计,和当前param下的最好weight的更新
		* SGD-Based方式,将两者混合更新
			*　每次前向从后验的一簇函数族中按当前参数采样，并且做一个前向之后计算Clf Loss,再加一项KL-Divergence Loss(与采样出来的Mask有关)
			* KL Divergence Loss - Max(ELBO) - E_{q(M|\phai)}[(f(M|\theata))]
* 目前的大部分VI的实现方式都是基于最大化ELBO(等价于最小化先验和后验之间的KL距离)_
	* 一种传统方式是通过理论推导,当我确定后验是一簇含参数的函数族的时候,能通过固定的解析方式找到更新参数的Closed-Form解
	* 另外一种方式是借助梯度来解决,即通过Reparametrize(见下面)的方式来将Sample这个Non-Differentiable的过程变得可导,然后用正常的SGD来更新参数

# Non-Differentiable Solution

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191223215556.png)
