---
layout:     post                    # 使用的布局（不需要改）
title:      论文阅读《DAN》          # 标题 
subtitle:   是不是我的所有idea都被人想过了呢...   #副标题
date:       2019-10-23            # 时间
author:     tianchen                      # 作者
header-img:  img/10_1/bg-nmb3.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 论文阅读
     - 迁移学习
---

# [DAN](https://arxiv.org/pdf/1502.02791.pdf)
* *很经典的一篇文章了*
* **解决的问题**```the feature transferability drops significantly in higher layers with increasing domain discrepancy```
  * 很好理解因为分布变得比较厉害
* 办法就是,A Pair Of数据进来,MMD Loss计算出两个img进来的Feature Vector分布差异性,并且分析其差异.而一般的Classification Loss是证明原因
* Semi-supervised LapCNN provides no improvement over CNN
  * 喷了一下Semi的这种方式,认为Semi的这种方式不能够更好的提升DA性能
  * ~~我也是这么想的,所以我拿AdaBN来做Transfer,拿Semi来做LifeLong~~
* **核心思想**
  * 把Task-Specific Layer的Hidden Representation(Feature Map/Weight Vector)
  * 映射到一个Reproducing Kernel Hilbert Space(相当于完成了一个Embedding)
  * 去吧不同Domain Distribution的Mean-Embedding显示地适配起来
    * 使用了一种Optimal Multiple Kernel Selection的方法
  * *本质上还是去适配不同Domain的Feature Representation*