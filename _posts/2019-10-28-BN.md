---
layout:     post                    # 使用的布局（不需要改）
title:      论文阅读《Batch Normalization-Accelerating Deep Network Training by Reducing Internal Covariate Shif》          # 标题 
subtitle:   重温经典...   #副标题
date:       2019-10-28            # 时间
author:     tianchen                      # 作者
header-img:  img/10_1/bg-nmb3.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 论文阅读
---

# [《Batch Normalization-Accelerating Deep Network Training by Reducing Internal Covariate Shif》](https://arxiv.org/pdf/1502.03167.pdf)

* NN训练的非线性性造成层间数据分布不均匀,或者说是*Internal Covariance Shift*
  * 前层的一些小变化积累到后面会很大
* 对每个Mini-Batch做Norm
* 可容忍更高的LR以及初始化
* 也可以看做是一种regularizer,从而可以不用使用Dropout
* 14x Training Steps加速

* 当年relu+small lr+careful initial
  * 来避免gradient vanish(也就是saturation的问题)

> 又是一篇看了一般感觉没什么卵用的论文阅读呢🙌