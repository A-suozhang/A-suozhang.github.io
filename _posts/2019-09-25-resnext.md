---
layout:     post                    # 使用的布局（不需要改）
title:      ResNeXT是什么，SOTA的CNN设计思路如何？           # 标题 
subtitle:   可能还附带了一些自己对卷积设计的理解？        #副标题
date:       2019-01-01              # 时间
author:     tianchen                      # 作者
header-img:  img/bg-zynq.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - DL
     - CNN
---

> 参考了[知乎-ResNeXT详解](https://zhuanlan.zhihu.com/p/51075096)

# ResNeXT
* ```ResNet + Inception```
* 本质是**分组卷积**([Group Convolution](https://zhuanlan.zhihu.com/p/50045821))

##  Split-Transform-Merge
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190925220635.png)
* fc(对于上一层的每一个神经元)都是一个Split-Transform(Activation)-Merge的结构
* 而Inception Module，也是一个这样的Split-Transform-Merge的结构

## Change Inception
* ResNext的作者认为Inception V4中Inception Module内部需要设计，太人工和刻意(太多的超参数难以调节)
    * 合理也不合理：这样貌似没有考虑到多种感受野
* 所以对所有的Inception Module采取*相同的拓扑结构*，只通过Cardinalty(基数-这个词在数学里面是“势”)来控制有多少个这样的模块
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190925221542.png) 
* 与Inception V4很类似，只是Inception是先concat再```1*1conv```，而ResNext是先分路```1*1Conv```再Concat

## Group Conv
* 介于正常卷积和Depth-wise卷积之间的折中
    * 既不是对每一个C用一个卷积核，也不是对所有C用一个卷积核

# WRN(Wide ResNet)

> ResNet的又一个经典变体

* 所谓的宽，就是feature map的channel数...
* 命名方式 WRN-n(卷积层数)-k(宽度系数)-B(block的结构)
* 作者进行疯狂实验之后得到结论
    * 带Dropout更牛逼
    * 有的时候深度太深反而会GG
    * B(3,3)是比较好的结构，指一个Block里面2个3x3卷积


