---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      Paper Stack æ–‡ç« å †æ ˆ           # æ ‡é¢˜ 
subtitle:   å¾…è¯»çš„æ–‡ç« å’Œæ— å¤„å®‰æ”¾çš„æ–‡ç« è§£è¯»   #å‰¯æ ‡é¢˜
date:       2020-08-06            # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/7_1/3-build.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - å¿ƒå¾—
     - å¸¸ç”¨
     - è®ºæ–‡é˜…è¯»
     - ç»¼è¿°
---

# çŸ³å¢¨æ–‡æ¡£List

* [GATES Related](https://shimo.im/sheets/T3Cp36PRHPj38TKp/MODOC)
* [NAS Reading List](https://shimo.im/sheets/T3Cp36PRHPj38TKp/MODOC)



# Paper Stack æ–‡ç« å †æ ˆ

### GNN

* Foundation

### NAS


* [Channel Gating Neural Networks](https://papers.nips.cc/paper/8464-channel-gating-neural-networks.pdf)
* æ€æƒ³æ¥æºäºFeature Mapä¸­æœ‰å¾ˆå¤šç»å¯¹å€¼å°çš„åœ°æ–¹ï¼Œè¿™äº›ä¸œè¥¿å…¶å®æ˜¯å¯ä»¥ä¸éœ€è¦çš„
* æ ¸å¿ƒè§‚ç‚¹åœ¨äºï¼Œè®¤ä¸ºpartial sumå’Œglobal sumæ˜¯æœ‰ç›¸å…³æ€§çš„(æˆç«‹çš„å‰æå‡è®¾-åšäº†å®éªŒä¸€åŠçš„channelæœ‰0.86çš„ç›¸å…³æ€§)ï¼Œæ‰€ä»¥partial sumå‘ç°ä¸effectiveçš„éƒ¨åˆ†æ²¡æœ‰å¿…è¦ç»§ç»­åšè®¡ç®—(ä»¥ä¸€å®šcondition)
  * è¿™ä¸ªthresholdæ˜¯ç¡¬çš„
  * å¯ä»¥fine-grainä¹Ÿå¯ä»¥channel-wise
  * æŠŠæ‰€æœ‰çš„W_låˆ†ä¸€éƒ¨åˆ†W_på’Œå¦å¤–çš„W_r,å®ƒä»¬çš„ç»“æœåº”è¯¥åœ¨è¿™ä¸€å±‚æ˜¯è¦åŠ èµ·æ¥çš„ï¼ŒæŠŠW_p* X_pçš„è¾“å‡º(Partial Sum)è¾“å…¥ä¸€ä¸ªChannel Gatingæ¨¡å—ï¼Œä»¥ä¸€ä¸ªthresholdæ¥ç¡®å®šæ˜¯å¦éœ€è¦å¯¹å‰©ä¸‹çš„W_r Apply Maskï¼Œç»™å‡ºä¸€ä¸ªOut_Cç»´åº¦çš„Maskï¼Œé€‰æ‹©çš„ä¾æ®æ˜¯Feature Mapçš„Activationç»å¯¹å€¼
  * ç›¸å½“äºæ˜¯ä¸€ä¸ªæ–°çš„layer type
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200313154531.png)
* æœ‰ä¸€ä¸ªchannel groupingç¡®ä¿all channels are treated equally
  * ç›¸å½“äºé˜²æ­¢æ¯æ¬¡éƒ½æŠŠå‰å‡ ä¸ªchannelåšä¸ºW_rï¼Œæ‰€ä»¥åŠ äº†ä¸€ä¸ªchannel shuffling
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200313162432.png) 


* [Review Netowrk Pruning via Transformable NAS]()
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200317204835.png)
  * ç”¨NASæ¥å¤„ç†Channel Pruningçš„é—®é¢˜
  * æ¯ä¸ªlayerçš„channelsï¼Œé¦–å…ˆéœ€è¦è®¾è®¡ä¸€ä¸ªdistributionï¼Œç”¨ç±»softmaxçš„æ–¹å¼æ¥ï¼Œå†³å®šé€‰æ‹©æ¯ä¸ªchannelçš„æ¦‚ç‡
    * é€‰å–channelçš„è¿‡ç¨‹æœ¬èº«ä¸differntiableï¼Œç”¨gumble-softmaxæ¥
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200317203915.png)
    * CWI (Channel-Wise Interpolation) -  Adaptive Average Pooling
  

* [HAQ: Hardware-Aware Automated Quantization with Mixed Precision](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.pdf)


* [NAO]
    * Discrete -> Continuos 
      * åŒ…å«äº†ä¸€ä¸ªencoderï¼Œå°†archæ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­ç©ºé—´ï¼ŒåŒæ—¶è¿˜æ­é…ä¸€ä¸ªdecoder
      * è¿˜æœ‰predictor 
    * ä¸æ­¤ç±»ä¼¼çš„æ˜¯DARTSï¼Œè¯´DARTSè®¤ä¸ºæœ€å¥½çš„archæ˜¯å½“å‰weightä¸‹çš„argmaxï¼Œè€ŒNAOç›´æ¥ç”¨ä¸€ä¸ªdecoderæ˜ å°„å›æ¨¡å‹
      * è¿˜æœ‰ä¸€æ”¯æ˜¯Bayesian Optimizationï¼Œä½œè€…è®¤ä¸ºGPçš„æ€§èƒ½äºCovariance Functionçš„è®¾è®¡å¼ºç›¸å…³
    * Search Spaceè®¾è®¡
      * ä¸¤æ­¥ï¼Œé¦–å…ˆå†³å®š1ï¼‰which 2 previous nodes as inputs 2)ç¡®å®šè¦ç”¨ä»€ä¹ˆop
    * symmetricçš„design:ä¸ºäº†ä¿è¯symmetricçš„æ¨¡å‹ï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„embeddingä¸€è‡´ï¼Œpredictorç»™å‡ºå·®ä¸å¤šçš„ç»“æœ
      * ç”¨äº†Augmentationï¼ˆflipï¼‰æ¥è®­ç»ƒencoder
    * Encoderå’ŒDecoderéƒ½æ˜¯LSTMï¼Œpredictoræ˜¯ä¸€ä¸ªmean-poolingåŠ mlp
    * ä¸‰è€…Jointly Train
      * è®¤ä¸ºpredictor could work as regularizationå»é¿å…encoderåªå¯¹åº”decoderçš„ç»“æœï¼Œè€Œæ²¡æœ‰æ­£å¸¸è¡¨å¾
        * è¿™ä¸€æ­¥å’Œä¼ ç»ŸVAEä¸­çš„åŠ noiseä¸€è‡´
    * è®¤ä¸ºweight-sharingå’ŒNAOæ˜¯complementaryçš„

* [SemiNAS](https://arxiv.org/pdf/2002.10389.pdf)
  * ç”¨Controller(å…¶å®æ˜¯é‡Œé¢çš„predictor)å»åœ¨å¤§é‡æ— æ ‡æ³¨çš„archä¸Šåšæ ‡æ³¨(ä¸ç»è¿‡Evaluation)ï¼Œå°†æ–°çš„Data-PairåŠ å…¥è®­ç»ƒ
  * å–ç‚¹æ˜¯èƒ½å¤Ÿæ›´å¿«çš„æ‰¾åˆ°æ¯”è¾ƒå¥½çš„æ¶æ„ï¼Œæ¯”EAä¹‹ç±»çš„æ•ˆç‡æ›´é«˜
    * ä½†æ˜¯é«˜çš„ä¹Ÿä¸æ˜¯å¾ˆæ˜æ˜¾ï¼Œæ€€ç–‘æ˜¯å¦æ˜¯NAOå¸¦æ¥çš„è€Œä¸æ˜¯Semiå¸¦æ¥çš„(æ¯•ç«Ÿç”¨è‡ªå·±æ¨æ–­å‡ºæ¥çš„æ•°æ®æ¥è®­ç»ƒè‡ªå·±(ä½†æ˜¯flowä¸Self-Supervisedåˆä¸å¤ªä¸€æ ·))
  * ç”¨NAO as exampleï¼ˆå› ä¸ºå®ƒæ—¢å¯ä»¥ç”¨åœ¨conventional train from scratchä¹Ÿå¯ä»¥åšWeight sharingï¼‰

---

* [MAML - Model Agnostic Meta-Learning for Fast Adaptation]()
  * Learn2Learn
  * å¯¹æ ‡çš„æ˜¯æœ€ç»å…¸çš„ä»pretrain modelçš„transfer learning (ä¸¥æ ¼æ¥è¯´Fintuneä¹Ÿæ˜¯ä¸€ç§Meta-Learning)
  * MAMLè¿˜å…³æ³¨ç€å¤šä»»åŠ¡(ä¸MultiTaskLearningæœ‰è®¸å¤šçš„å…³è”)
    * æ¯”å¦‚äººè„¸å…³é”®ç‚¹å’Œå®šä½æ˜¯åŒæ ·çš„ä¸€ä¸ªé—®é¢˜ï¼Œä½†æ˜¯å¯¹NNæ¥è¯´æ˜¯ä½œä¸ºä¸¤ä¸ªé—®é¢˜æ¥å¤„ç†çš„
    * MAMLæ˜¯ä»¥ä¸€ä¸ªTaskä¸ºåŸºæœ¬å•ä½çš„
  * æœ€ç»ˆç›®çš„æ˜¯**å­¦ä¸€ä¸ªå¥½çš„åˆå§‹åŒ–å‚æ•°**
  * æ–°çš„Loss Function
    * ä¸€éƒ¨åˆ†lossæ˜¯ç»å…¸çš„lossï¼Œå¦å¤–ä¸€ä¸ªéƒ¨åˆ†å…¶ä»–ä»»åŠ¡çš„æŸå¤±å‚æ•°
    * æœ‰ä¸€ä¸ªæ€æƒ³æ˜¯ä¸æ‰¾å•ä¸ªä»»åŠ¡çš„å…¨å±€æœ€ä¼˜(Global Minima)ï¼Œå› ä¸ºè¿™æ ·å¯èƒ½å¾ˆéš¾æ”¶æ•›åˆ°åˆ«çš„taskçš„å¥½ç»“æœï¼Œè€Œæ˜¯æ‰¾åˆ°å¤šä¸ªtaskä¹‹é—´çš„ä¸€ä¸ªå¹³è¡¡(ä¹Ÿä¸ç”¨æœ€å¥½)
    * è€Œä¼ ç»Ÿçš„finetuneé¦–å…ˆæ˜¯ä»ä»»åŠ¡Açš„æœ€ä½³è§£å‡ºå‘ï¼Œæ‰€ä»¥ä¸ä¸€å®šå¯¹ç¬¬äºŒä¸ªä»»åŠ¡æ•ˆæœå¥½
  * æ“ä½œ
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200317213623.png)
    * æˆ‘ä»¬å…ˆæœ‰äº†éšæœºåˆå§‹åŒ–çš„ç½‘ç»œtheta_0,åœ¨ç¬¬ä¸€ä¸ªtaskä¸ŠæŒ‰ç…§æ­£å¸¸çš„sgdè®­ç»ƒè·å¾—å‚æ•°theta_m,ç„¶åä¸æ­¤ä¸ºåŸºç¡€è·å¾—valid lossï¼Œå¹¶ä¸”ç”¨è¿™ä¸ªæ¢¯åº¦ï¼Œä¸å»ä¼˜åŒ–theta_mè€Œæ˜¯å»ä¼˜åŒ–theta_0,å»å¾—åˆ°theta_1.ä¸‹é¢å†ä»¥theta_1ä½œä¸ºä¸Šä¸€æ­¥çš„theta_0ï¼Œå»applyåˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼ˆ*è¿™æ ·å»ç»å†A String Of Tasks-æœ‰ç‚¹åƒä¸€ç³»åˆ—è¯¾ç¨‹*ï¼‰

---


* [Convolutional Networks withAdaptive Inference Graphs](https://arxiv.org/pdf/1711.11503.pdf)
  * æŒºæœ‰æ„æ€çš„ä¸€ç¯‡æ–‡ç« ï¼Œæ€æƒ³æ˜¯CNNä¸éœ€è¦ä¸€ç›´FeedForwardä¸‹å»ã€
  * ä»¥ä¸€ä¸ªAdaptive Inference Graphçš„æ€æƒ³
  * Robustnessè§’åº¦ä¹Ÿèƒ½æ›´å¥½

* [Learning multiple visual domains with residual adapters](https://arxiv.org/abs/1705.08045)
* ğŸ”‘ Key:    
  * é€šè¿‡ä¸€ä¸ªResidueçš„Adapterï¼Œå­¦ä¹ å¤šä¸ªTaskçš„Share Representation
* ğŸ“ Source:  
  * VGG-Oxford   NIPS2017 
* ğŸŒ± Motivation: 
  * ä¼ ç»Ÿçš„æ–¹æ³•å¯¹ä¸åŒçš„Problem/Data,å­¦ä¹ ä¸åŒçš„Representationï¼Œæœ¬æ–‡ç›®çš„æ˜¯å­¦ä¹ ä¸€ä¸ªRepresentationå»å¤„ç†å¤šä¸ªä»»åŠ¡
  * Tunable Network Structureï¼Œé€šè¿‡ä¸€ä¸ªAdapter Residue Blockï¼Œå¯ä»¥å¯¹ä¸åŒdataset on-the-flyçš„è°ƒæ•´  
* ğŸ’Š Methodology:
  * å¯¹Data-dependent Learningé—®é¢˜ï¼Œå¯ä»¥æŠ½è±¡ä¸ºç”¨ä¸€ä¸ªAuxiliaryç½‘ç»œæ¥ä»Input Dataï¼ŒPredict Domainï¼Œdomain specificçš„paramå°±å¯ä»¥ç”¨ä¸€ä¸ªç½‘ç»œæ¥predictå‡º(ç›¸å½“äºå»Parameterizeè¿™äº›domain-specificçš„param)
  * æœ¬æ–‡è®¤ä¸ºçº¿æ€§çš„å¯¹filter paramåšparameterizationç­‰ä»·äºåŠ å…¥ä¸€ä¸ªæ–°çš„ä¸­é—´å±‚ï¼Œè¿™æ ·å°±å¯ä»¥æ˜¾å¼çš„åŒºåˆ†å¼€Domain-specificå’Œdomain-invariantçš„featureäº†
  * æ‰€ä»¥å¼•å…¥äº†ä¸€ä¸ªAdaptive Residue Block
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200415131036.png)
    * å…¶ä¸­\alphaè¢«é™å®šä¸º1x1Conv
    * åœ¨Adapter1x1Convä¹‹å‰æœ‰ä¸€ä¸ªBNï¼Œå…¶scaleä¸biasä¹Ÿæ˜¯domain-specificçš„
    * å¯¹æ¯”äº†ä¸€ä¸‹domain-(in)variantçš„dataæ¯”ä¾‹
      * 2(h^2C^2+hC)ä¸ªæ— å…³çš„/2(C^2+5C)ä¸ªæœ‰å…³çš„(hæ˜¯kernel sizeï¼Œä¸€èˆ¬æ˜¯3)
  * ä¼ ç»Ÿçš„Multi-Domain Learningç”¨finetuneçš„æ–¹å¼ä¼šé™·å…¥Catastrophic Forgetting
    * æœ¬æ–‡é¦–å…ˆåœ¨ä¸€ä¸ªå¤§çš„domain(æ¯”å¦‚Imagenet)ä¸Šè®­ç»ƒdomain-invariantçš„ï¼Œç„¶åå†åªtune domain-specificçš„
* ğŸ“ Exps:
  * Different Visual Tasks
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200415115730.png)   
    * FGVC-Aircraftï¼š 100Imagex100Class    
    * DPes: 50,000 Greyscale Pedestrain [18*36]
    * DTD: Texture,5640image,47class
    * GTSR: Traffic sign 43 class
    * Flower102: 102 Class of Flowers (40-258 image per class)
    * ILSVRC12: 1k Class, 1.2M
    * Omniglot: 50 class, 1623 Handwritten character
    * SVHN: 10 class, 70,000 real world sign
    * UCF101: action-recog 101 class 113,320 videosï¼ˆå¯¹æœ¬æ–‡æ¥è¯´ï¼Œå¯¹è§†é¢‘åšäº†Dynamic Image Codingï¼Œå°†æ•´ä¸ªvideo encodeæˆäº†ä¸€å¼ å›¾ï¼‰
  * Metricæ˜¯decathlon discipline
    * (éœ€è¦åœ¨æ‰€æœ‰taskä¸Šæ•ˆæœå¥½)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200415133227.png)
* ğŸ’¡ Ideas:  
  * Different Domain Share Low/Mid level patterns     

---

```
* ğŸ”‘ Key:   
* ğŸ“ Source:  
* ğŸŒ± Motivation: 
* ğŸ’Š Methodology:
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 
```


---


* [DetNAS: Backbone Search for Object Detection](http://arxiv.org/abs/1903.10979)
* ğŸ”‘ Key:   
  * nas 4 Det backbone
  * Supernet
* ğŸ“ Source:  
  * Megvii
* ğŸŒ± Motivation: 
  * Det often needs imagenet pretraining & NAS requires accuracy as supervised signal
    * Imagenet-pretraining + Det finetune
  * Pre-training and finetuning are costly
    * Following One-shot, decouple the weight training and the architecture
  * Det task perf. as guide, to search for the Backbone Feature Extractor
* ğŸ’Š Methodology:
    * Steps: 
      1. SuperNet Pretrain on ImageNet and finetune on Det Task
      2. NAS on Supernet with EA
        * Path-wise(at one time, only updating samples path)
    * Finetuning BN
      * Freezing BN(Traditional) wont work for supernet(normalize couldnt acquired at different paths)
      * SyncBN replace regular BN
        * Compute BN Statistics across multiple GPUs (save memory consumption)
      * Also when EA searches arch, each BN param should be independent
        * need to re-accumulate the BN for every new arch
    * SS Design
      * Small/Big (40/20 Blocks)
        * Small used in Ablation Study
      * based on ShuffleNetV2 Block(involves channel split and shuffle operation)
        * 4 Choices:
        * x3: kernel-size [3,5,7]
        * x1: replacing right branch with Xception block(3 repeated DW 3x3 Conv)
        * 4^(40) choices for big ss
    * EA
      * Mutation + CrossOver
      * arch dont meet constraint will be removed when updating
* ğŸ“ Exps: 
* ğŸ’¡ Ideas:
  * Det's Direction
    * Architecture: FPN - Top-Down arch with lateral connection, integrating features at all scales
    * Loss: RetinaNet's Focal Loss, dealing with the class imbalance(Instability in earlier training?)
    * MetaAnchor: dynamic anchor mechanism
  * AmeobaNet - plain EA without Controller could also achieve
  * Det often uses the image classification backbone which could be sub-optimal(DetNet59 > ResNet101)
  * NAS-FPN only searches for the Feature Pyramid Network
  * EA could better handle Constraints than RL & Gradient-based 
  * [SyncBN](https://tramac.github.io/2019/04/08/SyncBN/)
    * Plain BN with DataParallel - Input distiributed to subsets and build different models on different GPU
    * (Since independent for each GPU, so batch size actually smallen)
    * Key2SyncBN: Get the global mean & var
      * Sync 1st then calc the mean & var
      * (Simplest implemention is first sync together calc mean, then send it back to calc var) - Sync twice
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418111031.png)
        * modify computation flow and only sync once
    * [Code](https://github.com/tamakoji/pytorch-syncbn)
  * ------------------------------------------
  * Rather Small SS
  * Simple Shared-Weights with interesting handle of BN


* [NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](http://arxiv.org/abs/1904.07392)
* ğŸ”‘ Key:   
  * Search a FPN(Feature Pyramid Network) in a ss covering all cross-scale connections
* ğŸ“ Source:
  * Quoc V Le Google  
* ğŸŒ± Motivation: 
  * Huge design space(increase exponentially)
* ğŸ’Š Methodology:
  * Following Cell-based SS(author called it as scalable architecture), main contribution is designing **search space**
    * the SS is **modular**  (Repeat the FPN N times then concat into a large net)
  * RNN RL Controller
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418171859.png)
  * Feature Pyramid Network
    * (following RetinaNet, use last layer in each group of feature map as *the input of the FPN*)
    * 5 Scales(C_{3,4,5,6,7}) stride of 8/16/32/64/126 pixelï¼ˆ6,7 purely max-pooling of 5ï¼‰ for Merging Cell
    * Composed of multiple merging cells  
    * Input/Output same size - can be stacked (num to stack would control the acc/flops trade-off)
    * Each merging cell gives output are appended into the candidate layers, also feeds into next merging cell
      * Finally the 5 merging cells are output 
  * Merging Cell
    * basic element of FPN
    * Merging 2 input feature map of different size
    * Each Cell has its own resolution(the output)
    * 4 Step: 
      1. choose input-layer-1 
      2. choose input-layer-2 
      3. choose output feature resolution
      4. select binary op(add or maxpool) - (scales are handles b4 the binary op)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418173400.png)
  * Meshgrid Representation
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418173458.png)
* ğŸ“ Exps:
  * Scalable: Stacking NAS-FPN blocks could improve acc while stacking simple block couldnt
* ğŸ’¡ Ideas:
  * FPN - fuse features across different scales (cross-scale connection in ConvNets)
    * Deeper layer feature semantically strong but less resolution, are upsampled and added with lower-representation for feature with both good semantic and resolution  
    * sequentially combining 2 adjacent layer feature (with top-down/lateral connection)
    * RW
      1. [Path aggregationnetwork for instance segmentation. In CVPR, 2018.]() - add an additional bottom-up pathway
      2. [M2det: A single-shot object detector based on multi-level feature pyramid network. AAAI, 2019.]() - Multiple U-shaped Modules
      3. [Deep feature pyramid reconfiguration for object detection. In ECCV, 2018]() - Combine features at all scale + Global attention
    * Problem: manually designed and shallow(compared to backbone)
  * Any-time detection: dont necessarily need to forward all pyramid networks
    * Desired when computation effort is concern
  * ----------------------------------------------------------------------------------
  * Google's Work, really strange Hyper-paramï¼ˆlr-0.08/8 epochs trainingï¼‰ (Maybe Grid-Search Again?)


* [EfficientDet: Scalable and Efficient Object Detection](http://arxiv.org/abs/1911.09070)
* ğŸ”‘ Key:   
	* Systematically NAS for Det Task
  * Combining EfficientNet Backbone + Bi-FPN + Compound Scaling
* ğŸ“ Source:  
	* Quo V Le Google Brain
* ğŸŒ± Motivation: 
	* Weighted Bi-directional FPN - for multi-scale feature fusion (Better Feature Aggregation)
	* Compound Scaling method - uniformly scale the resolution/depth/width (Scalable)
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418223126.png)
  * the "BiFPN", analogy of FPN & PANet
    * from traditional "Top-down" structure(1-way information flow)
    * Adding an extra edge when I/O is at the same level
    * remove node with only one input edge 
    * Adding Weighted Feature Fusion - like Attention
    * exponential scale up BiFPN width
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418220047.png){:height="100" width="100"}
* ğŸ“ Exps:
* ğŸ’¡ Ideas:
  * One-Stage Det: (Anchor-Free) whether have a region proposal step

---

* [Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning](http://arxiv.org/abs/1904.03137)
* CVPR2019
* Arch:
  * Conditional GAN with learnable neural masking
    * During SGD Add Mask on Layer/weight
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420174733.png)

* [Lifelong Learning with Dynamically Expandable Networks](http://arxiv.org/abs/1708.01547)
* ICLR2018
* Arch:
  * Add Neurons by heuristic rule(Add and re-intialize new neurons)

* [An Adaptive Random Path Selection Approach for Incremental Learning](http://arxiv.org/abs/1906.01120)
* NIPS2019
  * Dynamic adaptive path selection
  * Attention-based on Peak Path Response
    * Paths Re-weighted with new task 
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420185856.png)

* [Memory Aware Synapses: Learning what (not) to forget](http://arxiv.org/abs/1711.09601)
* ECCV 2018
  * Memory-Aware Synapses: Marking the parameter of a NN online and unsupervised

* [IL2M: Class Incremental Learning With Dual Memory](https://ieeexplore.ieee.org/document/9009019/)
* ECCV 2019
* Dual Memory 1. for examplers 2. initial class statistics(class predictions) - used for rectify the classification(deal with imbalance)

* [Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights](http://arxiv.org/abs/1801.06519)
* ECCV 2018
  * Learn Multi-task Masks  
  * Task-specific loss train the soft threshold
    * one mask per task
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420192129.png)

* [Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting](http://arxiv.org/abs/1904.00310)
* ICML 2019
  * decouple the arch learning and param learning
  * Growing the network-NAS-like DARTs
    * Search for block (new/reuse/adaption)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420202125.png) 

* [Efficient Lifelong Learning with A-GEM](http://arxiv.org/abs/1812.00420)
* ICLR2019
  * Evaluating Efficecny of lifelong learning methods (metric for measuring how quickly learning)
  * Improve the original GEM(Gradient Episodic Memory)
    * Distribute an episode memory
  * mainly discussed why other methods fail to fitting the "Lifelong Learning" because not dealing with a single pass of the stream of data


* [Learning without Memorizing](http://arxiv.org/abs/1811.08051)
* CVPR2019 2019
  * no stroing data sample for old task
  * change in classifier attention map for old tasks
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420204205.png)

* [Lifelong Learning with Searchable Extension Units](http://arxiv.org/abs/2003.08559)

* ğŸ”‘ Key:  
  * * Lifelong Learning predefine arch, this method search for extension model
* ğŸ“ Source:
  * arxiv 2003 & ZJU 
* ğŸŒ± Motivation: 
  * Summarize the lifelong learning
    1. Fix methods: retaining data information of previous task
      * Often encounter capacity bottleneck
    2. Expansion Methods: 
      * Newly added params are added, though the old are frozen or regularized
      * Then re-train whole/subnet for new task
      * May Increase rapidly(redundant)
      * Also added elements are constrained to be the same with the previous
* ğŸ’Š Methodology:
  * Contains: 1. Unit Searcher   2. Task model creator
    * Super Model(Contain Multihead model)
    * Task Model(Part of the super model,all has the same layer)
    * Each task has task-specific output layer, other layer are shared (composed of many EUs)
  * Proceudre ï¼ˆseems pretty plain for NAS...ï¼‰
    1. Search EU - in current task
    2. Expand the SuperModel: adding EU in each intermediate layer
    3. Create Task Model: Select the best EUs(part)
    4. Train the task model, train the net
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420213758.png)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200420215011.png)
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 
  * new metric named mixed score: measuring both param size and accuracy of the life learning problem
---

---

### 2020-03-17 MCMC

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200317151113.jpg)
* æ’’è±†å­ç®—é¢ç§¯çš„å®éªŒ


### 2020-03-20 DeConv
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200320185002.png)

### 2020-03-24 Gumble Softmax
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200324113027.png)
* ä»VAE(è¿ç»­åˆ†å¸ƒé‡‡æ ·çš„æ±‚å¯¼)ï¼ŒåŠ å…¥ä¸€ä¸ªGumbleå™ªå£°(åŒ…å«äº†å‚æ•°çš„ä¿¡æ¯)ï¼Œé¦–å…ˆä»Gaussianä¸­é‡‡æ ·ï¼Œå¸¦å…¥å€¼
* å¯¹äºç¦»æ•£åˆ†å¸ƒï¼ŒåŠ ä¸€ä¸ªArgmax->Softmax 


### 2020-03-26 Self-Supervised Learning

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200325231129.jpg)
* [MoCo](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.05722.pdf) by Kaiming
  * å°†Contrastive Learningçš„é—®é¢˜æŠ½è±¡ä¸ºäº†ä¸€ä¸ªDictionary Learningçš„é—®é¢˜
    * ç»´æŒä¸€ä¸ªDictå…¶ä¸­æœ‰ä¸€äº›Encodingï¼Œå½“è¿›å…¥ä¸€ä¸ªæ–°çš„Sampleçš„æ—¶å€™ï¼Œç»è¿‡è¿™ä¸ªEncoderï¼Œæˆ‘éœ€è¦å®ƒèƒ½ä»è¿™ä¸ªDictä¸­Queryå‡ºä¸€ä¸ªä¸å…¶æœ€ç±»ä¼¼çš„Keyï¼Œè¿™æ ·å°±æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š 1. dictè¦è¶³å¤Ÿå¤§ 2. Dictè¦å…·æœ‰ä¸€è‡´æ€§
    * Similarityæ˜¯ç”¨InfoNCE(a kind of contrastive Loss Function)
    * Dictä¸­çš„Keysæ˜¯On-the-flyè¢«å®šä¹‰å‡ºæ¥çš„(Momentum Encoder ç»™å‡ºçš„ç»“æœ)
  * å¦‚ä½•åˆ¤å®šæ˜¯ä¸æ˜¯åŒç±»åˆ«ï¼ŸInstanceDiscrimination:
    * è¯¥Sampleä¸å…¶ä»–çš„æ‰€æœ‰å›¾ç‰‡éƒ½æ˜¯å¼‚ç±»ï¼Œå¯¹å…¶æœ¬èº«Augå‡ºçš„ç»“æœè®¤ä¸ºæ˜¯åŒç±»
  * Inferenceçš„æ—¶å€™åé¢ç›´æ¥è·Ÿä¸€ä¸ª1å±‚çš„fcï¼ŒåŠ ä¸€ä¸ªsoftmax
* CPC - CPCé€šè¿‡å¯¹å¤šä¸ªæ—¶é—´ç‚¹å…±äº«çš„ä¿¡æ¯è¿›è¡Œç¼–ç æ¥å­¦ä¹ ç‰¹å¾è¡¨è¾¾ï¼ŒåŒæ—¶ä¸¢å¼ƒå±€éƒ¨ä¿¡æ¯ã€‚è¿™äº›ç‰¹å¾è¢«ç§°ä¸ºâ€œæ…¢ç‰¹å¾â€

---

### 2020-03-27 

* ç”Ÿæˆæ¨¡å‹çš„å‡ ç§æµæ´¾
  1. AutoRegressive 
  2. VAE
  3. GAN
  4. FLOWï¼ˆNICEï¼‰
* éƒ½æ˜¯é€šè¿‡éšæœºå™ªå£°æ¥ç”Ÿæˆæ•°æ®ï¼Œåœ¨å»ºæ¨¡åˆ†å¸ƒçš„æ—¶å€™ï¼Œéƒ½æ˜¯é€šè¿‡åº¦é‡å™ªå£°ä¸è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå·®å¼‚
* ä¸åŒç‚¹
  * VAEçš„zåˆ°xçš„Dependencyæ˜¯ä¸€ä¸ªéšæœºçš„è¿‡ç¨‹ï¼Œè€ŒGANæ˜¯ä¸€ä¸ªç¡®å®šçš„ï¼Œå› è€ŒVAEçš„Inferenceæ˜¯Well-Definedï¼Œè€ŒGANçš„å°±ç›¸å¯¹ç—…æ€äº†
  * GANæ˜¯ä¸ºäº†ç”Ÿæˆæ–°å›¾ç‰‡ï¼ŒVAEé™¤äº†è¿™ä¸ªä¹‹å¤–è¿˜éœ€è¦å»ºæ¨¡æ•°æ®çš„åˆ†å¸ƒä»¥åŠHiddenState
  * æµæ¨¡å‹ç›´æ¥ä»åŸå§‹é—®é¢˜å‡ºå‘ï¼Œç›´æ¥å»ºæ¨¡è®­ç»ƒæ•°æ®å’Œç”Ÿæˆæ•°æ®çš„æ¦‚ç‡å…³ç³»ï¼Œç”¨å¯é€†æ¨çš„NNæ¥è®­ç»ƒï¼Œzä¸xçš„å…³ç³»æ˜¯ä¸€ä¸€å¯¹åº”çš„
  * æœ€ç›´è§‚çš„ååº”æ˜¯Lossä¸åŒ
    * VAEæ˜¯ELBO(æœ€å¤§ä¼¼ç„¶ä¼°è®¡-ç­‰ä»·äºæœ€å°åŒ–KLæ•£åº¦-æ³¨æ„KLä¸‰åº¦ä¸æ˜¯åœ¨æ•°æ®å’Œå™ªå£°ä¹‹é—´çš„ï¼Œè€Œæ˜¯æ¨¡å‹çš„p(x)ä¸æ•°æ®çš„p(x)ä¹‹é—´çš„)
      * ELBOæ˜¯Likelyhoodçš„Lower Bound
    * GANæ˜¯æœ€å°åŒ–Jenson-Shannon Divergenceï¼Œè¿™ä¸ªä¹Ÿæ˜¯Modelç»™å‡ºçš„P(x)å’Œæ•°æ®ç»™å‡ºçš„P(x)ä¹‹é—´çš„(**è¯´äººè¯å°±æ˜¯ä¸¤ä¸ªEmbeddingä¹‹é—´çš„**)
    * FLOWç”¨çš„ä¹Ÿæ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œç”±äºç”¨çš„æ˜¯å¯é€†çš„NN

---

### 2020-04-05

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200405093121.jpg)

---

### 2020-06-17


- [Searching for Accurate Binary Neural Architectures]()
* Huawei Noah
* only search for width(channels), acquire higher acc with less flops
* the arch remain the same with the original fp32 model


- [Learning Architectures for Binary Networks]()
* GIST(South Korea)
* seems like eccv ...
* cell-based, proposed a new cell template composed of binary operations
* novel-searching objective - Diversity Regularization  
* è¿™äº›new ssåŸºæœ¬éƒ½æ˜¯ä¸€ä¸ªå¥—è·¯ï¼Œå¯¹dw separableï¼Œdilated conv binaryåŒ–
  * è¿™ç¯‡æ–‡ç« é‡Œå¤šäº†ä¸€ä¸ªzerorise layer(æ²¡ææ‡‚)ï¼Œè¯´æ˜¯ä½œä¸ºplaceholder

- [Binarized Neural Architecture Search]()
* Beihang Univ
* Darts foundation
* channel sampling / operation space reduction
  * abbadon less potential operation

- [BATS: Binary ArchitecTure Search]()
* Cambridge
* binarized ss
* search strategy



