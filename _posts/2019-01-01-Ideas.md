---
layout:     post                    # 使用的布局（不需要改）
title:      开新坑的一些Idea记录           # 标题 
subtitle:   。。。        #副标题
date:       2019-10-27             # 时间
author:     tianchen                      # 作者
header-img:  img/bg-zynq.jpg  #这篇文章标题背景图片  
catalog: false                       # 是否归档
tags:                               #标签
     - private
---

# NN Training Application
* FPGA Online Train 

* 核心思想,去获得**更Task Specific**的模型
* 到实际场景里去训练
* 同一个Framework可完成Semi和Transfer
  * 做semi的话是从一个相对少量带Label数据中获得比较好的效果
    * 比如从Imagenet Pretrain中得到,一个舰船的分类模型,或者说是鸟CUB200(总之就是))
  * 此外也可以做Transfer    



### 2019-11-28 讨论
* 大概需要什么地步
  * 用HLS跑通。
* 首先需要确定一个明确的场景
  * 数字？Cifar
  * 是否需要针对器件（ZCU？）
    * 如果对于仿真来说
* Learning Rate小对低比特来说更新不上？（即使gradient有一个高精度buffer，是否weight也有一个高比特buffer）
* 因为一次内存存不下，什么时候从DDR存取交互（绘图，FP也要分几次）

* **目前已经有的几个idea**
  * 反向和前向的时间比例 - Gradient累积（pingpong）
  * 在更新参数的同时可以把下一次的TCH给跑了（0.99OLD+0.01NEW）（不用显示写回DDR）（是否逐层做？）
  * 可以用logit更新来避免计算softmax
  * Batch的threshold，抵御Bad Sample
  


### 🚧 **TODO LIST**
* 须要把SVHN上的semi给跑了
* ~~须要吧SVHN-别的东西的Transfer给跑了~~
* 模型压缩相关的

---

### ❓ **Fuckin Problems**
* 需要证明从一个已经训练好的上去Finetune?
  * 其实理论上有点不太好
  * 但是也说不太清,我可以有意去放缓它的学习
  * **需要实验**
* 非常关键的也就是模型的问题
  * ~~我到底选择一个什么样的模型去做(?)~~
  * 目前已经有的代码和实验场景都是基于那个10几层的convnet去做的,还没有做迁移的  



### 2019-11-14
* 看了一些关于剪枝的文章,有的文章提出其实剪枝是在学"结构"?
  * 所以我们的Pretrain的模型可以是一个稀疏的,定点的,给加速带来更好的效果
  * 还有就是本身AdaBN也可以嵌入这个工作
* 现在**必须选择一个场景去做**
  * 不然铺开来工作量太大了*或者说是两者之间的关系是什么*
    * 目前看起来可能可以吧Semi放在主要的,然后顺便也可做Transfer
      * 🚧那么就需要把SVHN的Semi给跑出来
      * 关于SVHN的Transfer可以说是已经有了
  * 关于  
  * 标题可以是Consistent Training
    * 这个相对宽泛一点也可做对应的拓展
  * 







### 2019-11-10
* 看到了[IROS的一个Lifelong的比赛](https://lifelong-robotic-vision.github.io/competition/Object-Recognition.html)
  * 还是要有incremental的意思才会更有价值?
  * 需要思考一下怎么做到**Life Long** (可继续思考一下)
    * 从Representation和Replay的角度入手
    * 让ratio逐渐warm Up?
* 是否可以用KD的思想去替换MSE   
  * 有人用了KL 散度
  * (但是这个weight EMA的过程好像就是知识的transfer了)
  * 别人用了效果不是特别好


---

### 2019-11-8
* 重新思考故事应该怎么讲
* 大框架还是在线获得更加Task-Specific的模型
* 可以作为Semi也可以作为Transfer
  * transfer当中也可以做Finetune(从imagenet pretrain model拓展到新的Task（其实相对更有意义）)
* ~~    Semi肯定是不能Pretrain的...~~
  * 理论上也可以，如果把小的task作为Semi的任务




###　2019-11-5
* 算法复现有了一定可能性
* 出现了问题,发现算法不是很稳定
* 几个点可入手
  * 剪枝(先放一放)

### 2019-10-30
* 发现自己之前的idea被别人做过了(rnm)
  * Self-Ensemble的方法(甚至比我的思路更Elegant)
* 可以拿过来做压缩 - 作为我的切入点
  * （效果还不是很清楚，需要实验）
  * 理论上可行是因为实际上的迁移学习的场景大部分是拿ImageNet Pretrain的模型来Finetune
    * 所以理论上这个模型一定是有冗余的
* （之前在Semi上考虑的问题太多了，比如一直训练会不会崩这种，其实不用考虑那么多）
  * 比如现在这个就存在一个来的数据非常Class Imbalance怎么办
  * 其实是一个OpenSet的问题，也可一定程度上解决

## Problem
* 现实场景中的Label会比较少
* 现实场景与预先的场景不一致(会遇到完全没有见过的东西)

## Possible Application
* Trasnfer
    * Domain Adap
    * ~~其实这个角度会比较偏向算法层面了~~
* KD 
    * 好处在于拿KD做量化基本能够保证涨点数
    * 拿KD相当于Ensemble去做肯定能涨，但是在现实场景中不一定有意义
        * 要是提前训练好了的
    * 现实场景中样本Label较少
    * Facebook的最近的研究：[Billion-scale semi-supervised learning for image classification](https://www.semanticscholar.org/paper/Billion-scale-semi-supervised-learning-for-image-Yalniz-J%C3%A9gou/88ee291cf1f57fd0f4914a80b986a08a90d887f1)
* 在线压缩
    * （？）有一点意味不明，既然我可以fit下一个大模型，为啥还要动小模型

## Related Work
* Semi-Supervised-Learning-From-Unlabel Data
* Self-Training
* Transfer Learning
* Class Imbalance


## Training需要面对的问题
* Label哪里来？
* 存储是怎么样的form？
* 训练崩了
* 在线训练的需求？
    * online场景能够接受到最真实的数据
    * online场景能够接受到大量的数据


### TODO:LIST
1. 需要把几个数据集的环境搭起来
    * Mnist-Synthdigits-SVNH
    * ~~Office~~
    * ~~ImageCLEF~~
      * ~~需要拿bash命令做一下数据集,可以仿照imagenet那个 ~~
        * rnm那个是个暴力脚本,搞死了
    * VisDA 
2. ~首先复现一下在各个数据集上的finetune效果 (基本就是office) **Sat**~~
3. 无论如何我们都需要证明一下AdaBN能否work  **Sun**
    * ??存疑 
    * ~~以最简单的Office数据集做实验~~
      * 并不Work！！
4. ~~重现一下Async-Tri Training这种填补数据集的方法~~
   1. 还可以参考一下Semi的尝试复现论文有没有用
   2. ??存疑,且非常关键
5. 找到了一个新的算法把我所需要做的东西基本上做过了
   * Self-Ensemble的方法，我傻了 


---

# ✅ 思路一：Semi-Supervised Training

### 可以Brag的点
* 提升NN效率-设计好的架构，是提高数据利用率
    * 可以说是从**增加数据**的角度提高了网络的Performance
* 利用了没有被利用的数据，自己采集数据更新自己，一定程度上的Incremental

### 解决的问题 (有什么意义)
* 可以解决Label的问题
    * 通过一个Teacher来获得Label，目前除了Self-Training之外没有特别好的别的解决办法
* 可以保证底线
    * （但是理论上如果数据域和训练集差距实在太大的话也会出问题）
    * Teacher Model是Fixed，精度有一定的保证
* 完成了一个Transfer Learning的问题
    * 严格来说不太算是Transfer...但是由于接触到了实际数据域
    * （但是理论上必须是数据集中有的东西，也就是做不到Transductive或者是Incremental）
* 相比于直接Training
    * 有更多的数据，保证能够提高精度
    * 真实世界的数据，拟合Task-Specific有保证
    * 同时支持新的Label的进入(由于维护了一个TrainingSet，不完全需要)
* 与IL的相关部分
    * 相当于没有Label-或者有少数Label的Joint Training
        * *对于很多实际应用场景来说，缺的是Label而不是Data
* 其提供的是workflow，可以尝试对其他网络用

### Related Field
1. Semi-Supervised Learning
    * *Semi-Supervised Learinng 核心在于利用现有的模型，通过一些方法维护一个扩大的Training Set获得更好的效果。*
        * 与Few-Shot Learning相联系，当一开始的数据量和类别很少的时候
        * 与Incremnetal的联系，当Semi -Supervised可以学习到新的类别的时候，就可以叫做Transductive Learning，也就是一定意味上的IL(~~目前我们的workflow做不到，可以作为未来拓展~~)
        * 由于传统NN的Training Scheme属于Full-Supervised Learning，做semi-supervised的时候其实是对如何建立新的Training Set的时候下功夫
            * 有一些基于Toll的方法
            * 如果需要新增类别有时候有一些基于图的方法，比较复杂
    * (FAIR的文章中说的)目前主要的Unsupervised方法，只能取得微弱提升[ECCV2018 Paper](https://scholar.google.com/scholar_url?url=http://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html&hl=zh-CN&sa=T&oi=gsb&ct=res&cd=0&d=8358288919170046195&ei=iBSfXdO5CMi2ywTcv7PYAg&scisig=AAGBfm368DTBQjMnx_giq93EFc4r29RbNQ)
        * 除非是一些Weakly Supervised的(新来的数据不是完全无Label)
    * 传统的一些用到*Label Propogation*的算法
        * 在初始类别少的时候work，但是多了之后gain不明显
        * 而且需要维护一个很大的Graph
        * Low-shot learning with large-scale diffusion
    * 特点是用部分已有的数据，针对大量Unlabeled的数据去提高Performance
    * 在传统ML中是一个很传统的问题，DL领域研究相对少
    * 如果有涉及到Transductive Learning就可以认为是incremental了
        * *也就是是否可以学到pretrain所提供的以外的类别*
        * 但是本文目前没有提及，并且理论上不可以    
        * 而且目前已有的实现方法普遍比较玄学而且复杂
2. with Transfer Learning
    * *TL完成的是一个Domain Adaptation的问题，当实际场景的数据未知(在我们的case中实际场景有数据但是没有Label，也是一定意义上的未知)，通过一些训练技巧以及结构的设计，完成训练*
    * 应该本质上属于弱的迁移学习
        1. 首先完成的任务是一样的
        2. 其次数据物理上差距不大(都是光学，只是不一样的语义信息)
    * ~~又在一定意义上和迁移学习的本质相矛盾，迁移学习是在*获得不到实际场景数据的时候*~~
3. KD 知识蒸馏

### 存在的问题(慢慢看)
1. 不知道论文结果是否能复现 ~~这是个灵魂问题~~
2. 需要一个更小一点的题目做为一个Part
    * 先把题目定为针对硬件的算法优化，这样如果硬件实现做不完的话也可以
    * **也太tm Large Scale了!!** 可不可以把**问题的规模**缩小?（*因为实际应用场景很多不需要一个这么完全的模型，一般都是针对小一些的问题*）
        * 是否可以尝试对小问题？
        * 是否可以尝试对小网络对小一点的数据集？
            * 但是Kill The Bits已经能够把Res压到5Mb了，并没有证明*该种学习方法和压缩网络是否compatible*
                * 为啥可能会不匹配呢？
                    * 被压缩之后存在误差，本身的学习能力可能更差
            * 又但是这篇Work还是没有证明稀疏和压缩网络也具有这继续学习的能力（可不可以是一种新的方式）
3. ✅需要存储的数据到底有多少？
    * 每个Mini-Batch是否是独立的
    * 而且训练数据本来就是存储在DDR里面的，体量比较大
4. 比较依赖于再在true label的数据上面进行标注，那么究竟需要多少这样的数据
    * 和本身需要存下的训练数据是一个体量的，所以还ok啦
5. ✅Dual Network对硬件来说很蠢
    * 可以时分复用，一份逻辑资源，同样的网络结构
6. 必须要存下来一开始具有clean Label的数据集进行Finetune
    * 这个好像大家都必须要存，所以问题不大
7. 如果放在硬件上的话大概率是用T/S一样的Model，没体现这个架构将大模型蒸馏到小模型当中的这样一个优点
    * 需要进一步思考
8. 没利用到KD
    * ~~这个倒不是什么问题...~~
9. 存储的问题
    * 训练数据需要存下来，占据的空间有点大，有优化空间
10. 我们的实验到底需不需要这么用这么大的数据集
    * 可不可以把需要解决的问题简化？

# 可能的创新点
* 软件算法层面
    * 数据集的构建上
        * 它是对所有的排序的，这个对Streaming式的硬件不友好
            * 实际的数据是按照时间到来的，而不是一次性全都有
        * 是否需要构建一个Balanced的数据集？
            * 所以我怀疑存在的问题也许不存在
            * 我现在考虑的是实际世界场景的Class不如ImageNet多，实际只有几类，但是是否我pretrain就直接用实际场景会有的(比如kitti)
    * 是否可以和Compression结合起来
        * 和之前定点稀疏训练的算法？
    * ⭐从Transfer的角度入手
        * 单纯从Classification提点数的意义不是很有效
            * 而且~~客观条件不大允许整个10几T的数据集哈~~
        * 简单的Transfer的Flow   
            * Example-CUB
            * 先再ImageNet上面训练一个网络，最后的FC拿掉，换一个新的Softmax（1k类变200类）然后finetune出一个结果
            * 看上去很Attempting，问题是 *❌实际场景新的类别哪里来的Label呢？去拿Semi做Trasnfer可能识别新类别嗷*
            * 实际能够对应的应用场景也就是
                * 实际的数据和之前训练的数据有一定的差异，原先的模型*不会崩*，经过一个Semi的过程之后去让⭐**整个模型梗贴合实际场景**
                    * 走这个思路的化需要思考一下怎么❔实验验证
        * 🌟由于只训练一个Fc的效果不错，是否可以多个fc共用一个BaseNetwork，完成多任务迁移学习(是否可以与fine-grain的分类相结合？)
            * *但是这样的话相当于需要Training的就只是最后的一个Softmax，用不到很多的Training Trick，而且本身和Semi-Supervised Training关系不大，没有之前那个模型的话也是做不了*
            * 将两个环节结合起来，完整地完成整个学习流程之外，还利用这个做MultiTask，可以用于Fine-Grain的分类
            * ~~从我个人实验角度看不是特别Feasible~~
    * 解决Class Imbalance的问题
        * 利用Imbalance⭐做一个分类问题的*Class-Decrease*,将一个原本比较Unified的大模型去变成一个更加Task-Specific的模型，从而让模型更小
            * 去更好的Handle固定的几类

* 硬件设计层面
    * Teacher与Student的时分复用
        * ~~DPU是基于指令集的，所以这个问题实际没有必要~~

---

### 2019-10-15 组会上讲了一下

* 和云端的区分还是比较难，貌似只有*隐私问题*
    * 作为Online training会有一定的需求
    * 这个WorkFlow对应着一种Framework可能可行
* 数据集维护，可以存一段时间  
    * 理论上只要有比较好的新的数据就可以了
    * 数据集的体量是多少？
        * 目前取得比较好的效果是10x原来数据集大小的样子（对于ImageNet来说我们的实验环境难以接受了）
* 单纯的Classification提点数
    1. 实现起来不太可能，存储量太大了（这个Training一定是一个企业级的活，我自己做一定不能够这么做）
        * 除非换一个比较小的任务
    2. 本身只是为了提高一点点数，用处不是那么大
    * **往Transfer走相对会更有意义一些，需要深入思考和深入了解一下Transfer Learning，有一个系统的见解**
        * 往Transfer方向考虑的曾C说了一个余老板那边的实际需求：仿真环境训出来的东西到了实际场景用不了
            * 这个还是一个比较浅一点的TL，基本都是光照，移动
* 确实Online Training 梯度也会有精度损失，会不会产生影响
* **Class Imbalance**也确实是一个需要解决的问题
    * 一个Idea是是否需要解决这样的一个问题？ 可不可以就以一个大model为Base，然后去依据到来的数据训练出对应的模型（这也是一定意味上的De-cremental？）
        * 其实这个思路就很类似于Transfer了
    * *需要Handle的一个问题是文章中所谓的当dataset不balance的时候准确度下降*其实是因为某些类没有被训练好而导致拉低了总体的精度

* 关于**Online Learning**
    * 是一个肯定存在需求的方向
        * 但其实还是取决与这个Online的场景能不能学到所谓"新的知识"
            1. 按照semi的workflow是完全没有的，还是原来的东西
            2. 按照Transfer的方案，其实是去后训一个Classifier
                * 目前看来最Promising的一个⭐
            3. 能做到辨识新的东西
                * 需要一个Incremental了，目前的算法感觉不是特别靠谱
    * 目前来说很多研究感觉*实验设计都不太考虑实际应用场景，很多都往所谓AI的“学习能力”在做，不是特别和硬件实现相契合关契合*
        * 说白了就是Training的这个Workflow还没有定论，甚至都没有人去解决Label的问题，如果我们最后的目标是硬件实现或者这加速的化，不是很Feasible
        * 所谓的Online偏向Continual“继续学习”的能力
        
---

## 2019-10-18 有了新的思路和思考
* 大概有几个思路
    * Semi-Supervised的思路：Teacher-Student的FrameWork，Require更大量的数据
        * 优✅
            * 可以和后两个思路相结合
        * 劣❌
            * 单纯的提升Classification的点数意义不大
                * 🉑是不是可以和Transfer结合起来？给与Transfer更大的数据量
            * 实现起来不是特别Feasible
                * 如果对ImageNet做拓展的化，需要10倍的数据，到达10T的量级,算法做起来压力会比较大
    * Transfer Learning的思路，本质上是一个Domain Adaptation问题
        * 可以用AdaBN，不会引入太多的额外计算
            * 也可以看是否可以和Coral结合 
        * 可以和Semi-Supervised的思路联系起来
            * AdaBN本质上是一个Unsupervised Transfer的问题，解决的是实际场景没有Label和Data的问题
            * 论文中也提到了AdaBN可以和Semi-supervised的TL联系起来，用Label做Finetune
            * 同样也可和第一文章中的相结合，可以认为是利用Teacher产生的Soft Label去Finetune，获得更好的效果
        * 优✅
            * 实现起来比较Feasible，数据集和代码都比较多
            * 更有意义一点，可以增加模型的*鲁棒性*
            * *如果与Semi的方式结合*可以完成一个压缩（Teacher是一个大网络，Student是一个小的）
        * 劣❌
            * 数据集相对比较小，ResNet会干的很高
            * *Semi-Supervised*的一个要求是Teacher必须基本靠谱，那么就会有一个灵魂问题，*当我们已经有了一比较好的模型的时候为什么还要继续Semi呢？*
                * 🍰:Finetune理论上一定可以增加一定的TL性能（前提是给的Label正确）
                    * 在Teacher推断的时候加上AdaBN（理论可以达到SOTA的）
                * 🍰:可以通过这个过程做一个*压缩*
        * 🔨拟构想实现方式 
            * Transfer Learning的基本方式
                * 先在某一个Sub-Set上Pretrain teacher
                * 按照Semi的方式对数据做一个Soft Label
                    1. 维护一个新的Training Set
                    2. 或者利用这些Soft Label去首先对Teacher/Student做Finetune
                    3. （这个过程中是否可以引入Coral的方式进行训练？）
            * ❌问题：
                * TL的数据集，数据量达不到Larger的程度
                    * 但是理论上不太需要得到
    * Class-Decrease的想法-从一个大模型入手，逐渐走向
        * 🔨也可以利用Teacher-Student的Semi-Learning架构
            * 相比于上面的Design Flow，可以把Teacher做为ImageNet Pretrain模型
            * 然后实际场景的训练是更小的一个Task，同时有Domain Shift
                * 但是不大可能识别之前类别中没有的...

* 原本的数据到底需要存多少?
    * 从AdaBN的角度看只要能够反映分布
    * 从Semi的角度看，需要让最后那个Finetune能保证Clean Label足够
* 还是没有办法解释为何不做云计算训练?

### 目前的思路 2019-10-18 Noon
* 思路 – 进一步改进TL
    * 《Billion Scale Semi-Supervised Learning》说明了大量带Noise Label的数据可以提升性能，以及Teacher-Student架构可以将Stuednt的性能提升到SOTA之上
        * 前提是有一个相对还不错的Teacher，引入AdaBN可以解决这个问题
    * 《AdaBN》不需要额外的计算，就能让Trasnfer的性能达到SOTA
        * 并没有对模型的参数进行改进，所以模型其实是没有改进的
    * 通过大量数据对网络进行Finetune可以显著提升Transfer Learining的性能(显然)
    * Finetune需要Target Domain数据的Label （利用Semi-Supervised的思路可以为新网络提供Noise Label）

* 思路 – 进行模型蒸馏
    * 以上的Teacher-Student Scheme, Teacher Model可以是一个更大的网络，而我们实际的Student可能是一个更小的网络,提升的是小网络上的TL效果
    * Online Pruning(?)
    * 有可能可以解决Online Training的Overfitting的问题
    * 是否可以做Class-Decremental
        * 但是做不到新的类别
* 可能的问题
    1. Training Set 如何构建
        * Training Set取多大？对于Source Domain的数据需要存多少？(用于Semi-Supervised的Design Flow最后的Finetune)
        * Semi文中对构造Training Set是按照置信度排序的（硬件不友好）
        * Semi文中强调构造一个Class-Balance的Training Set
            * 当预训练Dataset中类别比实际类别多的时候其实并不需要分所有
    2. 与量化压缩的方法可否Compatible
        * 模型压缩量化为训练中引入了额外的精度损失，算法在压缩后是否Work？（理论问题不是特别大）
    3. Transfer Learning的算法实现问题
        * 待实验
    4. Online Trainig过多Iter是否会Suffer Overfitting？
        * 需要找到一个采集数据以及训练iter的平衡
        * 或者尝试去做Online Prunning
    5. 当我们已经有了一个相对好的Teacher的时候，Student的必要性？
        * 可以做压缩，以更少的资源代价获得（或许）更好的性能
        * 可以做Class Decremental，更加高效

### 新的思路考虑 2019-10-25 (该找一个切入点了)
* 多出一个所谓的可以做压缩!
  * 做剪枝,体现了硬件友好,而且可以加速
  * 和定点训练理论也可一定程度上结合起来
* 但是是否能够work?
  * 拿一个Teacher给出Soft Label拿过来做训练倒是很有说服力度,也可以和KD结合
  * 一个大Teacher和一个小Student,小的Student还可以是稀疏的,进一步剪枝
    * 大Teacher可以是一个比较普遍的模型
* ATDA还是纯利用训练一个新的fc来做trasnfer
  * 本质上有点暴力,没有从distribution的角度考虑问题
  * 我们通过引入AdaBN,来解决这个问题
* 
* 目前**急需一个能够串起来的一个思路马上去干**
* 目前来说可以Settle的
    * 对Student是Pretrain一个还是Finetune做剪枝,都可以
    * 对于数据集的构建方式(与其说是可以选不如说是要去找到一种)

---
## 🤔待讨论的问题🤔
* 一个CNN的数据表征能力取决于两个要素
    * 模型Capacity和数据量(无限增加两者都会先线性增加，然后趋于平稳)
    * 对于量化与剪枝是否是冗余度越大，就越能够压缩（！）
        * 而KD与一般的剪枝可以并列的原因就在于：KD相当于增加数据，以获得更好的表征能力（用有限的资源解决问题的效率）；而一般的剪枝可以看做是减少资源
        * 该方法与KD相类似，都是通过增加表征能力
* 目前可不可以认为整个CNN结构设计总体上在向Res+Inception收敛
    * NAS相关的领域有没有给出一个很新的很breakthrough很elegant新架构
* ~~定点量化的Finetune和KD是真滴合适~~
* NN的过拟合是不是数据表征能力过于强（模型太大了），而数据集不够大
    * FAIR的SEMI文章的结果表明，引入了数据之后，性能随着训练iter数目的拐点向后延迟了
        * 越训练NN的表征能力越强，同时性能也会越强，当超出数据集之后体现为Overfitting

---

# Reading List

## Transfer Learning (Task Transfer)

### Finetune Methods

* [Do Better ImageNet Models Transfer Better?](https://arxiv.org/abs/1805.08974)
* [Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)  
  * [Code](https://github.com/hendrycks/pre-training)

---

* [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/abs/1804.08328v1)

* [Transfer Learning Using Classification Layer Features of CNN](https://arxiv.org/pdf/1811.07459.pdf)
  * Finetune with Existing Classification Layer Boost Performance
  * *Mentions That The Generlization Of CNN is Based On The Large Network & Large Training Set*

* 🌟 [When Semi-Supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets](https://arxiv.org/abs/1812.05313)
  * Combine The Semi scheme in Finetuning from the Pretrained Model
  * Did Comprehensive Empirical analysis 3 Conclusion:
    * The SSL Gain from full-supervised baseline(with fewer label) is smalller
    * When Domain Shifts, SSL Better
    * Some SSL Methods can outperform full-label training

## Semi-Supervised Learning

> All Work Could Find Code On [PaperWithCode](https://paperswithcode.com/)

### [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://arxiv.org/abs/1804.09170)

* 2018
* [Code](https://github.com/brain-research/realistic-ssl-evaluation)
* Ian Goodfellow
* Evaluation For SSL Methods  

---

### [Temporal Ensembling for Semi-Supervised Learning](https://arxiv.org/abs/1610.02242)

* 2016
* ICLR 2017
* Nvidia
* Consistency Training + Temporal Ensemble
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120202926.png)

### [Mean-Teachers Are Better Role-models](https://arxiv.org/abs/1703.01780)

* Curious AI
* Change Temporal Ensemble in Pi-Model into Weight-Averaging 
* [Blog](http://a-suozhang.xyz/2019/10/28/MeanTeacher/)

### [VAT](https://arxiv.org/abs/1704.03976)

* Virtual Adversal Learning Regularization Term
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120203446.png)
* 2-Moon
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120203347.png)

### [Mixup](https://arxiv.org/pdf/1710.09412.pdf)

* MIT & FAIR
* originally proposed to increase robustness against adversal examples
* Methodology
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120194906.png)
  * Fitting One-Hot - Fitting Linear Combination

### [ICT](https://arxiv.org/abs/1903.03825)

* IJCAI 2019
* Bengio
* Bring Mixup For Consistency Training
* **VAT** not suitable for large scale Data
* Consistecy Training(Modified) encourages the prediction at **an interpolation** of unlabeled points to be consistent with the interpolation of the predictions at those points
* moves the decision boundary to low-density regions of the data distribution
* Methodology
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120193417.png)
* Two-Moons Problem
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120191340.png)


### [SWA - There Are Many Consistent Explanation, Why Should You Avergage](https://arxiv.org/pdf/1806.05594v3.pdf)

* [Code](https://github.com/benathi/fastswa-semi-sup)
* Cornell
* ICLR 2019
* SGD Could Not Converge Well For Consistency Loss(Taking  Big Step & Change Prediction), **Changed Optim**
* *Simplifying Pi-Model*
  * Apply Input Pertubation : The Consistency Loss is The Input-Output Jacobi-Norm
  * Apply Weight Pertubation: The Consistency Loss Is The EigenValue For Hessian Matrix
* Inspired By Weight-Averaging In Pi-Model Boost Performance(Even Bigger in Supervised Case)
  * Modified SWA(Stochastic Weight Averaging)
    * Cyclic Cosine LR

### [MixMatch](https://arxiv.org/abs/1905.02249)

* Google Research / Ian Goodfellow
* [Code](https://github.com/google-research/mixmatch)
* K Augmentation - Average - Sharpen
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191120204429.png)
* *Thinking About Entropy Minimization*
  * Encourage Model Giving Low-Entropy Label For Unlabeled Data
  * Pseudo Label - Change Soft conf into Hard Label
* [Blog](http://a-suozhang.xyz/2019/11/12/UDA/)

### [UDA(Unsupervised Data Augmentation)](https://arxiv.org/abs/1904.12848)

* Combine Lots Of Data AUG
* [Blog](http://a-suozhang.xyz/2019/11/12/UDA/)

### [EnAET: Self-Trained Ensemble AutoEncoding Transformations](https://arxiv.org/abs/1911.09265)

* [Code](https://github.com/wang3702/EnAET)
* Self-Ensmeble + AutoEncodingTransform + MixMatch
* Amazing Results with extreme few label

---

## Unsupervised Domain Adaptation

> [Zhihu](https://zhuanlan.zhihu.com/p/27657910)

> [Awesome-Transfer-Learning](https://github.com/artix41/awesome-transfer-learning#general-methods)

> [Transfer-Learning-Baseline](https://github.com/jindongwang/transferlearning/blob/master/data/benchmark.md)

> [Transfer-Learning-Resources](https://github.com/jindongwang/transferlearning)

> [VisDA Competetion](http://ai.bu.edu/visda-2019/)


### [Deep Visual Domain Adaptation: A Survey](https://arxiv.org/abs/1802.03601)

---

### Distribution-Based

### [DaNN - Domain Adaptive Neural Network](https://arxiv.org/abs/1409.6041)

* Older Version Of DAN

### [DAN - Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/pdf/1502.02791)

* DAN - Propose a MMD Loss, Measuring The Mean-Embedding Distance In Reproduction-Hilbert-Kernel Space

### [DeepCORAL - Return of Frustratingly Easy Domain Adaptation](https://arxiv.org/abs/1511.05547)

* Align Corelation Matrix

### [AdaBN - Revisiting Batch Normalization For Practical Domain Adaptation](https://arxiv.org/abs/1603.04779)

* Test Time BN Statistic Using Whole Train Set - Use The New Mean & Var In New Domain

### [MEDA - Visual Domain Adaptation with Manifold Embedded Distribution Alignment](https://arxiv.org/abs/1807.07258)

* Manifold-Embedded-Distribution-Alignment

### Data-Based

### [Asymmetric Tri-training for Unsupervised Domain Adaptation](https://arxiv.org/abs/1702.08400)

* 3 Network, 2 For Labelling 

### Feature-Based

### [Semi-supervised representation learning via dual autoencoders for domain adaptation](http://arxiv.org/abs/1908.01342)

### Adversal-Based

### [Self-ensembling for visual domain adaptation](https://arxiv.org/abs/1706.05208)

* Mean-Teacher in Domain Adaptation


---

## 参考文献📚

#### Co-Learning

* Dasgupta S, Littman M L, McAllester D A. PAC generalization bounds for co-training[C]//Advances in neural information processing systems. 2002: 375-382.

* Balcan M F, Blum A, Yang K. Co-training and expansion: Towards bridging theory and practice[C]//Advances in neural information processing systems. 2005: 89-96.

* Saito K, Ushiku Y, Harada T. Asymmetric tri-training for unsupervised domain adaptation[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017: 2988-2997.

#### Transfer Learning (Domain Adaptation)
* Sun B, Feng J, Saenko K. Return of frustratingly easy domain adaptation[C]//Thirtieth AAAI Conference on Artificial Intelligence. 2016.
* Tzeng E, Hoffman J, Darrell T, et al. Simultaneous deep transfer across domains and tasks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 4068-4076.
* Motiian S, Piccirilli M, Adjeroh D A, et al. Unified deep supervised domain adaptation and generalization[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 5715-5725.
* Long M, Cao Y, Wang J, et al. Learning transferable features with deep adaptation networks[J]. arXiv preprint arXiv:1502.02791, 2015.
* Yang S, Wang H, Zhang Y, et al. Semi-supervised representation learning via dual autoencoders for domain adaptation[J]. arXiv preprint arXiv:1908.01342, 2019.
* Shen J, Qu Y, Zhang W, et al. Wasserstein distance guided representation learning for domain adaptation[J]. arXiv preprint arXiv:1707.01217, 2017.
* Wang M, Deng W. Deep visual domain adaptation: A survey[J]. Neurocomputing, 2018, 312: 135-153.
* F. Zhuang, X. Cheng, P. Luo, S. J. Pan, and Q. He. Supervised representation learning: Transfer learning with deep autoencoders. In IJCAI, pages 4119–4125, 2015.
* K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classifier discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1712.02560, 2017.
* S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 201
* V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015
* R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 999–1006. IEEE, 2011.
* L. Zhang, Z. He, and Y. Liu. Deep object recognition across domains based on adaptive extreme learning machine. Neurocomputing, 239:194–203, 2017.
* Saito K, Kim D, Sclaroff S, et al. Semi-supervised Domain Adaptation via Minimax Entropy[J]. arXiv preprint arXiv:1904.06487, 2019.

#### Semi-Supervsied Learning

* 


#### Others
* S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448–456, 2015.
