---
layout:     post                    # 使用的布局（不需要改）
title:      开新坑的一些Idea记录           # 标题 
subtitle:   。。。        #副标题
date:       2019-01-01              # 时间
author:     tianchen                      # 作者
header-img:  img/bg-zynq.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - private
---

# NN Training Application
* FPGA Online Train 
* Senstime在做的是Online Fix这相关的工作
    * 采集到用户识别错误的反馈样本，去Fix整个模型
    * 我理解上可能就是用户这次识别不出来了，as a feedbcak, 然后处理的是人脸上相关的一些Class imbalance的问题，让样本平衡起来


## Problem
* 现实场景中的Label会比较少
* 现实场景与预先的场景不一致(会遇到完全没有见过的东西)

## Possible Application
* Trasnfer
    * Domain Adap
    * (其实这个角度会比较偏向算法层面了)
* KD 
    * 好处在于拿KD做量化基本能够保证涨点数
    * 拿KD相当于Ensemble去做肯定能涨，但是在现实场景中不一定有意义
        * 要是提前训练好了的
    * 现实场景中样本Label较少
    * Facebook的最近的研究：[Billion-scale semi-supervised learning for image classification](https://www.semanticscholar.org/paper/Billion-scale-semi-supervised-learning-for-image-Yalniz-J%C3%A9gou/88ee291cf1f57fd0f4914a80b986a08a90d887f1)
* 在线压缩
    * （？）有一点意味不明，既然我可以fit下一个大模型，为啥还要动小模型

## Related Work
* Deep Mutual Learning - Multi-终端去做Mutual Learning  
    * 我感觉本质还是基于Ensemble一定能提点，把Ensemble提的点转移到各个Net上去
    * *可以只要有一个Teacher，去训练很多STU*
* Semi-Supervised-Learning-From-Unlabel Data
* Self-Training
* Transfer Learning

## De-cremental
* Label哪里来？
    * 对应着现在的Self-Training（需要一些额外的阅读）
* **问题**
    * 意义问题?既然空间能够部署下一个大的模型，为什么要再部署一个小的
    * 相当于去Finetune，对性能提升不能保证
    * Label哪里来？


## Training需要面对的问题
* Label哪里来？
* 存储是怎么样的form？
* 训练崩了
* 在线训练的需求？
    * online场景能够接受到最真实的数据
    * online场景能够接受到大量的数据

---
## ✅ Semi-Supervised Training

### 可以Brag的点
* 提升NN效率-设计好的架构，是提高数据利用率
    * 可以说是从**增加数据**的角度提高了网络的Performance
* 利用了没有被利用的数据，自己采集数据更新自己，一定程度上的Incremental

### 解决的问题 (有什么意义)
* 可以解决Label的问题
    * 通过一个Teacher来获得Label，目前除了Self-Training之外没有特别好的别的解决办法
* 可以保证底线
    * （但是理论上如果数据域和训练集差距实在太大的话也会出问题）
    * Teacher Model是Fixed，精度有一定的保证
* 完成了一个Transfer Learning的问题
    * 严格来说不太算是Transfer...但是由于接触到了实际数据域
    * （但是理论上必须是数据集中有的东西，也就是做不到Transductive或者是Incremental）
* 相比于直接Training
    * 有更多的数据，保证能够提高精度
    * 真实世界的数据，拟合Task-Specific有保证
    * 同时支持新的Label的进入(由于维护了一个TrainingSet，不完全需要)
* 与IL的相关部分
    * 相当于没有Label-或者有少数Label的Joint Training
        * *对于很多实际应用场景来说，缺的是Label而不是Data
* 其提供的是workflow，可以尝试对其他网络用

### Related Field
1. Semi-Supervised Learning
    * *Semi-Supervised Learinng 核心在于利用现有的模型，通过一些方法维护一个扩大的Training Set获得更好的效果。*
        * 与Few-Shot Learning相联系，当一开始的数据量和类别很少的时候
        * 与Incremnetal的联系，当Semi -Supervised可以学习到新的类别的时候，就可以叫做Transductive Learning，也就是一定意味上的IL(~~目前我们的workflow做不到，可以作为未来拓展~~)
        * 由于传统NN的Training Scheme属于Full-Supervised Learning，做semi-supervised的时候其实是对如何建立新的Training Set的时候下功夫
            * 有一些基于Toll的方法
            * 如果需要新增类别有时候有一些基于图的方法，比较复杂
    * (FAIR的文章中说的)目前主要的Unsupervised方法，只能取得微弱提升[ECCV2018 Paper](https://scholar.google.com/scholar_url?url=http://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html&hl=zh-CN&sa=T&oi=gsb&ct=res&cd=0&d=8358288919170046195&ei=iBSfXdO5CMi2ywTcv7PYAg&scisig=AAGBfm368DTBQjMnx_giq93EFc4r29RbNQ)
        * 除非是一些Weakly Supervised的(新来的数据不是完全无Label)
    * 传统的一些用到*Label Propogation*的算法
        * 在初始类别少的时候work，但是多了之后gain不明显
        * 而且需要维护一个很大的Graph
        * Low-shot learning with large-scale diffusion
    * 特点是用部分已有的数据，针对大量Unlabeled的数据去提高Performance
    * 在传统ML中是一个很传统的问题，DL领域研究相对少
    * 如果有涉及到Transductive Learning就可以认为是incremental了
        * *也就是是否可以学到pretrain所提供的以外的类别*
        * 但是本文目前没有提及，并且理论上不可以    
        * 而且目前已有的实现方法普遍比较玄学而且复杂
2. with Transfer Learning
    * *TL完成的是一个Domain Adaptation的问题，当实际场景的数据未知(在我们的case中实际场景有数据但是没有Label，也是一定意义上的未知)，通过一些训练技巧以及结构的设计，完成训练*
    * 应该本质上属于弱的迁移学习
        1. 首先完成的任务是一样的
        2. 其次数据物理上差距不大(都是光学，只是不一样的语义信息)
    * 又在一定意义上和迁移学习的本质相矛盾，迁移学习是在*获得不到实际场景数据的时候*
3. KD 知识蒸馏

### 存在的问题(慢慢看)
1. 不知道论文结果是否能复现 ~~这是个灵魂问题~~
2. 需要一个更小一点的题目做为一个Part
    * 先把题目定为针对硬件的算法优化，这样如果硬件实现做不完的话也可以
    * **也太tm Large Scale了!!** 可不可以把**问题的规模**缩小?（*因为实际应用场景很多不需要一个这么完全的模型，一般都是针对小一些的问题*）
        * 是否可以尝试对小问题？
        * 是否可以尝试对小网络对小一点的数据集？
            * 但是Kill The Bits已经能够把Res压到5Mb了，并没有证明*该种学习方法和压缩网络是否compatible*
                * 为啥可能会不匹配呢？
                    * 被压缩之后存在误差，本身的学习能力可能更差
            * 又但是这篇Work还是没有证明稀疏和压缩网络也具有这继续学习的能力（可不可以是一种新的方式）
3. ✅需要存储的数据到底有多少？
    * 每个Mini-Batch是否是独立的
    * 而且训练数据本来就是存储在DDR里面的，体量比较大
4. 比较依赖于再在true label的数据上面进行标注，那么究竟需要多少这样的数据
    * 和本身需要存下的训练数据是一个体量的，所以还ok啦
5. ✅Dual Network对硬件来说很蠢
    * 可以时分复用，一份逻辑资源，同样的网络结构
6. 必须要存下来一开始具有clean Label的数据集进行Finetune
    * 这个好像大家都必须要存，所以问题不大
7. 如果放在硬件上的话大概率是用T/S一样的Model，没体现这个架构将大模型蒸馏到小模型当中的这样一个优点
    * 需要进一步思考
8. 没利用到KD
    * ~~这个倒不是什么问题...~~
9. 存储的问题
    * 训练数据需要存下来，占据的空间有点大，有优化空间
10. 我们的实验到底需不需要这么用这么大的数据集
    * 可不可以把需要解决的问题简化？

### 可能的创新点
* 软件算法层面
    * 数据集的构建上
        * 它是对所有的排序的，这个对Straeming式的硬件不友好
            * 实际的数据是按照时间到来的，而不是一次性全都有
        * 是否需要构建一个Balanced的数据集？
            * 所以我怀疑存在的问题也许不存在
            * 我现在考虑的是实际世界场景的Class不如ImageNet多，实际只有几类，但是是否我pretrain就直接用实际场景会有的(比如kitti)
    * 是否可以和Compression结合起来
        * 和之前定点稀疏训练的算法？
    * 从Transfer的角度入手
        * 由于只训练一个Fc的效果不错
        * 是否可以多个fc共用一个BaseNetwork，完成多任务迁移学习(是否可以与fine-grain的分类相结合？)
        * *但是这样的话相当于需要Training的就只是最后的一个Softmax，用不到很多的Training Trick，而且本身和Semi-Supervised Training关系不大*
* 硬件设计层面
    * Teacher与Student的时分复用

---

## 👐待讨论的问题👐
* 一个CNN的数据表征能力取决于两个要素
    * 模型Capacity和数据量(无限增加两者都会先线性增加，然后趋于平稳)
    * 对于量化与剪枝是否是冗余度越大，就越能够压缩（！）
        * 而KD与一般的剪枝可以并列的原因就在于：KD相当于增加数据，以获得更好的表征能力（用有限的资源解决问题的效率）；而一般的剪枝可以看做是减少资源
        * 该方法与KD相类似，都是通过增加表征能力
* 目前可不可以认为整个CNN结构设计总体上在向Res+Inception收敛
    * NAS相关的领域有没有给出一个很新的很breakthrough很elegant新架构
* 定点量化的Finetune和KD是真滴合适

