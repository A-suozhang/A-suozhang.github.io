---
layout:     post                    # 使用的布局（不需要改）
title:      开新坑的一些Idea记录           # 标题 
subtitle:   。。。        #副标题
date:       2019-01-01              # 时间
author:     tianchen                      # 作者
header-img:  img/bg-zynq.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - private
---

# NN Training Application
* FPGA Online Train 
* Senstime在做的是Online Fix这相关的工作
    * 采集到用户识别错误的反馈样本，去Fix整个模型
    * 我理解上可能就是用户这次识别不出来了，as a feedbcak, 然后处理的是人脸上相关的一些Class imbalance的问题，让样本平衡起来


## Problem
* 现实场景中的Label会比较少
* 现实场景与预先的场景不一致(会遇到完全没有见过的东西)

## Possible Application
* Trasnfer
    * Domain Adap
    * (其实这个角度会比较偏向算法层面了)
* KD 
    * 好处在于拿KD做量化基本能够保证涨点数
    * 拿KD相当于Ensemble去做肯定能涨，但是在现实场景中不一定有意义
        * 要是提前训练好了的
    * 现实场景中样本Label较少
    * Facebook的最近的研究：[Billion-scale semi-supervised learning for image classification](https://www.semanticscholar.org/paper/Billion-scale-semi-supervised-learning-for-image-Yalniz-J%C3%A9gou/88ee291cf1f57fd0f4914a80b986a08a90d887f1)
* 在线压缩
    * （？）有一点意味不明，既然我可以fit下一个大模型，为啥还要动小模型

## Related Work
* Deep Mutual Learning - Multi-终端去做Mutual Learning  
    * 我感觉本质还是基于Ensemble一定能提点，把Ensemble提的点转移到各个Net上去
    * *可以只要有一个Teacher，去训练很多STU*
* Semi-Supervised-Learning-From-Unlabel Data
* Self-Training
* Transfer Learning

## De-cremental
* Label哪里来？
    * 对应着现在的Self-Training（需要一些额外的阅读）
* **问题**
    * 意义问题?既然空间能够部署下一个大的模型，为什么要再部署一个小的
    * 相当于去Finetune，对性能提升不能保证
    * Label哪里来？


## Training需要面对的问题
* Label哪里来？
* 存储是怎么样的form？
* 训练崩了
* 在线训练的需求？
    * online场景能够接受到最真实的数据
    * online场景能够接受到大量的数据

---
## ✅ Semi-Supervised Training

### 解决的问题 (有什么意义)
1. 可以解决Label的问题
    * 通过一个Teacher来获得Label，目前除了Self-Training之外没有特别好的别的解决办法
        * 但是Self Training存在问题
            1. 稳定性不能保证
2. 可以保证底线
    * （但是理论上如果数据域和训练集差距实在太大的话也会出问题）
    * Teacher Model是Fixed，精度有一定的保证
3. 完成了一个Transfer Learning的问题(部分)
    * （但是理论上必须是数据集中有的东西）
4. 灵活地使用了KD，起到了很好的效果
5. 相比于直接Training
    * 有更多的数据，保证能够提高精度
    * 真实世界的数据，拟合Task-Specific有保证
    * 同时支持新的Label的进入(由于维护了一个TrainingSet，不完全需要)
6. 与IL的相关部分
    * 相当于没有Label-或者有少数Label的Joint Training
        * *对于很多实际应用场景来说，缺的是Label而不是Data
7. 其提供的是workflow，可以尝试对其他网络用

### Related Field
1. Semi-Supervised Learning
    * *Semi-Supervised Learinng 核心在于利用现有的模型，通过一些方法维护一个扩大的Training Set获得更好的效果。*
        * 与Few-Shot Learning相联系，当一开始的数据量和类别很少的时候
        * 与Incremnetal的联系，当Semi -Supervised可以学习到新的类别的时候，就可以叫做Transductive Learning，也就是一定意味上的IL(~~目前我们的workflow做不到，可以作为未来拓展~~)
        * 由于传统NN的Training Scheme属于Full-Supervised Learning，做semi-supervised的时候其实是对如何建立新的Training Set的时候下功夫
            * 有一些基于Toll的方法
            * 如果需要新增类别有时候有一些基于图的方法，比较复杂
    * (FAIR的文章中说的)目前主要的Unsupervised方法，只能取得微弱提升[ECCV2018 Paper](https://scholar.google.com/scholar_url?url=http://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html&hl=zh-CN&sa=T&oi=gsb&ct=res&cd=0&d=8358288919170046195&ei=iBSfXdO5CMi2ywTcv7PYAg&scisig=AAGBfm368DTBQjMnx_giq93EFc4r29RbNQ)
        * 除非是一些Weakly Supervised的(新来的数据不是完全无Label)
    * 传统的一些用到*Label Propogation*的算法
        * 在初始类别少的时候work，但是多了之后gain不明显
        * 而且需要维护一个很大的Graph
        * Low-shot learning with large-scale diffusion
    * 特点是用部分已有的数据，针对大量Unlabeled的数据去提高Performance
    * 在传统ML中是一个很传统的问题，DL领域研究相对少
    * 如果有涉及到Transductive Learning就可以认为是incremental了
        * *也就是是否可以学到pretrain所提供的以外的类别*
        * 但是本文目前没有提及，并且理论上不可以    
        * 而且目前已有的实现方法普遍比较玄学而且复杂
2. with Transfer Learning
    * *TL完成的是一个Domain Adaptation的问题，当实际场景的数据未知(在我们的case中实际场景有数据但是没有Label，也是一定意义上的未知)，通过一些训练技巧以及结构的设计，完成训练*
    * 应该本质上属于弱的迁移学习
        1. 首先完成的任务是一样的
        2. 其次数据物理上差距不大(都是光学，只是不一样的语义信息)
3. KD 知识蒸馏

### 存在的问题(慢慢看)
1. 不知道论文结果是否能复现 ~~这是个灵魂问题~~
2. 需要一个更小一点的题目做为一个Part
    * 先把题目定为针对硬件的算法优化，这样如果硬件实现做不完的话也可以
3. 需要存储的数据到底有多少？
    * 每个Mini-Batch是否是独立的
    * 而且训练数据本来就是存储在DDR里面的，体量比较大
4. 比较依赖于再在true label的数据上面进行标注，那么究竟需要多少这样的数据(?)
5. Dual Network对硬件来说很蠢
    * 可以时分复用，一份逻辑资源，同样的网络结构

### 可能的创新点
* 软件算法层面
* 硬件设计层面
    * Teacher与Student的时分复用
* 数据集的构建上
    * 它是对所有的排序的，这个对Straeming式的硬件不友好
    * 是否需要构建一个Balanced的数据集？
---


