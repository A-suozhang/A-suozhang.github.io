---
layout:     post                    # 使用的布局（不需要改）
title:      开新坑的一些Idea记录           # 标题 
subtitle:   。。。        #副标题
date:       2019-01-01              # 时间
author:     tianchen                      # 作者
header-img:  img/bg-zynq.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - private
---

# NN Training Application
* FPGA Online Train 
* Senstime在做的是Online Fix这相关的工作
    * 采集到用户识别错误的反馈样本，去Fix整个模型
    * 我理解上可能就是用户这次识别不出来了，as a feedbcak, 然后处理的是人脸上相关的一些Class imbalance的问题，让样本平衡起来


## Problem
* 现实场景中的Label会比较少
* 现实场景与预先的场景不一致(会遇到完全没有见过的东西)

## Possible Application
* Trasnfer
    * Domain Adap
    * (其实这个角度会比较偏向算法层面了)
* KD 
    * 好处在于拿KD做量化基本能够保证涨点数
    * 拿KD相当于Ensemble去做肯定能涨，但是在现实场景中不一定有意义
        * 要是提前训练好了的
    * 现实场景中样本Label较少
    * Facebook的最近的研究：[Billion-scale semi-supervised learning for image classification](https://www.semanticscholar.org/paper/Billion-scale-semi-supervised-learning-for-image-Yalniz-J%C3%A9gou/88ee291cf1f57fd0f4914a80b986a08a90d887f1)
* 在线压缩
    * （？）有一点意味不明，既然我可以fit下一个大模型，为啥还要动小模型

## Related Work
* Deep Mutual Learning - Multi-终端去做Mutual Learning  
    * 我感觉本质还是基于Ensemble一定能提点，把Ensemble提的点转移到各个Net上去
    * *可以只要有一个Teacher，去训练很多STU*
* Semi-Supervised-Learning-From-Unlabel Data
* Self-Training
* Transfer Learning

## De-cremental
* Label哪里来？
    * 对应着现在的Self-Training（需要一些额外的阅读）
* **问题**
    * 意义问题?既然空间能够部署下一个大的模型，为什么要再部署一个小的
    * 相当于去Finetune，对性能提升不能保证
    * Label哪里来？


## Training需要面对的问题
* Label哪里来？
* 存储是怎么样的form？
* 训练崩了
* 在线训练的需求？
    * online场景能够接受到最真实的数据
    * online场景能够接受到大量的数据

---
## ✅ Semi-Supervised Training

### 解决的问题
1. 可以解决Label的问题
    * 通过一个Teacher来获得Label，目前除了Self-Training之外没有特别好的别的解决办法
        * 但是Self Training存在问题
            1. 稳定性不能保证
2. 可以保证底线
    * （但是理论上如果数据域和训练集差距实在太大的话也会出问题）
    * Teacher Model是Fixed，精度有一定的保证
3. 完成了一个Transfer Learning的问题(部分)
    * （但是理论上必须是数据集中有的东西）
4. 灵活地使用了KD，起到了很好的效果
5. 相比于直接Training
    * 有更多的数据，保证能够提高精度
    * 真实世界的数据，拟合Task-Specific有保证
    * 同时支持新的Label的进入(由于维护了一个TrainingSet，不完全需要)
6. 与IL的相关部分
    * 相当于没有Label-或者有少数Label的Joint Training
        * *对于很多实际应用场景来说，缺的是Label而不是Data
7. 其提供的是workflow，可以尝试对其他网络用

### Related Field
1. Semi-Supervised Learning
    * (FAIR的文章中说的)目前主要的Unsupervised方法，只能取得微弱提升[ECCV2018 Paper](https://scholar.google.com/scholar_url?url=http://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html&hl=zh-CN&sa=T&oi=gsb&ct=res&cd=0&d=8358288919170046195&ei=iBSfXdO5CMi2ywTcv7PYAg&scisig=AAGBfm368DTBQjMnx_giq93EFc4r29RbNQ)
        * 除非是一些Weakly Supervised的(新来的数据不是完全无Label)
    * 传统的一些用到*Label Propogation*的算法
        * 在初始类别少的时候work，但是多了之后gain不明显
        * 而且需要维护一个很大的Graph
        * Low-shot learning with large-scale diffusion
    * 特点是用部分已有的数据，针对大量Unlabeled的数据去提高Performance
    * 在传统ML中是一个很传统的问题，DL领域研究相对少
    * 如果有涉及到Transductive Learning就可以认为是incremental了
        * *也就是是否可以学到pretrain所提供的以外的类别*
        * 但是本文目前没有提及，并且理论上不可以    
        * 而且目前已有的实现方法普遍比较玄学而且复杂
2. with Transfer Learning
    * Not So Much...

### 存在的问题(慢慢看)
1. 不知道论文结果是否能复现
2. 需要一个更小一点的题目做为一个Part
3. 需要存储的数据到底有多少？
    * 每个Mini-Batch是否是独立的
4. 比较依赖于再在true label的数据上面进行标注，那么究竟需要多少这样的数据
---


