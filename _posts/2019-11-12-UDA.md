---
layout:     post                    # 使用的布局（不需要改）
title:      论文阅读《Unsupervised Data Augumentation For Consist Learning》          # 标题 
subtitle:   目前的Semi的SOTA...   #副标题
date:       2019-11-12            # 时间
author:     tianchen                      # 作者
header-img:  img/10_1/bg-nmb5.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 论文阅读
---

# [Unsupervised Data Augmentation For Consist Learning]
* 对Semi-Supervised做了新的诠释
  * 用在label是scarce的情况下
  * 利用大量的Unlabel Data，将模型Constrain在对input noise invariant的情况下
  * 本文的切入点是**Advanced Data Augmentation**
  * 适用于多种Scenario
    * Visual/Language
    * Semi/Transfer
* SSL(Semi-Supervised Learning)归纳主要流派为**Consistency Training**
  * Regularize Model To Robust For Noise
  * 加不同的Noise  
    * Gaussian
    * Dropout
    * Adversial
* 作者发现在Supervised中有效的Data Aug在SSL中也有用
* 降低Consistent Loss的过程可以认为是**Propagate Label Domain To Unlabeled**

### Data Augmentation
* 可以看作额外添加了一个Augmented Set
* 本文做的是Unsupervised Data Aug
  * 认为原本的Supervised Data Aug，比较的“Cherry On The Cake"
  * 本文推广到了更大量的Unlabel data
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191112101038.png)
* 参考了AutoAugment
  * 其将PIL中的所有Aug方法组合，去搜一个最好的Aug方法
  * *本文中的方法不需要Search*
* 而对NLP场景，Aug是*Back Translation*
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191112103038.png)

### Semi-Sup
* 一个问题是当label数据量少的时候，很容易直接在原来的数据上过拟
  * Solu： Training Skill - **Transfer Signal Annealing**
  * Gradually Release Training Signal
  * 实现方法：
    * 当给出正确置信度高于某个阈值的时候，这个epoch不计算
      * 这个阈值随着训练的进行而增加
    * *避免了过多的去训练那些easy-to-be-labeled sample*
    * 一定程度上减缓了训练，防止了过拟合

