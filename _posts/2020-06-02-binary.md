---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      All about that binary       # æ ‡é¢˜ 
subtitle:   Binarize em.    #å‰¯æ ‡é¢˜
date:       2020-05-08          # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/6_1/building1.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - ç¯å¢ƒé…ç½®
---

# Papers

- [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/abs/1511.00363)
    - 2015
    - Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David
- [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)
    - 2016
    - Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)
    - 2016
    - Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi
- [Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing](https://arxiv.org/abs/1603.08270)
    - 2016
    - Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, Dharmendra S. Modha
- [Ternary Weight Networks](https://arxiv.org/abs/1605.04711)
    - 2016
    - Fengfu Li, Bo Zhang, Bin Liu
- [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients](https://arxiv.org/abs/1606.06160)
    - 2016
    - Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou
- [Flexible Network Binarization with Layer-wise Priority](https://arxiv.org/abs/1709.04344)
    - 2017
    - Lixue Zhuang, Yi Xu, Bingbing Ni, Hongteng Xu
- [ReBNet: Residual Binarized Neural Network](https://arxiv.org/abs/1711.01243)
    - 2017
    - Mohammad Ghasemzadeh, Mohammad Samragh, Farinaz Koushanfar
- [Towards Accurate Binary Convolutional Neural Network](https://arxiv.org/abs/1711.11294)
    - 2017
    - Xiaofan Lin, Cong Zhao, Wei Pan
- [Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving](https://arxiv.org/abs/1804.06332)
    - 2018
    - Jiaolong Xu, Peng Wang, Heng Yang, Antonio M. LÃ³pez
- [Self-Binarizing Networks](https://arxiv.org/abs/1902.00730)
    - 2019
    - Fayez Lahoud, Radhakrishna Achanta, Pablo MÃ¡rquez-Neila, Sabine SÃ¼sstrunk
- [Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization](https://arxiv.org/abs/1906.02107)
    - 2019
    - Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, Roeland Nusselder
- [Least squares binary quantization of neural networks](https://arxiv.org/abs/2001.02786v1)
    - 2020
    - Hadi Pouransari, Oncel Tuzel
- [Widening and Squeezing: Towards Accurate and Efficient QNNs](https://arxiv.org/abs/2002.00555)
    - 2020
    - Chuanjian Liu, Kai Han, Yunhe Wang, Hanting Chen, Chunjing Xu, Qi Tian
- [Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm](http://arxiv.org/abs/1808.00278)
    - 2018
    - æ·»åŠ äº†ä¸€æ¡é¢å¤–çš„shortcut
        * å·ç§¯æˆ–è€…æ˜¯BNçš„è¾“å‡ºï¼Œåœ¨binarizeåŒ–ä¹‹å‰, connect this real activationåˆ°consecutive block
    - ç”¨ä¸€ç§æ–°çš„tight approxæ¥å–ä»£STEå¯¹gradè¿›è¡ŒçŸ«æ­£
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200704195451.png)
    - magnitude-aware gradient
    - ä»å…¨ç²¾åº¦çš„pretrainedæ¨¡å‹å¼€å§‹ï¼Œactivationç”¨clipæ”¹ä¸ºReLU
	- æåˆ°äº†è®­ç»ƒBNNçš„ä¸¤ä¸ªé—®é¢˜
		* Non-diff
		* Gradç›¸å¯¹å¤ªå°ä¸èƒ½æ”¹å˜sign
			* ä¿ç•™ä¸€ä»½real-value weightå°±å¯ä»¥
- [Training Competitive Binary Neural Networks from Scratch](http://arxiv.org/abs/1812.01965)
    - 2018
    - å¼ºè°ƒäº†ç›®å‰(ç°åœ¨çœ‹èµ·æ¥å¥½åƒä¸æ˜¯ç›®å‰äº†)çš„å¾ˆå¤šæ–¹æ³•éƒ½éœ€è¦å€ŸåŠ©2-stage trainingæˆ–è€…æ˜¯full-precision model
        * æ‰€ä»¥æƒ³æå‡ºä¸€ç§ç›¸å¯¹simpleçš„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿç›´æ¥from scratch
    - åŒæ—¶é¦–å…ˆæå‡ºäº†binary+Denseçš„æ¨¡å¼
    * æåˆ°äº†Bi-real net
        * binarizeåŒ–çš„resnetï¼ŒåŠ å…¥äº†additional shortcut
			* ä»¥ä¸€æ¬¡é¢å¤–çš„real-value Additionä¸ºä»£ä»·
			* å°†Signå‡½æ•°ä¹‹å‰çš„éƒ¨åˆ†ï¼Œshortcutcåˆ°BNä¹‹å
        * a change of gradient computation
        * complex training strategy, finetuning from full-precision
    * ç¨å¾®ä¿®æ”¹STE
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701161704.png)
    * no weight decay
    * how 2 choose scaling factor
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701162106.png)
        * æœ‰äººproveäº†åœ¨å‰å‘çš„æ—¶å€™å¯¹weightåšfilter-wiseçš„scaling factoræ˜¯æ— æ•ˆçš„ï¼Œä½†æ˜¯å¯¹gradå¯ç”¨
        * scaling feature for activation
        *   è®¤ä¸ºlearning a useful scaling factoræ˜¯å¾ˆéš¾ï¼Œå› ä¸ºBNçš„å­˜åœ¨
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701163703.png)
- [XNORNet++](http://arxiv.org/abs/1909.13863)
    * BMVC
    * åŸå…ˆç‰ˆæœ¬çš„å¯¹Activation feature mapè®¡ç®—scaling factoréœ€è¦é’ˆå¯¹è¾“å…¥æ¯æ¬¡å»åšï¼Œcomputational expensive
    * fuse weight & activation scaling factor into one
    * (æ„Ÿè§‰å¾ˆtrivialå¾ˆempirical)
- [MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?](https://arxiv.org/abs/2001.05936)
    - 2020
    - Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, Christoph Meinel
    * æœ¬è´¨ä¸Šæ˜¯æå‡ºäº†ä¸€ç§æ–°çš„Block
    * æŒ‡å‡ºäº†åŸæœ¬çš„BNNçš„ä¼˜åŒ–æ–¹æ³•æœ‰
        * åŠ å¤§Channelæ•°ç›®
        * multiple binary basis
    * è®¤ä¸ºBNNä¸­çš„ä¸»è¦æŸå¤±
        * FP32ä¹˜æ³•ä¸Binaryä¹˜æ³•ä¹‹é—´çš„è¯¯å·®
        * ç»Ÿä¸€çš„Scaling Factoræ‰€å¯¼è‡´çš„Feature Mapç©ºé—´æœ‰é™
- [IRNet-Forward and Backward Information Retention for Accurate Binary Neural Networks](https://arxiv.org/pdf/1908.05858.pdf)
    * IRNet(Information Retention Network)
    * 2ç§ä¸»è¦æ–¹æ³•åˆ†åˆ«é’ˆå¯¹forwardå’Œbackward
        * Libra-parameter-binarization - minimize the q-error and information loss
            * balance & standardize weiht
        * Error Decay Estimator(EDE)
    * minimizing the quantization error - ||A-Q(A)|| - not always work
        * Objective Function: Min(Q-error) + Max(Binary Entropy)
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701084603.png)
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701084527.png)
            * Bernoulli Distribution
        * ä¸ºäº†è®­ç»ƒæ›´åŠ stableï¼Œå¯¹wå‡å‡å€¼å¹¶ä¸”norm
            * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701084704.png)
        * é¢å¤–åŠ å…¥äº†ä¸€ä¸ªoptimal bit-shift scalar
            * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701085049.png)
            * <<>>sè¡¨ç¤ºå·¦/å³shift
        * activationçš„binaryåˆ™æ˜¯æœ€ç®€å•çš„Sign
        * ä»Information Viewè§£å†³é—®é¢˜ï¼Œå…ˆbalanceå†binarizeï¼Œå»retainè¶³å¤Ÿçš„information
            * Libra-PBæœ‰bernoulliåˆ†å¸ƒä¸‹çš„æœ€å¤§information entropy
    * EDE
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200701102734.png)


# Training Details

> regular used BNN flow

### BATS

* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200726094832.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200726094903.png)





# Ideas

* Binarizeæœ¬è´¨ä¸Šæ˜¯ä¸€ç§æé™çš„Quantizeï¼Œæ‰€é¢ä¸´çš„é—®é¢˜å…¶å®ä¹Ÿå’ŒQuantizeç±»ä¼¼ï¼Œé™¤äº†å‡ ä¸ªåŒºåˆ«
    * Overflowçš„Tradeoffä¸å­˜åœ¨ï¼Œç”±äºåªæœ‰ä¸€ä¸ªé‡åŒ–åŒºé—´
    * åŒç†Roundingçš„æ“ä½œä¹Ÿä¸å­˜åœ¨
* ä½†æ˜¯å­˜åœ¨çš„Erroréƒ½æ˜¯å‰å‘çš„QuantizationErrorä¸ç­‰ç²¾åº¦çš„æ¨¡å‹è¿›è¡Œå¯¹æ¯” - Forward Error
    * å¦å¤–æ˜¯ç”±äºæ¢¯åº¦çš„é—®é¢˜å¯¼è‡´æ”¶æ•›ä¸åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„ç»“æœ - Gradient Error
* è§£å†³ä»¥ä¸Šä¸¤ç§é—®é¢˜çš„æ–¹æ³•ä¸»è¦æœ‰
* å‰å‘è¯¯å·® - æœ¬è´¨ä¸Šæ˜¯é€šè¿‡å¾®è°ƒ(æ•´ä½“çš„)åœ¨æœ‰é™çš„è¡¨è¾¾èƒ½åŠ›ä¸‹å°½å¯èƒ½çš„å»è¿‘ä¼¼Counterçš„ä¸€ä¸ªå…¨ç²¾åº¦æ¨¡å‹
    * Scaling Factor(æ›´ç»†ç²’åº¦çš„)
    * Adjustable/Learnableçš„NonLinear Function - PACT
    * Gradual/Iterative/Recursive(äº¤æ›¿çš„ä¼˜åŒ–binaryå‡½æ•°ä»¥åŠå‚æ•°)-æœ‰ç‚¹ç±»ä¼¼äºQuantize-aware trainingçš„æ€æƒ³
    * é€šè¿‡å¼•å…¥ä¸€å®šçš„éšæœºæ€§(ä¸ªäººè‡†æµ‹æ¯”è¾ƒç„å­¦ï¼Œä¸çŸ¥é“æ•ˆæœå¦‚ä½•) - ç±»ä¼¼Stochastic Roudningæ€æƒ³çš„(æŠ–è‰²æ·±çš„) - Half-wave Gaussian 
    * å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›(å¢åŠ æ¨¡å‹å¤§å°ï¼Œå †å )
        * çº¿æ€§ç»„åˆ(multiple binary basis) - ABCNet
        * KD
        * Wide
* åå‘è¯¯å·® - æœ¬è´¨ä¸Šæ˜¯ä¿®æ­£/è¡¥å¿æ¢¯åº¦æ‰€å¸¦æ¥çš„è¯¯å·®
    * Lossä¿®æ­£ - æœ€æœ´ç´ çš„æ˜¯è¿›è¡Œæ­£åˆ™åŒ–ï¼ŒåŠ ä¸€ä¸ªå¥‡å½¢æ€ªçŠ¶çš„é¡¹æ¥æ‹‰åŠ¨
        * æ¯”è¾ƒé«˜ä¸€ç‚¹çš„æ˜¯ä¸€ç³»åˆ—Differentiableçš„æ–¹æ³•ï¼ŒæŠŠLosså¯¼åˆ°å„ä¸ªéƒ¨åˆ†æ¥ååŠ©ä¼˜åŒ–
        * æ›´å¤šlearnable parameter - LQNet
    * å¯¹STEè¿›è¡Œä¿®æ­£

> å¯¹äºç©¶ç«Ÿå“ªç§æ–¹æ³•çœŸæ­£workï¼Œè®²é“ç†æ˜¯éœ€è¦æ·±å…¥å¯¹æ¯”å„ä¸ªpaperçš„å®ç°ç»†èŠ‚ä»¥åŠè‡ªå·±å»å°è¯•ï¼Œç›®å‰è¿˜æ²¡æœ‰å±•å¼€ï¼Œæ‰€ä»¥åªæ˜¯è‡†æ–­

---

* ç°åœ¨çœ‹èµ·æ¥å¯¹äºbinaryæ¯”è¾ƒå¥½çš„structureç›¸å¯¹è¿˜æ˜¯æ¯”è¾ƒunclearçš„
    * (å¬ä¸Šå»ååˆ†Straightforwardä½†æ˜¯æ€ä¹ˆè¿›ä¸€æ­¥å‘¢-è®²é“ç†ç±»Dartsçš„æ–¹æ³•å°±æ˜¯åœ¨è§£å†³è¿™ä¸ªé—®é¢˜)
* ä¸€ç¯‡Surveyè¿˜æœ‰æåŠåˆ°transfer binary net 

* æœ‰æ²¡æœ‰å¯èƒ½å„ç§ä½ç½®çš„å„ç§æ¨¡å—ï¼Œæ‰€éœ€è¦çš„Nonlinear functionæ˜¯ä¸ä¸€æ ·çš„ï¼Œå¦‚ä½•å°†å…¶åŠ å…¥æœç´¢(deriveè¿‡å»)


- [Binary Neural Networks: A Survey](https://arxiv.org/abs/2004.03333)
    * å­˜åœ¨çš„é—®é¢˜ï¼š severe information loss & Discontinuality
    * Explainableè§£é‡Š
        * some filters are redundnant(play a similar role)
    * æœ‰è¿™æ ·çš„æ“ä½œï¼Œå»è¡¡é‡æŸä¸€å±‚æ˜¯å¦é‡è¦-å°†å®ƒä»fpæ¢æˆ0/1ï¼Œçœ‹acc decay
        * traitioinal understanding for the layer sensitivity, is keeping the 1st and -1st layer with full precision
    * beneficial for understanding the structure of NN
    * ensemble to help training the BNN, but may face over-fitting 
    * åˆ†ç±» naive binarization / optimized binarization
        * åè€…é’ˆå¯¹quantize erroræˆ–è€…æ˜¯loss functionåšäº†ä¸€äº›é¢å¤–çš„æ“ä½œ
    * Preliminary
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200603142600.png)
        * BP: STE with clamp(-1,1)
    * Category
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200603142842.png)
        * å‡ ä¸ªæ–¹å‘
            * Minimize Q Error
                * Introducing Scale Factor(XNOR)
                * Hash Map with Scaling Factor(BWHN)
                * Train with Quantized Gradient, improved XNOR(DoReFa)
                * More Channels, imporved XNOR(WRPN)
                * Group and gradually Quantize
                * Recursive approximation (HOPQ-Higher-order residue quantization)
                    * not 1 step quantization, linear combination of each recursive step
                * Similar linear combination of binary masks - ABCNet
                * 2 Step Quantization 
                    1. only quantize activation with learnable function
                    2. then fix function and quantize weight (learn scale factor)
                * PACT
                    * learnable upper bound 
                * LQ-Net (Learnt Quantization)
                    * Jointly trainig the NN and the Quantizer
                    * Surrport arbitary bit 
                * XNORNet++
                    * Fuse w/a scaling factor together to support arbitary bits
            * Improved Loss func
                * ä¸»è¦focusåœ¨å¦‚ä½•å»globallyé€‚é…binarization(å¯¹åº”ä¸Šé¢çš„Q-errorçš„å°±æ˜¯æ›´focusåœ¨å±€éƒ¨ä¸Š)
                * LAB - Loss-aware-binarization  (quasi-Newton )
                * Problems
                    * degeneration
                    * saturation
                    * gradient mismatch
                * INQ (Incremental Network Quantization)
                    * é€æ¸æŠŠGroupæ‰©å¤§
                * Apprentice Network
                * DBNN - Distillation BinaryNN
            * Reduce Gradient Error 
                * ä¿®æ­£STE
                    * æœ€æœ´ç´ çš„æ–¹å¼ä¼šå¯¼è‡´åœ¨[-1,1]ä¹‹é—´çš„å‚æ•°ä¸ä¼šè¢«æ›´æ–°(?)
                    * æ ¸å¿ƒæ–¹å¼æ˜¯Softï¼ŒAdjustableï¼ŒSignFunction(ç„¶åä¸ç”¨STE)
                    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200605144853.png)
                * Bi-real: approx sign function(ä¸€ä¸ªäºŒæ¬¡å‡½æ•°)
                * CBCB - Circulant BNN
                * BNN+
                * HWGQ - Half Gaussian Quantization
                * Differentiable Soft Quantization


# BNN+NAS

* [Searching for Accurate Binary Neural Architectures]()
* Huawei Noah - ICCV19 W
* WRPN uniform expand
* only search for width(channels), acquire higher acc with less flops
    * encode channel num into ss, EA as optimization
* the arch remain the same with the original fp32 model
* DoReFaNet Forward
    * é™¤äº†ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚
* 4 is empirical upper bound of expansion ratio
    * [0.25,0.5,1,2,3,4]
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627094001.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627094100.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627094241.png)


* [Learning Architectures for Binary Networks]()
* GIST(South Korea)
* seems like eccv ...
* å·ç§°è‡ªå·±å¯ä»¥å’ŒSOTAçš„æ–¹æ³•æ‰“å¹³ï¼Œè€Œä¸ç”¨å¾ˆå¤šæŠ€å·§ï¼Œåªæ˜¯åŠ å¤§
* cell-based, proposed a new cell template composed of binary operations
* é¦–å…ˆå®éªŒç›´æ¥å¯¹Dartsç­‰æœå‡ºæ¥çš„ç»“æ„ç›´æ¥ç”¨XNORçš„binary scheme
    * æ•ˆæœå¾ˆå·®(å¾ˆåˆç†)
* novel-searching objective - Diversity Regularization  
* SS design
    * should be robust to quantization error
    * dialted convä¸ä¸€åŠconvå¯¹Q-erroræ¥è¯´ä¸€è‡´ï¼Œè¿™ä¸¤è€…ç›¸å¯¹å¯¹Q-erroræ¯”è¾ƒé²æ£’
    * separableæœ‰å¾ˆå¤§çš„Q-error
    * zeorise - è¾“å‡ºä¸º0ï¼ŒåŸå…ˆæ˜¯ç”¨æ¥å»ºæ¨¡æ²¡æœ‰shortcut connectionçš„è¿‡ç¨‹
        * æœ¬è´¨æ˜¯å› ä¸ºæœ‰æ—¶å€™binaryä¹‹åçš„è¯¯å·®å®åœ¨æ˜¯å¤ªå¤§äº†ï¼Œå¯¼è‡´æ¯”ç›´æ¥æŠŠç»“æœç½®0è¿˜å¤§
        * ä¿ç•™è¿™ç§å±‚å»å‡å°‘Q-Errorï¼Œè€Œä¸æ˜¯åªæ˜¯å°†å…¶ä½œä¸ºPlaceholder(?)
        * æœ‰ä¸€ä¸ªpossibilityæ˜¯å¦åŒ…å«zeroise
* Cell Template Deisgn
    * unstable gradient
    * å¼ºåˆ¶ä¸åŒCellä¹‹é—´å¸¦Skip - InterCell Skip connection(less quantization error)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200629131618.png)
* Diversity Regularizer
    * åŒåˆ«çš„æ–‡ç« ï¼Œä¸€æ ·å‘ç°äº†å¸¦å‚æ•°çš„opä¸€å¼€å§‹ä¸å®¹æ˜“è¢«é€‰ä¸­
    * exponential annealed entropy regularizer

* çœ‹ä¸Šå»åƒæ˜¯ä¸€ä¸ªå¸¦Hotfixçš„æ–¹æ³•ï¼Œä½†æ˜¯åšçš„è¿˜æ˜¯æ¯”è¾ƒsolidçš„


* [Binarized Neural Architecture Search]()
* Beihang Univ
* Darts foundation
* channel sampling / operation space reduction0
   * abbadon less potential operation
* åŸºæœ¬å°±æ˜¯PCDarts+Binaryå¤è¿°äº†ä¸€ä¸‹â€¦

* [CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks]()
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200629140108.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200629140956.png)
    * sample without replacement 
    * å¯¹Kä¸ªopæ¯ä¸ªsampleè¿‡ä¸€æ¬¡ï¼Œï¼Œå¾ªç¯Kæ¬¡ï¼Œä¹‹ååšSS reduction
* Pair Optimization for binary
    * minimize distribution error between fp32 and binary
    * minimize output-class inrta-class feature 

* [BATS: Binary ArchitecTure Search]()
* Cambridge
* è¡¨ç¤ºç›´æ¥æŠŠNASå¥—ç”¨åˆ°binary domainä¼šå¸¦æ¥å¾ˆå¤§é—®é¢˜ï¼Œæ‰€ä»¥éœ€è¦ä¸€äº›æ“ä½œå»alleviate
    * binarized ss
    * search strategy (control and stablilze the searching)
        * temperature-based
* binaryçš„æ–¹å¼
    * follow XNORNet - ä½†æ˜¯scaling factor åä¼ å¾—åˆ°è€Œä¸æ˜¯analytically
* search space
    * é¦–å…ˆè¡¨ç¤ºä¸€ä¸ªæ¯”è¾ƒå¥½çš„sså³ä½¿ç”¨random searchä¹Ÿå¯ä»¥è·å¾—æ¯”è¾ƒå¥½çš„æ•ˆæœ
    * è®¤ä¸ºdepthwiseæœ¬èº«å·²ç»æ˜¯compactäº†ï¼Œæ‰€ä»¥æ›´éš¾åšbinary - bottleneckä¹Ÿæ˜¯
        * high group size å»è¿‘ä¼¼ depthwise
        * æœ‰å°è¯•è¿‡åœ¨æ¯ä¸ªgroupä¹‹ååŠ å…¥ä¸€ä¸ªchannel shuffle,ä½†æ˜¯å…¶å®æ²¡æœ‰å¤ªå¥½çš„æ•ˆæœï¼ŒçŠ¹è±«groupä¸€èˆ¬æ¯”è¾ƒå¤§
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627095554.png)
* Search Strategy - å¯¹DARTSçš„ç¨³å®šæ”¶æ•›çš„å„ç§æ“ä½œ
    * æ—©æœŸå‘ç°ä¼šå¾ˆå¿«æ”¶æ•›åˆ°real-value op(æ¯”å¦‚polling and skip-connect),æ—©æœŸæ¯”è¾ƒæœ‰æ•ˆ
    * ç”¨temerpatureæ¥è§£å†³ï¼Œè®©æ•´ä¸ªåˆ†å¸ƒå˜å¾—æ›´åŠ spiky
* 2-Stage Search ç”±äºTraining Binaryæœ¬èº«æ›´å›°éš¾ä¸€ç‚¹
    * weights realï¼Œ activation binarized
    * ä¸ªäººæ„Ÿè§‰è¿™ä¸ªä¸å¤§é è°±â€¦ä¹Ÿä¸ä¸€å®šâ€¦
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627100500.png)

* [APQ: Joint Search for Network Architecture, Pruning and Quantization Policy](http://arxiv.org/abs/2006.08509)
* MIT Han
* å°†Archï¼ŒPrunä»¥åŠQuantize unififyåˆ°ä¸€ä¸ªæ–¹å‘
* è¶…å¤§SS,ç”¨ä¸€ä¸ªQuantize Predictor
    * è®­ç»ƒå…¶éœ€è¦ä¸€ä¸ª{FP,QUAN}çš„Acc Pair,éœ€è¦è®¾è®¡Quantize-aware finetune
    * å€ŸåŠ©Transefer Knowledge(ä»FP32 predictoråˆ°Quant predictor),æ˜¾è‘—æå‡Sample Eff
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200630235848.png)
    * ä¸»ä½“æ˜¯Evo+Predictor+Shared-Weights(OFA)
* OFA Training - progressively distill smaller subnets sampled from the OFA
    * MobileNet V2 Base
    * to handle SS è¿‡å¤§çš„æ—¶å€™OFAçš„subnetä¸å‡†
* Quantization Predictor
    * Arch and Quantize Policy encoding


###  [Binarizing MobileNet via Evolution-based Searching](http://arxiv.org/abs/2005.06305)

* ğŸ”‘ Key:         
	1. find a balanced bianry mobilenet, mainly in the group-conv domain
	2. weight sharing 
* ğŸ“ Source:      
* ğŸŒ± Motivation:  
* ğŸ’Š Methodology: 
	* BinaryScheme
		* scaling factor and backprop like XNORNeto
		* enhanced shortcut like MoBiNet & Birealnet
		* Polynominal differentiable approximation like birealnet
		* only weighs are binarized at train/testo
	* Flow
		1. pre-training
		2. sample grouping strategy and EA 
		3. determine the strategy and train from scratch
	* module modification
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200724163159.png)
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200724163208.png)
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:       
	* depth-wise + point-wise = depth separable conv
		* for binary the channel(depth)wise, less binary numbers are added together and has low precision, so cannot converge
		* so group conv could be a surrogate



