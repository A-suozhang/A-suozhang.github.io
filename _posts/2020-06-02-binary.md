- [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/abs/1511.00363)
    - 2015
    - Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David
- [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)
    - 2016
    - Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)
    - 2016
    - Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi
- [Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing](https://arxiv.org/abs/1603.08270)
    - 2016
    - Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, Dharmendra S. Modha
- [Ternary Weight Networks](https://arxiv.org/abs/1605.04711)
    - 2016
    - Fengfu Li, Bo Zhang, Bin Liu
- [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients](https://arxiv.org/abs/1606.06160)
    - 2016
    - Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou
- [Flexible Network Binarization with Layer-wise Priority](https://arxiv.org/abs/1709.04344)
    - 2017
    - Lixue Zhuang, Yi Xu, Bingbing Ni, Hongteng Xu
- [ReBNet: Residual Binarized Neural Network](https://arxiv.org/abs/1711.01243)
    - 2017
    - Mohammad Ghasemzadeh, Mohammad Samragh, Farinaz Koushanfar
- [Towards Accurate Binary Convolutional Neural Network](https://arxiv.org/abs/1711.11294)
    - 2017
    - Xiaofan Lin, Cong Zhao, Wei Pan
- [Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving](https://arxiv.org/abs/1804.06332)
    - 2018
    - Jiaolong Xu, Peng Wang, Heng Yang, Antonio M. López
- [Self-Binarizing Networks](https://arxiv.org/abs/1902.00730)
    - 2019
    - Fayez Lahoud, Radhakrishna Achanta, Pablo Márquez-Neila, Sabine Süsstrunk
- [Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization](https://arxiv.org/abs/1906.02107)
    - 2019
    - Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, Roeland Nusselder
- [Least squares binary quantization of neural networks](https://arxiv.org/abs/2001.02786v1)
    - 2020
    - Hadi Pouransari, Oncel Tuzel
- [Widening and Squeezing: Towards Accurate and Efficient QNNs](https://arxiv.org/abs/2002.00555)
    - 2020
    - Chuanjian Liu, Kai Han, Yunhe Wang, Hanting Chen, Chunjing Xu, Qi Tian
- [MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?](https://arxiv.org/abs/2001.05936)
    - 2020
    - Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, Christoph Meinel
    * 本质上是提出了一种新的Block
    * 指出了原本的BNN的优化方法有
        * 加大Channel数目
        * multiple binary basis
    * 认为BNN中的主要损失
        * FP32乘法与Binary乘法之间的误差
        * 统一的Scaling Factor所导致的Feature Map空间有限

- [Binary Neural Networks: A Survey](https://arxiv.org/abs/2004.03333)
    * 存在的问题： severe information loss & Discontinuality
    * Explainable解释
        * some filters are redundnant(play a similar role)
    * 有这样的操作，去衡量某一层是否重要-将它从fp换成0/1，看acc decay
        * traitioinal understanding for the layer sensitivity, is keeping the 1st and -1st layer with full precision
    * beneficial for understanding the structure of NN
    * ensemble to help training the BNN, but may face over-fitting 
    * 分类 naive binarization / optimized binarization
        * 后者针对quantize error或者是loss function做了一些额外的操作
    * Preliminary
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200603142600.png)
        * BP: STE with clamp(-1,1)
    * Category
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200603142842.png)
        * 几个方向
            * Minimize Q Error
                * Introducing Scale Factor(XNOR)
                * Hash Map with Scaling Factor(BWHN)
                * Train with Quantized Gradient, improved XNOR(DoReFa)
                * More Channels, imporved XNOR(WRPN)
                * Group and gradually Quantize
                * Recursive approximation (HOPQ-Higher-order residue quantization)
                    * not 1 step quantization, linear combination of each recursive step
                * Similar linear combination of binary masks - ABCNet
                * 2 Step Quantization 
                    1. only quantize activation with learnable function
                    2. then fix function and quantize weight (learn scale factor)
                * PACT
                    * learnable upper bound 
                * LQ-Net (Learnt Quantization)
                    * Jointly trainig the NN and the Quantizer
                    * Surrport arbitary bit 
                * XNORNet++
            * Improved Loss func
                * 主要focus在如何去globally适配binarization(对应上面的Q-error的就是更focus在局部上)
                * LAB - Loss-aware-binarization  (quasi-Newton )
                * Problems
                    * degeneration
                    * saturation
                    * gradient mismatch
                * INQ (Incremental Network Quantization)
                * Apprentice Network
                * DBNN - Distillation BinaryNN
            * Reduce Gradient Error 
                * 修正STE
                    * 最朴素的方式会导致在[-1,1]之间的参数不会被更新(?)
                    * 核心方式是Soft，Adjustable，SignFunction(然后不用STE)
                    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200605144853.png)
                * Bi-real: approx sign function(一个二次函数)
                * CBCB - Circulant BNN
                * BNN+
                * HWGQ - Half Gaussian Quantization
                * Differentiable Soft Quantization