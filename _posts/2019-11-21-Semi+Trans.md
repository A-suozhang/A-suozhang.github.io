---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      è®ºæ–‡é˜…è¯»ã€ŠWhen Semi-Supervised Learning Meets Transfer Learningã€‹          # æ ‡é¢˜ 
subtitle:   å¥½çš„åˆæ˜¯ä¸€ä¸ªè¢«æƒ³åˆ°çš„Idea   #å‰¯æ ‡é¢˜
date:       2019-11-21            # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/10_1/bg-xueyuanroad1.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - è®ºæ–‡é˜…è¯»
---

#  ğŸŒŸ [When Semi-Supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets](https://arxiv.org/abs/1812.05313)

* Combine The Semi scheme in Finetuning from the Pretrained Model
* **Incorporate SSL Into Finetune**
* Did Comprehensive Empirical analysis 3 Conclusion:
    * The SSL Gain from full-supervised baseline(with fewer label) is smalller
    * When Domain Shifts, SSL Better
    * Some SSL Methods can outperform full-label training

## Related Work

* SSL
  * Consistency-Regularization-based
    * Ladder Network
    * Pi-Model
    * Mean-Teacher
  * GANs & Adversarial Training
    * VAT - Virtual Adversrail Loss
  * Entropy-based SSL
    * Entropy-Minmization
  * Co-Training
    * Tri-Learning - Relabelling
* Transfer Learning
  * Finetune 

## Datasets

* Transfer from Imagenet
  * Indoor (67 Scene, 6700)
  * CUB200
  * MURA (åŒ»å­¦å›¾åƒ-éª¨éª¼çš„CT?)

## Experiments

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191121141351.png)
  * 20 - Epochs
  * 1k labels
  * F+T Augmentation
    * F - HorizontalFlip
    * T - RandomTranslation
    * N - Gaussian Noise

##ã€€Conclusion

* *when you have enough labeled images,better pre-trained models seem to counteract the influenceof SSL methods.*