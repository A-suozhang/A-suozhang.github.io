---
layout:     post                    # 使用的布局（不需要改）
title:      日知录          # 标题 
subtitle:   看看自己能够坚持多久       #副标题
date:       2048-01-01            # 时间
author:     tianchen                      # 作者
header-img:  img/10_1/bg-tz1.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - private
---
# 日知录


> 这个习俗大概来自于高中？FXB让我们记录下每天学到的东西，高中时候没有记住，不知道之后能不能记下来


### 2019-10-25

* FCCM - Online Learning
    * 1-6  / 
* 罗列了很多了，找一个比较小的点去展开
    * 我的思路还是在Hardware-Friendly的TL方法(或者说是UDA)
* 首先还是要想一下目前的方法怎么不Hardware-Friendly了
    * 肯定是有东西的，优先度不是特别高
* TODO:*尽快把算法给拿出来*
* 和Design无关的盲测的方法
    * 给HLS Debug - 反馈回来的Verilog不可读 
    * Pre-Sliicon : Logic，
    * Post： 电特性，
        * 复制一份进去，检错
* TODO: **尽快找一个可行的方案去干**
    * Async是想要获得一个比较好的分类器，训练了一个Labelling网络(Feature Extractor共享这个还是比较elegant)
      * 问题是训练方式不是很elegant
      * 或者说如果能work的话这个方法也不是不可
    * Semi证明维护一个数据集能更好
        * 前提是需要一个Acceptable的老师
        * 尝试下AdaBN能否解决这个问题
* ~~TODO:盖章的问题~~.
    * 颖姐那边能handle


---

### 2019-10-24
* 中午讲了一下PPT
    * 问题
        * 提出往Meta-Learning（学习如何去学习），不是很Feasible
        * 提出Teacher有了Student有什么用
            * 俺准备把Teacher也放终端
        * 场景:我还是踩着TL这个场景
    * *先把算法整出来才是最关键的*
        * 淦...
    * 算法上的Contribution在哪里
        * 凯哥提的，Semi文章第一个是堆了大量的数据，和那种不是特别一致
        * *看我的TrainingSet准备怎么构建了*
            * 单纯的拿置信度应该是不大行？
* TODO: 改PPT，炼丹，
    * ~~ Load一下ImageNet  ~~ 
        * 顺便搭一下ImageNet的训练环境 
            * 搭好了，但是...好像被磁盘读取卡了性能
              * 测试实际涉及和sda的读写,速度10kb/s
              * ~~LZZSCL...~~
              * 另外测试了一下我的/home目录底下
                * 看不到速度,读取速度也正常,没有和sda通信
              * *目前的推测是我们的home目录是一SSD,但是那个3T的玩意估计是个机械*
            * 要么是DataLoader的问题，要么是?
    * 看懂DAN的那个MMD是怎么弄的
        * (写个论文解读)
    * ~~远程Jupyter Notebook~~ 。
        * 出现了奇妙bug，不能从0.0.0.0启动jupyter
        * 对Tensorboard给搞定了！  
        * 同理Jupyter Notebook也解决了!
        * 方案放在EVA开发日记里了




---

### 2019-10-23

* 今天是炼丹的一天
* 又深入了解了一下pytorch的设计,还是有点东西
* ~~多卡非常爽~~
* 继续读了几篇文章,昨晚发现的这个online trasnfer的pruning有点东西,和我的主题很切合
* TODO: 继续炼丹
  * 还有看一下Data AUG和可视化怎么做
  * 接着研究代码
* 继续读文章,改PPT,准备和汪总的交流
* 文件还没有出,就很尴尬

---

### 2019-10-22
* 把无线电导航和通信网络补了（上午）
* 下午上课，跑了个餐卡手续
* 晚上接着配环境
    * 发现充钱之后我在服务器上不能上线了（很尼玛神奇）通过手动添加ip的方式上线了 
* 下午在配置WSL，这东西有点好使
  * 记在UbuntuSetUp一文中了

---

* 配置了Linux环境下通过跳板机直接访问目标机器
  * 一次ssh而不是两次
  * 主要参考了[这篇文章](https://blog.csdn.net/DiamondXiao/article/details/52474104)
  * 在.bashrc里面加上
  ``` bash
    alias eva="ssh zhaotianchen@101.6.64.144 -p 22 -o ProxyCommand='ssh -p 42222 zhaotianchen@101.6.64.67 -W %h:%p'"
    alias fpga="ssh ztc@fpga1.nics.cc -p 22 -o ProxyCommand='ssh -p 42222 zhaotianchen@101.6.64.67 -W %h:%p'"
  ``` 

* 按照上面的配置之后还需要再进行一次秘钥copy
  * ```ssh-copy-id  -i id_rsa.pub zhaotianchen@101.6.64.144 -p 22 -o ProxyCommand='ssh -p 42222 zhaotianchen@101.6.64.67 -W %h:%p'```
  * 说明从跳板机登录（2次）需要的秘钥和一次的需要是不一样的
  * fpga1.nics.cc -> 101.6.68.236(由于用ssh config文件这种方式登录的话不能用域名)



---

### 2019-10-21
* 上午看了几篇新的论文，和VisDA的比赛解决方案，发现多了一些可用的Trick，同时发现多了一些文章要读
  * 主要就是在Noisy Label下怎么取得比较好的效果
* 今天FAIR把Semi文章的模型开源了    
  * 但是代码没有开源
  * 在git上找到了一个人的复现代码，但是好像有一些不大Work，可以参考
* 下午上课，复习了一下无线电导航
* 中午和晚上办网络的手续（好狠）
  * 有一说一网很贵
  * TODO: 问一下凯哥怎么整训练数据
* 晚上配置eva0

##### 网络实验
* 正常时间可以用DIVI（这个不要钱，而且挺快的）不属于清华内网
    * 可以通过跳板机连接eva和各种东西
* 如果用Tsinghua Secure，相当于连入了清华内网了，就可以直接通过ip地址连接eva了（而不需要通过跳板机）
* eva需要上线
  * 和wiki上不一样我只要进去调用eva_share目录下的Tunet-2018c启动了之后就直接可以联网了（而且还是外网）
  * 运行之后理论上会一直占用这个Term，但是我CTRL-C强制退出之后连接没有终止
* 当然每天要去充钱
  * 冲10块，只能微信（因为理论上你在看到自服务界面的时候还没有进入清华内网）- 但是每天充钱好像必须首先在清华内网里（日）
  * 所以说充钱也是需要在清华内网里面的（日）
* 上线之后打开usereg的自服务可以看到eva的ip出现，并且开始跑流量了
* 实际测试了一下，如果账户里没有钱了，是不能正常用服务器登录的
    * 但是可以传文件上去，目前利用Win里的MobaXterm证明是可以的，在服务器没有联网的情况下把数据传上去了
* 第二天起身发现，我的网络在服务器上是连不上去的，只有通过手动添加ip上去，所幸这样的方式目前能work

---

* 遇到问题 1.其实和网络没有太大的关联
  > Release file for http://… is not valid yet (invalid for another xxd xxh xxmin xxs)
  * [这篇文章](https://www.cnblogs.com/outs/p/9706437.html) 据说是改一下系统时间
  * 这个问题过了一段时间自己消失了，很nb
    * （~~炼丹炉的自我修复？~~）
  * 第二天这个问题又出现了，修改至阿里的源，问题暂时消失`

---

* 配置一次性跳板机的登录，并且推送图形界面
  * Win：利用好MobaXTerm
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191021230929.png)
    * 按这个方式配置，可以直接上去，然后测试过可以推送图形界面

* VsCode Jupyter-Notebook Built-In的支持
  * 先命令行```Python: Interpreter```
  * 然后```Python：Create New Blank Jupyter```
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191022201033.png)
  * 很香，然而服务器端用不了，因为Remote上没有Python插件？





---

### 2019-10-20
* 上午改了下PPT，做了点作业，看到一篇有意思的文章[数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)

> 概率论只不过是把常识用数学公式表达了出来。    —拉普拉斯

* 贝叶斯方法，一种相当General的推理框架
    * 从*逆概率*的角度思考
        * 正思路：从一个已知内部黑白球个数的袋子里摸球，摸出不同球的概率
        * 逆思路：从我们已经摸出来的球的情况，分析袋子里球的分布
    * why？ 人的观测能力是有极限的 ~~我不做人了JOJO！~~ 
        * 对于上面的问题：*不太可能完全获得袋子里面球的情况*而只能通过“取球”这个过程观测之后的结果
    * 分析问题方法
        * 首先给出几种Hypothesis（这里可以理解为*猜测*）
        * 然后根据观测到的结果，计算特定猜测的*后验概率*，从而选择最“靠谱”的假设，进行了*最大后验估计*
            * 当这个过程不考虑先验知识的话，就是*最大似然估计* （贝叶斯方法比简单的最大似然就是多了一个这个*先验概率*）
            * 对于上面的场景，先验知识就是袋子里都是球，~~不可能摸出一个皮卡丘~~
                * 而对于Bigger Picture来说，最大后验的事件的先验概率可能很小
                * 当我们实在没有先验信息的时候（在我们看来先验就是一个均匀分布）这时候只能用最大似然
                * 这里也是统计学家和贝叶斯学派的
                    * 统计学家：Let The Data Talk Itself
                    * 贝叶斯： 数据本身就有偏差
                * 一个有趣的例子 - 树后面有几个箱子
                    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191020110240.png)
                    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191020110312.png)
                    * 先验告诉我们两个箱子不大可能，因为两个箱子刚好一样颜色一样高概率比较小
                    * 但是这样的先验是否靠谱？这两个箱子会不会是同一批？    
            * 对于连续的情况，就是计算*PDF-概率密度函数*
    * 简单的贝叶斯公式
     > P(B|A) = P(AB) / P(A)
    * Example
        1. ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191020105057.png)
        2. 拼写纠正问题 thew被修改成什么

> 如果两个理论具有相似的解释力度，那么优先选择那个更简单的

* 谈及奥卡姆剃刀
    * 比如*Overfitting*的模型，将一些观测的误差，都尝试去解释
    * *贝叶斯奥卡姆剃刀*
        * 修正我们上面提到的先验出问题的问题？
        * 如果一个先验导致我们拟合的模型更加复杂
        * 相当于*奥卡姆剃刀原则*也是一种先验？

* EM算法
    * EM聚类，先验知识是数据一般按照*正态分布*
* 最大似然与二乘拟合  
    * 直线给出的是相对最靠谱的直线，而偏移的被认为是噪声
* 朴素贝叶斯方法
    * 条件独立假设：激进的认为条件概率竖线右边的各个事件是独立的
    * 为什么靠谱？
        * 讲道理是一定有关联不可能独立的
        * 有[前人帮我们证明了这些关联可以相互抵消](http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf)

---

* 下午跑了修理，还没解决
* 下午修改发给汪总的PPT
    * 这个东西和incremental的联系
        * 充分利用数据，持续做训练，提升性能
    * 避免Overfitting
* TODO: 打印毕设需要的文件，约汪总一次线下
    * 还需要解决上网的问题，汪总回复之后问一下子把
      * 给颖姐发了邮件，等着回复吧...
    * ~~问一下凯哥是不是可以不存明文（可以存在cfg文件里面）~~
      * GG，看来必须自己搞一个账号了

---

### 2019-10-19
* 上午摸🐟+修电脑+空想是否思路有修改空间
* 下午读论文，改PPT
* 把Prim给做了
* 晚上给郭哥讲了一下
    * 看上去还比较feasible
    * ~~需要把基础知识部分改进一下~~
    * ❔考虑哪里可以hardware-friendly...      
        * 一个是训练集的构建方式

---

### 2019-10-18
* 上午整理思路做PPT
* 下午给雷老师讲PPT，整理论文阅读Post
    * 提的一个比较关键的问题是怎么个Hardware-Friendly
* 晚上补充文章和思路
    * 重新Review了一下TL的DataSet，还是有一定效果的
    * 接下来改一下PPT发给汪总把


---

### 2019-10-17
 
##### 组会
* RL在实际应用中获取对应平台的新数据
* 搭建仿真器来代替在实际场景中训练
    * 快，成本低，操作空间更大
    * 仿真器有抽象/非抽象(更难)
    * 仿真器的粗粒度和细粒度不一，且不同精度之间不能直接迁移(分布不一样)
    * 在类似*自动驾驶*等实际场景成本高的情况下
* 以多车避障为背景，构建一个仿真器
    * 局部最优-不动了就
    * 用激光雷达扫周围的东西
* 涛哥的Work把高低成本的仿真器给结合起来了
    * 对于一个局部子问题上用低成本的搜索可行策略，并且传递回高层次仿真器，类似一个“专家”Advice，或者说是“攻略”
* 重新讲了一遍RL基础（挺有价值的）
* 仿真场景
    * 有的时候很难遍历
    * 一个经典的N chains问题，可以向左或者右，左可以+1，右走到尽头再向右会获得很多
    * 小车坡度的问题，向右开不上去，需要向左先走一段再向右
    * Multi-Bandit 多个老虎机,某个老虎机的P很高，两种策略1.先对每个做一个sample，选最好的，玩到死 2. 去趋向效果更好的，偶尔也试一次效率差的（这个概率如何选择？）
        * 后验采样Thompson Sampling-不是Greedy的对每个分布去均值，而是从分布中采样，因而对于劣势的分布方差大的化也可能被认为是一个好方法
* 一个Work是在NN都SGD Framework下去找

##### Sensetime 宣讲
* 一个港中文的老师 - 王晓刚
* 演讲的标题是工业界落地的
* 整了个自己的计算平台Parrots
    * 多卡的一个操作
* On Hardware 软硬结合，算法和传感器以及芯片的结合，创新点
    * 提到的是硬件背景
* 工业布局
    * 智慧城市
        * 大规模的人脸识别；
            * Smart Locker
            * SenseID
        * 跨摄像头的视觉追踪
        * 3D
            * 人脸的建模与特征点（AR）
            * Vtuber(雾)
        * AR Navigation
* 干货，招人相关

##### Tusimple宣讲

* 找了货运这样一个相对更加合理的场景  
* 技术
    1. 感知 (主要用Vision)-卡车紧急刹车距离会比实际长，需要感知更远(Lidar)
        * 直接
        * 融合： 统一的表示，融合
            * *?时间上的Alignment*
            * 需要将图像从2D-3D,ill-posed,需要额外的辅助信息
            * 雷达的精度实际还是比较差，虽然能够直接获得速度（via 多普勒效应）
        * 1km的感知Demo
            * 一个激光雷达80m左右
            * 短距以及长焦的摄像头
    2. 定位
        * 卫星惯导-可行性不够
        * 摄像头：信息丰富，冗余而且不直接
        * 本质在于*匹配*
            * 与之前已经有的先验做匹配（也就是和实际的地图做匹配）
            * 绝对与相对相结合
    3. 路径规划
    4. 控制
* 工程
    1. 车载系统
        * 模块的调度
        * 异构平台支持
    2. 仿真平台-用于集成测试
    3. 基础设施
        * 大规模计算平台
    4. 硬件设备
        * 做自己的摄像头
* *？硬件平台*
* 广告
* 汽车不是刚需
* 5G做云端运算不是特别靠谱
* 雨天等复杂环境

---

### 2019-10-16
* 上午上课并且把银行卡的问题给解决了
* 中午整理了一下子装备哈
*  ~~等具体方案出来之后先和郭哥碰一下（大概周五左右） 约了周六下午 ~~   
    * ~~ 顺便问一下是不是需要把几个思路都放在里面  ~~  
    * 周末把PPT改好给汪总发过去
* 目前暂时的方案还是以Online Training为一个核心话题点，展开有大概3个思路，都可以Dig Deep
    1. Semi-Supervised
    2. Transfer
    3. Class Imbalance

---

### 2019-10-15
* 上午重新装Ubuntu...
    * 被代理卡了好久...
    * 搞定了ssh显示图形界面

##### 组会

* 讲了一下Semi的东西
    * 和云端的区分还是比较难，貌似只有*隐私问题*
        * 作为Online training会有一定的需求
        * 这个WorkFlow对应着一种Framework可能可行
    * 数据集维护，可以存一段时间  
        * 理论上只要有比较好的新的数据就可以了
        * 数据集的体量是多少？
            * 目前取得比较好的效果是10x原来数据集大小的样子（对于ImageNet来说我们的实验环境难以接受了）
    * 单纯的Classification提点数
        1. 实现起来不太可能，存储量太大了（这个Training一定是一个企业级的活，我自己做一定不能够这么做）
            * 除非换一个比较小的任务
        2. 本身只是为了提高一点点数，用处不是那么大
        * **往Transfer走相对会更有意义一些，需要深入思考和深入了解一下Transfer Learning**
    * 确实Online Training 梯度也会有精度损失，会不会产生影响
    * **Class Imbalance**也确实是一个需要解决的问题
        * TODO: 约意如神聊一下
    * 关于**Online Learning**
        * 是一个肯定存在需求的方向
            * 但其实还是取决与这个Online的场景能不能学到所谓"新的知识"
                1. 按照semi的workflow是完全没有的，还是原来的东西
                2. 按照Transfer的方案，其实是去后训一个Classifier
                    * 目前看来最Promising的一个⭐
                3. 能做到辨识新的东西
                    * 需要一个Incremental了，目前的算法感觉不是特别靠谱
        * 目前来说很多研究感觉*实验设计都不太考虑实际应用场景，很多都往所谓AI的“学习能力”在做，不是特别和硬件实现相契合关契合*
            * 说白了就是Training的这个Workflow还没有定论，甚至都没有人去解决Label的问题，如果我们最后的目标是硬件实现或者这加速的化，不是很Feasible
            * 所谓的Online偏向Continual“继续学习”的能力
* 看了一下实验室的手册，准备预约一下机器
    * 内容好...充实
    * 顺手学习一个
        * [LDAP(Lightweight Directory Access Protocol)](https://segmentfault.com/a/1190000002607140)-专业的分布式数据库，写性能差，用于查询，满足树状结构
        * ICP备案
        * BeagleBone是TI与Digikey联合生产的低功耗开源单板计算机（信用卡大小，可跑Linux）
    * ~~一些数据集在服务器share（或eva_share、/dataset等共享的）文件夹已经有了，可以先问一下你的负责研究生~~
    * ~~填写一下参考，占用的容量~~
    * TODO: 熟悉一下官网上的vim和tmux教程  ~~希望自己不要又从这个简单的弄起~~
* ~~ 需要改一下PPT先和汪总发一版 ~~
    * ~~需要看一下怎么用PPT高雅地画图，太丑陋了(暂时搁置了)~~
    * TODO: (比较后的，预计是对TL有了一个比较Feasible的方案之后)
        * TL还是相对比较Feasible的，具体的找算法已经思考还需要进一步看（可以给几个Example大概讲一下子他们的）

##### 平头哥讲座
* CTO，首席科学家 - 谢源
* DL-BigData-算力的三重Positive Feedback（画饼可以用）
* AI芯片场景
    * 服务器端；移动终端；物联网 - 需求不同 
* 产品
    * 玄铁CPU-RISCV-16Core-2.5GHz
    * 无剑Soc平台   
        * 缩短研发周期
        * 基于中天微的指令集
    * 含光 - AI云上推理
        * ~~都是剑的名字，很恶趣味~~
* 异构计算 Heterogeneous
* 半导体公司的3 Stage
    1. TI 传统方式
    2. TSMC
    3. HiSilicon - 由系统公司驱动而来
* RISCV的生态
    * 开源指令集的使用
        * 目前choice相对比较多x86和mips以及arm
    * Chisel - 面向对象的RTL描述语言 （但是编译器其实不开放）
        * 有尝试利用其做敏捷设计的开发
        * EDA工具上中国有欠缺
* 低功耗，终端(IOT)的芯片
 

---

### 2019-10-14
* 上午把雄哥作业做了，把显示器和硬盘安排了
* 读了一下Pytorch文档，有微小收获嗷👍
    * 微软自带的输入法居然可以打Emoji🐂🍺
* 晚上看了HLS，Vivado2017有奇妙bug，卸载换新的，好像ubuntu系统引导坏了

---

### 2019-10-13
* 继续修改和进一步思考开题的问题
    * 发现了很棘手的问题，这个YFCC不是很好用，有一定阻力，但是不是不行
    * 也有一些新的idea
        * 可以去证明一下定点的网络是否还是具有这样的性质
* 更加看了一下Transfer learning的tutorial
    * 感觉这玩意还是玄之又玄
* 复习了一下熊哥的课，名词太tmd多了，淦


---
 
### 2019-10-12
* 今天主要工作是做开题的准备PPT，稍微更加深入了一下文章
    * 数据集是怎么构建的
    * 大概是怎么训练的
* 如果需要把题目开小一点的话，可不可以直接从迁移学习入手，训这个CUB数据集，比较好训一点
* 读了Weakly Supervised Training
    * 网路对于Label Noise的容忍程度比想象中要高一些
* CUB2011数据集
    * 经常用来做*迁移学习*
    * 还有*细粒度的图像分类*
* 晚上还看了一些关于CNN加速方式以及TL的内容

---

### 2019-10-11
* 感冒依旧，躺了好久...还要参加一个1802的开学典礼，今天就继续思考了一下开题
* 找到了一个possible的开源代码(但是是做量化的) - 然而kill-the-bits里面还是只是开源了模型没有开源代码 ~~还是要自己刚~~


##### New Idea
* 想到了一个让Teacher和Student做**时分复用**的Idea，还ok
* 还需要看看Self-Training相关的东西
    * 貌似现在还是一个比较开放的Field，主要描述了一个在Unlabel数据集上利用某一网络进行标注，维护一个新的Training Set(这里有一些玩头)
* [EfficentNet提出对CNN结构设计的一些思考](https://mp.weixin.qq.com/s/T1ZwpaGO6PJR5Z6t2MULGQ)
    * 提出了**混合尺度变换**
    * 看来已经全面进入NAS时代了
    * 从3个维度开始考虑问题
        1. 深度
        2. 宽度 (体现在CH数目)
        3. 分辨率 (体现在输入图片的大小) 会影响到fine-grain的特征
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191011211753.png)
        * 将三种NN的尺度，混合起来限定为一种Block(代表这使用一定的资源？)，依据Block去搜索，相当于约束了搜索空间
            * 至于这个约束条件怎么来的，比较arbitary
* [Pytorch 1.4他lei了](http://pytorch.org/)
    * 更新了[移动端](http://pytorch.org/mobile )和[量化](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html)的一些操作


---

### 2019-10-10
* 十一之后刚刚回学校，就持续感冒，约莫有2~3天没好了，对工作时间的大量浪费，下面需要学习养生
    * 早睡早起
    * 能运动运动
    * 吃吃早饭
* **可持续发展非常重要**快些养病
* 学习了一下*通信网络原理*
    * 复习了一下之前落下的内容(还真不少...)
    * Chap2 排队论
        1. 一些基础的概念和描述
        2. MM/m/n模型 - 马尔科夫
        3. M/G/1 - 非马尔科夫，无限队列容量
    * Chap3 通信网络服务的建模
    * Chap5的一点尾巴
    * Chap6图论的相关内容(在通信网络的field内与路由密集相关)，记住的
        * 几个anecdote-哥尼斯堡7桥；四色猜想
        * *树*是任意两个节点之间只有一条边的
            * *引申到最小生成树*
        * 欧拉图，能够走回来的
            * 充要条件：每个节点的**度(degree)都是偶数**

#### 组会
##### ReRAM组的主要Contribution
* 梳理了一下RRAM组的主要contribution(2013-2019)
    * 达成了从器件-组成逻辑结构-系统架构-算法的完整Workflow
    * 在EDA层面搭平台，设计仿真器，以替代spice
* RRAM非易失存储，高阻表示0，低阻表示1
* NVM - Non-Volatile Memory非易失存储
* 说到了一个新坑（据说是汪总现在在Stanford讲的）
    * 针对不同的平台通过NAS直接搜架构，是比较高层的一个说法
        * *不做对应设计，直接暴力搜。。。太狠了*

##### 曾C讲的关于FPGA虚拟化的一些Work
* 主要是对FPGA云服务器，单块FPGA做多任务复用的
    * 不用TDM那样效率低，实际用的还是空分复用
* 存在一个动态负载（Dynamic WorkLoad）的问题，不知道新的任务什么时候到来
* 主要任务-提高修改FPGA配置的速度
    * 完整重配置-生成bit-让EDA去选择如何布线去生成架构，太慢了-需要1day
    * 基于指令集的FPGA设计(DPU)需要10-100s
        * 目前的指令集，缺少对*多PE可控并行*，以及*动态重配置*（将配置的指令分为Static和Dynamic的，Static的只需要执行一次）的支持
> 感觉自己对基于指令集的FPGA设计理解不够深刻，也就是DPU是如何开发的，每一步做了什么，映射到这个问题上面是怎么更新

##### New Idea
* Multi-Precision CNN 对CNN来说不同层的敏感度是不一样的（可量化位数）

---

## 🤔自省🤔
* 认为自己不熟悉的领域过于万能，希望别的部分能够万能。（本质上还是一种程度上的推卸责任）
* 想得太往后，比如这一次的Semi-Supervised Learning，郭哥说的现在先想怎么把故事讲好
* 自己很容易追求短期内工作/学习获得的短暂成就感，但是没有及时巩固(或者所选择学习的东西不太有机会及时巩固)，比如Graph的那次
* 越来越感觉DL的某个领域到了这几年发展的论文有种比较排斥的感觉，不知道是超出了自己的认知范围还是为什么，我总是感觉他们在找水点，而没有靠谱的方案，反思一下自己认知里的“靠谱”是不是鼠目寸光了

