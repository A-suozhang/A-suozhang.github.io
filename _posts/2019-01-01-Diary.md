---
layout:     post                    # 使用的布局（不需要改）
title:      日知录          # 标题 
subtitle:   看看自己能够坚持多久       #副标题
date:       2048-01-01            # 时间
author:     tianchen                      # 作者
header-img:  img/10_1/bg-tz1.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - private
---
# 日知录
* 这个习俗大概来自于高中？FXB让我们记录下每天学到的东西，高中时候没有记住，不知道之后能不能记下来

---

### 2019-10-10
* 十一之后刚刚回学校，就持续感冒，约莫有2~3天没好了，对工作时间的大量浪费，下面需要学习养生
    * 早睡早起
    * 能运动运动
    * 吃吃早饭
* **可持续发展非常重要**快些养病
* 学习了一下*通信网络原理*
    * 复习了一下之前落下的内容(还真不少...)
    * Chap2 排队论
        1. 一些基础的概念和描述
        2. MM/m/n模型 - 马尔科夫
        3. M/G/1 - 非马尔科夫，无限队列容量
    * Chap3 通信网络服务的建模
    * Chap5的一点尾巴
    * Chap6图论的相关内容(在通信网络的field内与路由密集相关)，记住的
        * 几个anecdote-哥尼斯堡7桥；四色猜想
        * *树*是任意两个节点之间只有一条边的
            * *引申到最小生成树*
        * 欧拉图，能够走回来的
            * 充要条件：每个节点的**度(degree)都是偶数**

#### 组会
##### ReRAM组的主要Contribution
* 梳理了一下RRAM组的主要contribution(2013-2019)
    * 达成了从器件-组成逻辑结构-系统架构-算法的完整Workflow
    * 在EDA层面搭平台，设计仿真器，以替代spice
* RRAM非易失存储，高阻表示0，低阻表示1
* NVM - Non-Volatile Memory非易失存储
* 说到了一个新坑（据说是汪总现在在Stanford讲的）
    * 针对不同的平台通过NAS直接搜架构，是比较高层的一个说法
        * *不做对应设计，直接暴力搜。。。太狠了*

##### 曾C讲的关于FPGA虚拟化的一些Work
* 主要是对FPGA云服务器，单块FPGA做多任务复用的
    * 不用TDM那样效率低，实际用的还是空分复用
* 存在一个动态负载（Dynamic WorkLoad）的问题，不知道新的任务什么时候到来
* 主要任务-提高修改FPGA配置的速度
    * 完整重配置-生成bit-让EDA去选择如何布线去生成架构，太慢了-需要1day
    * 基于指令集的FPGA设计(DPU)需要10-100s
        * 目前的指令集，缺少对*多PE可控并行*，以及*动态重配置*（将配置的指令分为Static和Dynamic的，Static的只需要执行一次）的支持
> 感觉自己对基于指令集的FPGA设计理解不够深刻，也就是DPU是如何开发的，每一步做了什么，映射到这个问题上面是怎么更新

##### New Idea
* Multi-Precision CNN 对CNN来说不同层的敏感度是不一样的（可量化位数）

---

### 2019-10-11
* 感冒依旧，躺了好久...还要参加一个1802的开学典礼，今天就继续思考了一下开题
* 找到了一个possible的开源代码(但是是做量化的) - 然而kill-the-bits里面还是只是开源了模型没有开源代码 ~~还是要自己刚~~


##### New Idea
* 想到了一个让Teacher和Student做**时分复用**的Idea，还ok
* 还需要看看Self-Training相关的东西
    * 貌似现在还是一个比较开放的Field，主要描述了一个在Unlabel数据集上利用某一网络进行标注，维护一个新的Training Set(这里有一些玩头)
* [EfficentNet提出对CNN结构设计的一些思考](https://mp.weixin.qq.com/s/T1ZwpaGO6PJR5Z6t2MULGQ)
    * 提出了**混合尺度变换**
    * 看来已经全面进入NAS时代了
    * 从3个维度开始考虑问题
        1. 深度
        2. 宽度 (体现在CH数目)
        3. 分辨率 (体现在输入图片的大小) 会影响到fine-grain的特征
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191011211753.png)
        * 将三种NN的尺度，混合起来限定为一种Block(代表这使用一定的资源？)，依据Block去搜索，相当于约束了搜索空间
            * 至于这个约束条件怎么来的，比较arbitary
* [Pytorch 1.4他lei了](http://pytorch.org/)
    * 更新了[移动端](http://pytorch.org/mobile )和[量化](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html)的一些操作

    
--- 
### 2019-10-12
* 今天主要工作是做开题的准备PPT，稍微更加深入了一下文章
    * 数据集是怎么构建的
    * 大概是怎么训练的
* 如果需要把题目开小一点的话，可不可以直接从迁移学习入手，训这个CUB数据集，比较好训一点
* 读了Weakly Supervised Training
    * 网路对于Label Noise的容忍程度比想象中要高一些
* CUB2011数据集
    * 经常用来做*迁移学习*
    * 还有*细粒度的图像分类*
* 晚上还看了一些关于CNN加速方式以及TL的内容

---
### 2019-10-13
* 继续修改和进一步思考开题的问题
    * 发现了很棘手的问题，这个YFCC不是很好用，有一定阻力，但是不是不行
    * 也有一些新的idea
        * 可以去证明一下定点的网络是否还是具有这样的性质
* 更加看了一下Transfer learning的tutorial
    * 感觉这玩意还是玄之又玄
* 复习了一下熊哥的课，名词太tmd多了，淦

---
### 2019-10-14
* 上午把雄哥作业做了，把显示器和硬盘安排了
* 读了一下Pytorch文档，有微小收获嗷👍
    * 微软自带的输入法居然可以打Emoji🐂🍺
* 


---

## 🤔自省🤔
* 认为自己不熟悉的领域过于万能，希望别的部分能够万能。（本质上还是一种程度上的推卸责任）
* 想得太往后，比如这一次的Semi-Supervised Learning，郭哥说的现在先想怎么把故事讲好
* 自己很容易追求短期内工作/学习获得的短暂成就感，但是没有及时巩固(或者所选择学习的东西不太有机会及时巩固)，比如Graph的那次

