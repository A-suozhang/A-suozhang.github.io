---
layout:     post                    # 使用的布局（不需要改）
title:      论文阅读《Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure》          # 标题 
subtitle:   现在的Prunning都已经这样了吗...   #副标题
date:       2019-10-28            # 时间
author:     tianchen                      # 作者
header-img:  img/bg-cat1.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 论文阅读
---

# [Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure](https://arxiv.org/pdf/1904.03837.pdf)

* 核心思想是用C-SGD,将多个filter在parameter space中collapse成一个点,并且在训练完后之后删除掉一样的
  * **理论上没有精度损失**
  * **Merge Multiple Filters**
  * 作者认为由于NN本质上是在做Linear变换,所以去掉完全一样的Fliter不会带来精度损失
* 切入点是*Channel Prunning*或者说是Filter Prunning
  * 这种方法可以适用于各种网络结构
  * 不会引入额外的网络结构
  * 与其他模型压缩方法正交

##　Related Work

* Filter Prunning
  1. 直接把filter按照重要性排序,去除末尾,再Reconstruct网络
    * *对当前一些有特殊结构的网络比如Dense不是很友好,需要带有Constraint的压缩方法*
  2. 为了能够更好的recover网络,预先将一部分内容置0
     * Group Lasso Regularization
     * 可以更好的恢复出NN
  * How 2 Measure Filter IMportance?
    * Accuracy Reduction
    * Channel Contribution Variance
    * Taylor-Expansion criterion
    * magnitude (好暴力)  
* Connection Prunning
  * 只能让网络变得更稀疏,但是需要特殊的硬件设计才能让加速加速起来
  * 

* 一般来说都是layer-by-Layer或者是filter-by-filter的
  * 都需要几个stage的Finetune

