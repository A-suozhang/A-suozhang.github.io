---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      Paper Wrting å¿ƒå¾— & Digest           # æ ‡é¢˜ 
subtitle:   é«˜è€ƒè‹±è¯­ä½œæ–‡éƒ½è¢«è¯´å£è¯­åŒ–çš„äººæ€ä¹ˆå†™æ–‡ç«     #å‰¯æ ‡é¢˜
date:       2020-04-21            # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/1_28/nantong-3.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡  
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
     - å¸¸ç”¨
     - é‡æ–°å­¦ä¹ 
---

# Paper Write æ³¨æ„ç‚¹

# æœ€ç»ˆæ£€æŸ¥çš„ç»†èŠ‚

1. ä¸€äº›Termçš„ä¸€è‡´æ€§
2. Fig. Figure Tab. Table æ˜¯å¦ä¸€è‡´
3. å…¬å¼/å›¾è¡¨çš„captionæœ€åæœ‰æ²¡æœ‰æ ‡ç‚¹
4. e.g. / i.e. åé¢å¯ä»¥åŠ é€—å·å¯ä»¥åŠ 
5. å•åŒå¼•å· 
6. æ‹¬å·å‰é¢çš„ç©ºæ ¼  
  - ~~(ä¸€ä¸ª10å—çº¢åŒ…)~~

# ç›¸å¯¹Metaçš„ä¸€äº›çŸ¥è¯†

- æ–‡ç« 8é¡µæ²¡æœ‰å†™æ»¡ï¼Œæˆ–è€…æ˜¯ä¸­é—´æœ‰å¤ªå¤šçš„æ¢æ®µåªæœ‰å‡ ä¸ªå•è¯çš„æƒ…å†µï¼Œä¼šè¢«äººè®¤ä¸º **ä¸å¤Ÿä¸°æ»¡**
- æ•´ä¸ªæ–‡ç« åº”è¯¥æ˜¯ä¸€ä¸ªHierachicalçš„ç»“æ„ï¼ŒAbsé‡Œæœ€é‡æ•…äº‹è¯´ç†ï¼Œintroé‡Œéƒ¨åˆ†å±•å¼€ï¼Œmethodé‡Œå°½é‡å±•å¼€

- ç”»å›¾ï¼š
  - ä¸è¦æƒ³ç€èƒ½å¤ŸåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼Œçªå‡ºæœ€å…³é”®çš„ï¼Œè¦Clean
  - Teaser(å¼€å¤´çš„å›¾) ä¸€å®šè¦ç®€å•ï¼Œå°±åªè¯´æœ€æ ¸å¿ƒçš„ä¸€ä¸ªç‚¹

- å®éªŒç»“æœï¼š
  - è¦å…¨é¢ï¼Œè€ƒè™‘åˆ«äººä¼šå¦‚ä½•Question
  - æœ‰äº›çœ‹èµ·æ¥ä¸€èˆ¬çš„è¦ä¸å°±å¹²è„†åˆ«æ”¾
  - ä»”ç»†è€ƒè™‘**å¦‚ä½•æ’å¸ƒä½ çš„å®éªŒæ•°æ®**è®©å®ƒçœ‹èµ·æ¥å¥½

- æ–‡å­—å±‚é¢
  - æ¯ä¸ªæ¦‚å¿µ(å®¹æ˜“æ··æ·†çš„ï¼Œè‡ªå®šä¹‰çš„) ç¬¬ä¸€æ¬¡å‡ºç°çš„æ—¶å€™åŠ ä¸€å¥è¯è§£é‡Šä¸€ä¸‹ ï¼ˆå¯¹äºè§‰å¾—å¾ˆé‡è¦çš„å¯ä»¥ç”¨è„šæ³¨ - ä¸€èˆ¬ä¸ç”¨ï¼‰
    - `\footnote{We summarize the geometric pattern and density of voxels as their ``geometric information''.}`
  - å…¬å¼ä¹¦å†™ä¹‹å‰ï¼Œæ¯ä¸€ä¸ªå­—æ¯éƒ½ä¸€å®šè¦å‡ºç°è¿‡ï¼Œä¸è¦è®©åˆ«äººLost
  - é¿å…ä¸€äº›é¢†åŸŸå†…æœ‰äº‰è®®/ä¸æ˜¾ç„¶çš„ç»“è®ºï¼Œå‡ºç°çš„è¯ä¸€å®šè¦æœ‰å¼•ç”¨
  - ä¸è¦è·³é€»è¾‘ï¼å†™ç»“è®ºæˆ–è€…insightåˆ†æï¼Œåªå†™å½“å‰å±‚çš„ï¼Œä¸è¦ç›´æ¥å¯¼åˆ°æ ¸å¿ƒè§‚ç‚¹


# æ–‡å­—ç›¸å…³ç§¯ç´¯

## å†™ä½œæ³¨æ„ç‚¹(Explicit)

- æŸäº›è¯å¯èƒ½ä¼šæœ‰ç‰¹æ®Šå«ä¹‰ï¼Œå¯ä»¥ä¸ŠGoogleç›´æ¥æœè¿™ä¸ªè¯ï¼Œæ¯”å¦‚`resort to`è´Ÿé¢
- æ³¨æ„å¤æ•°æ€§è´¨ä¼šå»¶ç”³åˆ°åé¢ 
- and
  - å‰åçš„**æ—¶æ€ä¸€è‡´**
  - è¡¨ç¤ºåŒçº§ï¼Œå‰åä¸¤è€…åº”è¯¥æ˜¯ä¸€ä¸ªLevelçš„  
    - ```$\cap$'' calculates the intersection of these two regions and is normalized by the geometric region size $N(\varphi(\theta_{ij}))$` è¿™å¥é‡ŒååŠå¥çš„ä¸»è¯­æ˜¯è¿™ä¸ªcapæ“ä½œï¼Œè€Œä¸æ˜¯å‰é¢ç´§è·Ÿçš„ä¸œè¥¿
  - æ³¨æ„ç”¨andè¿æ¥çš„å¥å­çš„**ä¸»è¯­**åˆ°åº•æ˜¯ä»€ä¹ˆ
- the
  - åè¯æ²¡æœ‰å¤æ•°ï¼Œæˆ–è€…æ²¡æœ‰å½¢å®¹è¯æè¿°ï¼Œå°±è¦åŠ the
  - ç‰¹è´¨æŸä¸ªä¸œè¥¿ï¼Œå¯ä»¥ç”¨the(ä¸ç”¨açš„è¾¨æ)

### å¸¸ç”¨è¯è¯­åŠSynonymæ•´ç†

* ample è¶³å¤Ÿçš„ï¼Œå¤šçš„
* alleviate ç¼“å’Œ
* asymptotic æ¸è¿›çš„
* excel è¶…è¿‡
* extensive å¹¿æ³›çš„
* subpar æ¬¡ä½³
* paradigm èŒƒä¾‹
* scheme æ–¹å¼
* generic ä¸€èˆ¬(æ™®é)
* derive è·å¾—
* compelling å¼•äººç©ç›®çš„
* consolidate å·©å›º
* coincide ä¸€è‡´
* deviation è¯¯å·®
* counteractive æŠµæ¶ˆ
* need-demand-requirement-desire éœ€æ±‚
* derivation æ¨å¯¼
* facilicate ä¿ƒè¿›ï¼Œå¸®åŠ©
* neutralize æŠµæ¶ˆ
* implication å«ä¹‰
* anticipate é¢„è®¡
* contemporary å½“ä»£
* prohibitive ç¦æ­¢çš„(å¾ˆå›°éš¾çš„)
* exploit åˆ©ç”¨èµ·æ¥
* leverage åˆ©ç”¨xxx
* specifically(More concretely) è¯¦ç»†å±•å¼€
* incorporate åµŒå…¥

### å¸¸ç”¨çŸ­è¯­

* In light of above
* is based on å¯ä»¥ç”¨
* seamlessly incorporated into / work in parallel with
  * å‰è€…çš„ä¸»è¯­ç¨å¾®å°ä¸€äº›
* seek to do
* be prone to æœ‰å€¾å‘
* put forward
* deem to è®¤ä¸º
* sth. degree (Lr decay degree)
* engage/involve with æ¶‰åŠ ï¼ˆengage with -> involveï¼‰
* in-depth comparison
* ad-hoc - (for this purpose only)
* serve-as:  ä½œä¸º

### å¸¸ç”¨ç¼©å†™

* ```et al.``` - æ‹‰ä¸è¯­ä¸­çš„et aliaï¼Œè¡¨ç¤ºä½œè€…æ—¶å€™çš„çœç•¥
	* etåé¢ä¸åŠ .å› ä¸ºetæœ¬èº«ä¸æ˜¯ç¼©å†™
	* å¦‚æœæ˜¯åœ¨å¥å­æœ€åï¼Œä¸éœ€è¦ä¸¤ä¸ª.
	* å¦‚æœåé¢çš„ä¸æ˜¯å¥å·ï¼Œé‚£åé¢çš„.è¿˜æ˜¯éœ€è¦çš„ 
* ```etc.``` - è¡¨ç¤ºç­‰ç­‰
	* et cetera - ä¸€èˆ¬æ”¾åœ¨ä¸¾ä¾‹çš„æœ€åï¼Œè¡¨ç¤ºå‰é¢çš„ä¾‹å­è¿˜æ²¡æœ‰ä¸¾å®Œ
	* ä¸ä¸Šé¢çš„åŒºåˆ«æ˜¯äººç”¨et alï¼Œæ²¡æœ‰ç”Ÿå‘½çš„ç”¨etc
	* å‰é¢éœ€è¦æœ‰é€—å·ï¼å’Œä¸Šé¢é‚£ä¸ªä¸ä¸€æ ·
* ```e.g.``` - for example
	* exampli gratia
	* ä»£æ›¿for instance, such as
	* å‰é¢æœ‰é€—å·ï¼Œæœ€åæœ‰å¥å·ä»¥åŠé€—å·   
* ```i.e.``` - ä¹Ÿå°±æ˜¯
	* id est
	* ç­‰åŒthat is, in other words
	* ä¹Ÿæ˜¯æœ€åæœ‰ç‚¹å’Œé€—å·

### å¸¸ç”¨è¯­å¥

* To sum up, we make the following contributions:
* The rest of this paper is organized as follows. The related studies are introduced in Sec.~\ref{sec:related}. In Sec.~\ref{sec:methods}, we introduced our fixed-point training. Then in Sec.~\ref{sec:exp}, the effectiveness of our method is illustrated by experiments. We further discuss XXX in Sec.~\ref{sec:discuss}.Finally, we conclude our work in Sec.~\ref{sec:conclusion}.


# å…¬å¼ä¹¦å†™

- TODOï¼š  æ„Ÿè§‰è‡ªå·±å¯¹è¿™ä¸ªæ²¡æœ‰ç³»ç»Ÿè®¤çŸ¥ï¼Œä¸çŸ¥é“åº”è¯¥ç”¨ä»€ä¹ˆæ–¹å¼æ¥è¡¨è¾¾


# Tools

1. detexä»¥æ­é…grammarlyä½¿ç”¨ï¼š å› ä¸ºgrammarlyä¸èƒ½å¾ˆå¥½çš„å¤„ç†latexï¼Œæ‰€ä»¥å­¦ä¹ äº†detexå·¥å…·æ¥å°†latexæ–‡ä»¶è¾“å‡ºæˆraw textï¼Œå°†å…¶æ‹·è´å¦‚grammarlyå°±ä¸ä¼šæŠ¥é”™äº†(ç¾ä¸­ä¸è¶³æ˜¯è¿™æ ·ä¸èƒ½æ”¹å®Œä¹‹åç²˜è´´å›åŸæ–‡äº†)

- Detex  `detex PaperForReview.tex | grep -v '^\s*$' > PaperForReview.raw`
  - sudo apt-get install libfl-dev
  - sudo apt-get install make gcc flex
    - ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20211118112458.png)
  - [](https://www.cnblogs.com/lovychen/p/7429682.html)
  - [DeTex æ–¹æ³•](http://keyvaneslami.com/blog/knowledge/opendetex-package-remove-tex-constructs)

2. 


# ç›¸å…³èµ„æºä»¥åŠç½‘ç«™

## Docs

* çŸ³å¢¨
* è…¾è®¯
* Google Docs
* Notion

## Writing

* Grammarly
* [Liggle](https://linggle.com/)ï¼š æŸ¥å¥å­æ­é…çš„ç½‘ç«™
* [Synonym](https://www.synonym.com/synonyms/) æŸ¥åŒä¹‰è¯çš„ç½‘ç«™ï¼Œä½†æ˜¯æ„Ÿè§‰ç”¨å¤„ä¸æ˜¯å¾ˆå¤§
* [New Better Synonym](https://www.thesaurus.com/browse/optimize?s=t)
* [ESODA](http://www.esoda.org/)ï¼š è´µç³»åšçš„ç½‘ç«™ï¼Œèƒ½åŒ…å«ä¸€äº›æ‰¾å¥å­ç­‰çš„åŠŸèƒ½
* æœ‰é“ç¿»è¯‘
* ç›´æ¥googleè¿™ä¸ªå•è¯ï¼Œçœ‹é‡Šä¹‰ä»¥åŠéšè—å«ä¹‰


## Bib
* [ResearchGate](https://www.researchgate.net/?_sg=9D8JkWoEvbx5lKepanzgM0bx2GcheNWitXm5LIwKovHl43ewI72-pQS0vPCyRAmRJA37PppxLEoD)
* [Semantic Scholar](https://www.semanticscholar.org/)


## Plot

* [Color Hex](https://www.color-hex.com/)
* [Matplotlib Doc](https://matplotlib.org/contents.html)
  * [Cmap](https://matplotlib.org/3.1.1/tutorials/colors/colormaps.html)
  * [Marker](https://matplotlib.org/3.1.1/api/markers_api.html#module-matplotlib.markers)
* [Matplotlib Tutorial(Not Official)](https://riptutorial.com/matplotlib/)
* [Mathematcia Doc](https://reference.wolfram.com/language/)
* [è¿™ç¯‡æ–‡ç« å›¾ä¸é”™](https://arxiv.org/pdf/2006.05467.pdf)




# å¿ƒå¾— (ä»–äººç»éªŒ)

* é€»è¾‘é“¾å®Œå…¨ï¼Œè®©ğŸµéƒ½èƒ½çœ‹æ‡‚
* å¤šæŠ„åˆ«äººå¥å­
* å…ˆå•°å—¦ï¼Œä¸è¦çœä¸»è¯­
* æ¯ä¸€æ®µçš„ç¬¬ä¸€å¥è¯è¦æ€»ï¼ˆæœ€å¥½æ˜¯æ€»åˆ†æ€»ï¼‰
* methodä¸­éœ€è¦ç»™å‡ºchallenge
* ä¸è¦å†™è¿›å»èŠ‚å¤–ç”Ÿæçš„
* å¯¹åº”ï¼Œå›¾é‡Œæœ‰çš„ä¸€å®šè¦å¯¹åº”åˆ°æ–‡å­—
* æ¯ä¸ªæ®µè¿˜æœ‰ä¸€ä¸ªå•è¯ï¼Œç¼©å¥ç¼©æ’ç‰ˆ
* ç”¨è¿å­—ç¬¦æ¥æŠŠæ¯ä¸€è¡Œå¡«æ»¡
* éœ€è¦æ£€æŸ¥çš„
  * åè¯ä¸æ˜¯å¤æ•°å°±è¦åŠ aæˆ–è€…the
  * å¤§å°å†™ï¼
* å›¾è¦æ”¾åœ¨æœ€ä¸Šé¢
* æˆªç¨¿å‰æ³¨æ„å¤‡ä»½overleafï¼ˆæ´»ç”¨overleafçš„historyåŠŸèƒ½ï¼‰
* åœ¨æœ€åç»Ÿä¸€checkä¸€äº›è¡¨è¾¾
* "x"ç”¨```$\times$``` dollarå·ä¸èƒ½å¿˜ä¸ç„¶ä¼šæ ¼å¼å´©å
* æŠŠé‡è¦çš„ä¸œè¥¿å¾€å‰æ”¾ï¼Œç„¶åå†è¯´ä¸ºä»€ä¹ˆ
* æœ‰äº›æ–‡ç« ä¼šç”¨ä¸€ä¸ªå•ç‹¬çš„sectionæ¥è§£é‡Šåè¯(å¯èƒ½å½“é¢†åŸŸæ¯”è¾ƒæ··æ­çš„æ—¶å€™)
  * ä»¥åŠä¸€äº›æ–‡ç« åœ¨expéƒ¨åˆ†ä¼šä¼˜å…ˆå°†å„ä¸ªbaselineä»‹ç»ä¸€å“ˆ





# Rebuttal

## å¸¸ç”¨ç´ æ

* å¼€å¤´çš„å®¢å¥—è¯

```We thank all the reviewers 1 for acknowledging our novel contributions and providing valuable feedback.```
```we address the common concerns followed by detailed comments from each reviewer```
```We thank all the reviewers for their insightful and constructive feedbacks! We will revise the final version according to this rebuttal.```
```Weâ€™ll carefully proofread the paper and fix the typos in the final version.```
```Thanks for the suggestion.Weâ€™ll make the claim more rigorous in our finalversion```
	

* å§”å©‰çš„å–·

```We suspect / There may be some misunderstanding```

* ç»™ACè¯´

```
We appreciate the constructive comments given by the reviewers.However, weâ€™d like to mention thataccording to the comments, Reviewer 4â€™s knowledge about GCNdoes not seem to align with other reviewers, e.g., asking whether better minibatch gradients of GCN is a problem, and asking why GCNs need dropout.Moreover, there seems to be misunderstanding on Theorem 1.
```





---

> ä¸‹é¢æ˜¯å¹´è½»æ—¶å€™å†™paperçš„ä¸€äº›å¿ƒå¾—ä½“ä¼šï¼Œå°´å°¬çš„å¾ˆï¼Œæ”¾åˆ°æœ€ä¸‹é¢æ¥äº†

### 2020-03-06 ECCV å¿ƒå¾—

* ç°åœ¨æ˜¯2020-03-06-03:25 æ–‡ç« äº¤äº†ï¼Œæ€»ç»“ä¸€ä¸‹å‡ ç‚¹å¿ƒå¾—å§
  * ç°åœ¨è‡ªå·±å†™æ–‡ç« çŸ¥é“è¿™å¥è¯åº”è¯¥å†™ä»€ä¹ˆä½†æ˜¯è¿˜æ˜¯è¡¨è¾¾ä¸å¥½â€¦
  * grammarly check
    * ä¸€èˆ¬æœ€ågrammarlyæŸ¥å‡ºæ¥çš„ï¼š 1. ä¸‰å• 2. the/a 3. æå°‘çš„æ­é…
    * the/a åŠ äº†æ²¡æœ‰check
    * åœ¨æäº¤ä¹‹å‰ç»Ÿä¸€ä¸€ä¸‹ä¸€äº›ä¸œè¥¿çš„ç”¨æ³•
  * marginæ˜¯ä¸ªå°bitchï¼Œæˆ‘è¿˜æ˜¯ä¸ä¼šè°ƒ
  * ç›¸å¯¹minorçš„ç‚¹å¦‚æœä¼šå¼•èµ·è¯¯è§£å»ºè®®çœå»ï¼Œè¿˜ä¸å¤§ä¼šå‡ºç°æ–‡ç« å†™ä¸æ»¡ï¼Œåªæœ‰å‹ä¸ä¸‹æ¥
  * æ—©ç‚¹æ‰¾**æ²¡æœ‰å‚ä¸è¿‡å·¥ä½œçš„ï¼Œæ²¡æœ‰é¢†åŸŸèƒŒæ™¯çš„**åŒå¿—çœ‹æ–‡ç« ï¼Œä¼šæœ‰æ–°çš„å‘ç°(å•Šä½ methodçœ‹ä¸æ‡‚å•Š)
  * ä¸è¦åå•¬å¼ºè°ƒï¼Œæ–‡ç« çš„é‡ç‚¹å°±paraphraseåå¤çš„è¯´
  * æ”¹å›¾çš„ä¸€äº›å¿ƒå¾—
    * å¯¹ç§°æ˜¯å¥½æ–‡æ˜ï¼Œä¸ä»…æ˜¯ä½ç½®å¯¹ç§°ï¼Œè¿˜æœ‰å­—ä½“å­—å·çš„å¯¹åº”
    * é…è‰²ï¼Œè¿™ä¸ªå€’æ˜¯å¯ä»¥å±•å¼€è®²ï¼Œç›®å‰æˆ‘çš„ä½“ä¼šæ˜¯ï¼Œé¥±å’Œåº¦æ‹‰ä½ï¼Œè‰²ç³»å€’æ˜¯å¯ä»¥ç¨å¾®è·¨è¶Šä¸€äº›
    * å­—ä½“å¤§å°æ”¾å¤§ï¼
    * å®é™…æ”¾åˆ°æ–‡ä¸­çš„å›¾é‡Œçš„å­—å¤§å°ï¼Œå’Œppté‡Œå­—å¤§å°ä¸å®Œå…¨ç›¸å…³è”ï¼Œæœ‰æ—¶å€™éœ€è¦æŠŠå›¾å‹æ‰æˆ–è€…æ‹‰é•¿æ‰æ˜¯çœŸæ­£å…³é”®çš„ï¼Œå› ä¸ºè®ºæ–‡æ ¼å¼é‡Œé¡µå®½å°±è¿™ä¹ˆå¤š


### 2020-05-01 æŒ‚arxiv

* æ³¨æ„arxivéœ€è¦çš„æ˜¯latex source code
* åœ¨æ–‡ä»¶çš„ä¸€å¼€å§‹éœ€è¦æŒ‡å®š```\pdfoutput=1```
	* å³ä½¿æ˜¯åœ¨texç¼–è¯‘çš„æ—¶å€™æ˜¾ç¤ºerrorä¹Ÿæ˜¯æŒ‰ç…§è¿™ä¸ªè§„åˆ™æ¥åš
* æ‰“åŒ…æˆä¸€ä¸ªå‹ç¼©åŒ…åŒ…å«æ‰€æœ‰source codeï¼Œéœ€è¦ä¸å«subfolderï¼Œé‡Œé¢ç›´æ¥æ˜¯å†…å®¹
	* ä¸»texæ–‡ä»¶æ˜¯è¦æ˜¯```ms.tex```
* æ³¨æ„arxivå¹¶ä¸ä¼šç»™ä½ è·‘bibtex
	* æ‰€ä»¥æäº¤çš„æ—¶å€™ä¸€å®šè¦å°†bblæ–‡ä»¶ç»™åŒ…å«äº†ï¼Œå¦åˆ™ä¼šæ²¡æœ‰å¼•ç”¨ä¿¡æ¯ï¼


### 2020-04-20 ICML2020 Rebuttal

* å¯¹äºç»™é«˜åˆ†çš„ï¼Œå…ˆèˆ”ï¼Œè¡¨ç¤ºXXå¾ˆé‡è¦ï¼Œç„¶åè¯´æˆ‘ä»¬è€ƒè™‘äº†XXï¼Œæ²¡åšä»€ä¹ˆæ˜¯ä¸ºä»€ä¹ˆï¼Œè¿˜ä¼šåšä»€ä¹ˆ

### 2020-05-26 ECCV Rebuttal 

* é˜…è¯»äº†[æµ…è°ˆRebuttal](https://zhuanlan.zhihu.com/p/104298923)
* ä½“ä¼šå®¡ç¨¿äººçš„æ„å›¾ï¼Œä»ä»–çš„è§’åº¦å‡ºå‘
  * å®¡ç¨¿äººå–·åªæ˜¯incremental/å‚è€ƒï¼Œè¦æ¾„æ¸…æŸéƒ¨åˆ†çš„ä»·å€¼
  * å¦‚æœæŸäº›è´±äººåªæ‹¿Noveltyæœ‰é™è¯´äº‹ï¼Œè¯´æ˜å®ƒæ‰¾ä¸åˆ°ç¡¬ä¼¤
    * è¿˜æ˜¯åªèƒ½è¯´æ˜åˆ›æ–°ç‚¹åœ¨å“ªé‡Œ,é›†ä¸­è¯´æ˜æˆ‘ä»¬çš„åˆ›æ–°ç‚¹
  * å–·ã€Œfactual errorã€ï¼šå®¡ç¨¿äººä¸€æ—¦æ‰¾å‡ºæ–‡ç« çš„äº‹å®æ€§é”™è¯¯ï¼Œä½œè€…ä¸å¦¨å¤§æ–¹æ‰¿è®¤ï¼Œå¹¶è¡¨ç¤ºæ„Ÿè°¢ï¼ŒåŒæ—¶è¡¨ç¤ºä¼šåœ¨final versionä¸­æ›´æ­£é”™è¯¯ï¼›å¦ä¸€ç§æƒ…å†µæ˜¯ï¼Œå¯èƒ½å°±æ˜¯å› ä¸ºä½œè€…è‡ªå·±æ²¡å†™æ˜ç™½ï¼Œæ‰ä½¿å¾—å®¡ç¨¿äººé”™è¯¯ç†è§£ï¼Œå¦‚æ­¤ï¼Œä¹Ÿå¯å¤§æ–¹æ‰¿è®¤ï¼Œè¯´â€œæˆ‘ä»¬å·²ç»ä¿®æ”¹äº†è¿™éƒ¨åˆ†æè¿°ï¼Œå®é™…ä¸Šæ˜¯è¿™æ ·åšçš„ï¼Œå¹¶ä¸æ˜¯ä½ ç†è§£çš„é‚£æ ·ï¼Œblabla
  * Rebuttalæ—¶è‹¥å‘ç°å®¡ç¨¿äººçš„factual errorï¼Œå¦‚taæå‡ºçš„æŸä¸ªè§‚ç‚¹æœ‰æ˜¾ç„¶é”™è¯¯ã€æå‡ºéœ€è¦å¯¹æ¯”çš„æ•°æ®é›†æ˜¾ç„¶ä¸æ˜¯è¯¥é¢†åŸŸå¸¸ç”¨çš„æ•°æ®ç­‰ï¼Œä½œè€…å¯åœ¨rebuttalå›åº”æ­¤äººæ—¶é¦–å…ˆæŒ‡å‡ºå…¶é”™è¯¯
  * rebuttalæ—¶ä¸è¦æ¼ç‚¹ï¼Œè¦é€ç‚¹å›åº”åšåˆ°æœ‰é—®å¿…ç­”ã€‚è‹¥å› ç¯‡å¹…æœ‰é™ï¼Œå¯å°†ç±»ä¼¼çš„æ„è§åˆæˆä¸€ç‚¹ï¼Œä¸‡ä¸å¯å› ç¯‡å¹…æœ‰é™æ“…è‡ªåˆ é™¤ä¸€äº›è¦ç‚¹æˆ–é—æ¼è¦ç‚¹ï¼Œä»¥å…é€ æˆå«ç³Šä¸æ¸…ã€æµ‘æ°´æ‘¸é±¼ä¹‹å«Œ
* **è¦æœ‰é—®å¿…ç­”**

### 2020-06-20 Trans Review 

* Review Trans Paper
* æœ‰ä»€ä¹ˆä¸æ‡‚çš„å°±åŠ Question
  * ç»™Major Revision / Minor Revision 
* Template

```
Decision: Major revision

Comments:
The authors argue that imposing uniform regularizations on filters during structured pruning is problematic. And they propose to modify the existing regularization-based structured pruning methods, by weighting the regularization of different filters based on the saliency scores. The saliency score is calculated as ||grad x scale||^2 / FLOPs reduction.

Strength:
The accuracy results on CIFAR-10 and ImageNet are impressive. 

Weakness:
You should analysis the computational efficiency of the overall pruning process. In you abstract, you state that the overhead of calculating the saliency is minimum, however, there is an inner iteration to calculate the saliency scores every epoch during SASL, which might be time-consuming. Also, the post-processing pruning and fine-tuning may be time-consuming. I'm aware that you utilize a hard-mining technique to accelerate the pruning process, and in Ablation 4), you said â€œhard sample mining strategy can not only reduce the complexity overhead for the saliency estimationâ€œ. I would like to see a quantitive study on the computational efficiency of your proposed framework.

The description of the method and experiment settings is not very clear, here are some queries about the method and experimental settings:
- How do you â€œclassifyâ€ the filters into different â€œ hierarchyâ€? Define thresholds, or just uniform distribute them according to the ranking?
- How many iterations are there in the iterative pruning and fine-tuning process? In each iteration, how many filters are discarded?
- In the ablation study 2), how do you change the base regularization value according to the number of classes? Did you experiment with class number larger than 5?
- In SASL* (a.k.a, the aggressive scheme), what hyper-parameters are tuned? The base regularization value in SASL, or only the post-processing iterative pruning process?
- How do you handle skip connections?


Other comments:
- Page 5 line 24, Page 8 line 30 (right column): wrong quotation marks
- I donâ€™t think hierarchy is a very proper terminology for describing different saliency levels.


From my perspective, i suppose using the product of the gradient magnitude and the weight/activation magnitude as the saliency signal is common in early studies, e.g., [1][2]. Thus, the main contribution of this paper is to propose to use the quotient of the commonly used saliency score and the FLOPs reduction as the new saliency score, and then, the saliency scores are used to determine the weighting value for the regularizer.

[1] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, Pruning convolutional neural networks for resource efficient inference, ICLR 2017.
[2] Michael Figurnov , Aijan Ibraimova, Dmitry Vetrov, and Pushmeet Kohli, PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions, NIPS 2016.
```


```
This paper proposes several techniques to improve dense video caption performance, including an event-level feature fusion, a scene-level topic predictor, an NMS procedure that takes both temporal and linguistic similarity into consideration. By combining these techniques, this paper achieves very good results. Also, this paper conducts comprehensive ablation studies on the proposed techniques.

Basically, I think this is a good paper. Although the event-level feature fusion method is simple, the ablation studies have demonstrated its effectiveness. And, I like the idea of using topic model to discover latent topics, and then using the topic distribution of each video as the scene-level training signal. And itâ€™s good to see that the ablation study demonstrates its effectiveness on the evaluation metrics. Is this idea original? 

I have some queries:
- In equation (2), why do you use asymmetric linear embedding layers, W_Q, and W_K, this doesnâ€™t seem like a reasonable distance metric.
- Page3 line39 column right, you say that â€œthe low-dim vector P may not be easily separableâ€. I think separable is not a proper word here, since we are not solving a classification problem.
```

```
TCSVT review: multi-feature adaptive late fusion image retrieval based on cross-entropy normalization


Review confidence level: 4
Technical content:
novelty: 1
Tutorial: 1
Technical correctness: 1
Technical depth: 1
Application: 1

Presentation:
clarity: 1
organization: 1
conciseness: 1
english: 1
references: 2

Decision: Reject

Comments:
This paper proposes to select reference curves for each query image by minimizing the â€œcross-entropyâ€, and then use the calibrated similarity curves to fuse multiple features.

Basically, I think there is a lot to be improved on this paper. 

(1) Writing and Clarity
This paper is poorly written. There are many grammar mistakes, many sentences cannot convey the points, and many terminologies are not used correctly. The authors should check the grammar, and polish the presentation.

(2) Organization and conciseness
      Related work: The summary of the related work is not well organized, the logic chain between subsections is weak.
      Method: The organization of the method section is not good, itâ€™s hard to follow your method description and capture the crucial points, and I must read for the second time to capture what you mean. For an example, the title of III.C â€œEffect Analysis of Cross-Entropy Normalizationâ€ does not fit with the content.

(3) Technical correctness and depth
     I have some doubts about the method. The description in the paper seems to indicates that the S curve for each feature is normalized, and this paper aims to find a reference curve that can calibrate each featureâ€™s curve by fitting its tail. However, It seems from Eq.4 that S_i(u:v) is normalized across different features, maybe the notation is wrong that the summation is not across the index â€œiâ€?  If so, the summation of S_i(u, v) across the u-v segment should be less than 1 instead of equal 1.
     
     This method is purely heuristic and empirical, I donâ€™t think the derivation about the positiveness of the â€œrelative entropyâ€ is suitable in a paper, since itâ€™s a fact that is well known in textbooks. Thus, I would recommend that you remove the two theorems in your paper.

      Why do you use â€œarea under curveâ€ to calculate the fusion weights. The only place that mentions this intuition is the II.D section where you discuss that [17] utilized this intuition. Can you provide more theoretical or intuitive background , or at least, can you experiment with other alternatives to demonstrate the rationality of this weighting scheme. 

     The word â€œcomplementarityâ€ is repeatedly used in the paper, you emphasize that your method of selecting the reference curve can â€œoptimizeâ€ the complementarity. However,  I fail to capture the logic behind this statement. According to your description in III.D, it seems like you think minimizing the â€œcomplementarityâ€ of the reference curve and the current featureâ€™s similarity curve is a way to â€œoptimizeâ€ the complementarity? I suppose that utilizing feature complementarity means that you should choose features that maximally complement each other, and this is not related to the proposed method.

(4) Experiments, Application of the method
     This method might not be so useful in application. The proposed method is an incremental one, and brings extra memory footprints and query latency overhead. 

     In IV.G, you said that â€œweâ€™re not going to make a direct comparison on the query timeâ€. I donâ€™t think the differences in the computerâ€™s performance or the feature number is the reason for not doing the quantitive latency comparison, since all these variables can be controlled. So, you should compare the query time with more baselines.
```



### ECCV Camera Ready

> å¼€å§‹å‡†å¤‡eccv2020çš„camera readyäº†

* éœ€è¦æäº¤çš„å†…å®¹æœ‰
	* source code(latex) + camera readyçš„pdf
	* copyright pdf - éœ€è¦ç­¾å
	* supplmentary material


* gates
	* baselineæ–‡å­—çš„å›¾ç‰‡å’Œæ–‡å­—æ”¹ä¸€ä¸‹


### CVPR2021 -  2020-11-28

* ç¬¬äºŒæ¬¡å®Œæ•´çš„å†™æ–‡ç« ï¼Œæµæ°´è´¦ä¸€ä¸‹
* é¦–å…ˆæ˜¯åœ¨CMTä¸Šæ³¨å†Œè·å¾—paper-id,å å‘
	- è‡ªå·±å†™çš„æ—¶å€™è¯­æ³•é”™è¯¯å¥½å¤šï¼Œè¿˜éœ€è¦é”»ç‚¼ï¼ŒåŒ…æ‹¬æ–‡ç« çš„è¡¨è¾¾æ–¹å¼ï¼Œä»¥åè¯»æ–‡ç« çš„æ—¶å€™è¿˜éœ€è¦å¤šç§¯ç´¯
	- å¼€ç€grammarlyå†™æŠŠ
* å…³äºçœ‹ddlå¯ä»¥åŠ ä¸ªç¾¤æˆ–è€…ç”¨[AI Deadlines](https://aideadlin.es/) - å¯ä»¥ä¸ç”¨æå¤æ‚çš„æ—¶åŒºæ¢ç®—äº†



### 2021-08-05 TIP Recycle

1. å¯¹æ¯ä¸ªrevieweréœ€è¦ç»™ä¸€ä¸ªDetailed Contribution

>  [ä½ äº†è§£ä½œè€…ä¸­çš„è´¡çŒ®è€…è§’è‰²åˆ†ç±»æ³•ï¼ˆCRediTï¼‰å—ï¼Ÿ - WORDVICEçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/374106127)

```
* Tianchen Zhao: Conceptualization and BNN related papers investigation, software design(BNN & NAS part), All Experimental Designs, Writing â€“ original draft
* Xuefei Ning: Conceptualization and NAS related papers investigation, software design(NAS part),  All Experimental Designs, Writing â€“ original draft
* Xiangsheng Shi: Software design and Experiments about BNN baseline methods, Experimental results Visualization
* Songyi Yang: Software design and Experiments about NAS baseline methods, Writing â€“ original draft
* Shuang Liang: Validation of the searched BNN, Writing - Review & Editing
* Peng Lei: Validation of the effectiveness of the NAS method, Writing - Review & Editing
* Jianfei Chen: Methodology verification, Writing - Review & Editing
* Huazhong Yang: Project administration
* Yu Wang: Resources, Project administration
```

