---
layout:     post                    # 使用的布局（不需要改）
title:      图神经网络              # 标题 
subtitle:   Elegant Structure     #副标题
date:       2019-09-06              # 时间
author:     tianchen                      # 作者
header-img:     #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:    
    - DL                          #标签
---
# Graph Neural Network
* 图是一种很灵活多变的结构
    * 如果没有边，就会退化为*集合*
    * 如果只有垂直的边，就会变成*树*
* 与NN的联系：
    * 大部分的数据可以被看做“图”
    > 对于二维图像，不从像素入手，而是将图看做一种“超像素”
    * 还可以反映一些一些先验知识：
        * 比如： 人的姿态估计，面部识别
    * NN本身也可以看做是一张图（计算图）
    * 很多Task先天的类似图：
        * 知识图谱；动态对象的交互，问答，3D Mesh分类（图可以描述一个球面上的形状）

## CONV IN GNN
### WHY CONV?
* 也就是conv相对于fc的优点，会带来一些自然的优势
    1. 平移不变性
    2. 局部性
    3. 层次结构 （浅层低级纹理特征，深度高级特征）
> 这些属性对于设计GNN有启发意义
### An Exmaple To Analog NN to GNN
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190907214505.png)
* 图像描述
    * 对于mnist的图像，用24*24=784个像素来描述，对应地采用784个节点(Node)来描述
    * 对于位置比较近的像素，边(Edge)有一个相对比较大的值
* 计算描述
    * 将卷积抽象为了一种“滤波器”，也就是“聚合算子”
    * CNN和GNN的区别主要在**卷积是对顺序敏感的，而图是无序的**
    * 对于GNN我们也需要定义“聚合算子”得到一个好的**聚合器**
        * 平均值
        * 求和 （都是以某个节点为中心，对相连的节点相加之后乘上W（W-是一个可训练的Tensor））
        * *人们管这种方式叫做GNN中的“卷积”，但是严格来说并不是卷积，因为他没有方向性，主要利用了卷积中**局部性**的特点*
### * A CASE：
* GNN的核心思想是**聚合“邻值”**（而一般什么样的值为邻近值由您定义）
    * 个人理解：可以抽象为是很多的，布朗运动的混乱的粒子，邻近的粒子相聚合，聚合成一个图像的形状，GNN就是建模这样一个过程
* 借助邻接矩阵 -（N*N（784*784）的邻接矩阵，描述每个节点对之间的距离）
    * 是一种**距离建模**体现了“相近的像素比较容易组成图案，而比较远的不大可能”这样的先验知识
    * 这样的抽象过于Naive，而且图像本也不是图网络最好的用武之地，单纯利用这样的方式并不太能获得有效的分类
* 由于图具有无序性，所以我们给这784如何排序其实是不对的，不可能找到一个正确的排序
    * 需要假定像素被随机调整位置
    * 新的问题❓: 如何知道在哪一行放置对应节点的特征？
        * 如果忽略的话，训练出来的结果相当于随机打乱
        * Solution： ```X_l+1 = A * X_l * W```只要保证A中第一行对应着X中第一行的特征就可以
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190907215659.png)
