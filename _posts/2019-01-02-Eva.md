---
layout:     post                    # 使用的布局（不需要改）
title:      Eva0开发笔记          # 标题 
subtitle:   （待填一个eva梗）   #副标题
date:       2020-01-01        # 时间
author:     tianchen                      # 作者
header-img:  img/10_1/bg-eva.jpg  #这篇文章标题背景图片  
catalog: true                       # 是否归档
tags:                               #标签
     - 环境配置
     - 常用
---

# EVA 开发日记


> 本post中的内容相对deprecated，关于eva配置的内容已经更新到了[nicsefc-readme](https://github.com/thu-nics/nicsefc-readme)中，其他关于ubuntu配置的内容目前预计merge到了[post-Ubuntu配置](http://a-suozhang.xyz/2024/01/01/UbuntuSetup/)中

# 常见问题 Trouble Shooting

### A. 联网相关

1. ping的通域名，但是登录丝毫没有反应？
- 基本是域名没有准入，上usereg用自己的校园网账号进行准入。注意准入有校内和校外两种情况，校内不设置上限，校外(可以连接外网)设置上限(如果有新的设备登录可能会被挤掉)
- 个人在这里建议是用校内准入，需要联外网的时候登陆上服务器再使用脚本进行校外准入。
- 服务器脚本的获取方式 `scp -P 42222 zhaotianchen@101.6.64.67:/home/eva_share/auth-thu.linux.x86_64 ./` 用你自己实验室账户的密码进行登录

2. 可以正常准入(显示"登录成功")，但是ping不通container的域名
- 是ip route出问题了，可以联系网管给删除路由，目前建议参考下方```"具体Case指南: Crontab自动删除路由，来自动化该流程"```

3. usereg中准入报错，“不在ip表中”或者返回一个空的对话框
- 大概率是学校那边ip分配出问题了，等一段时间再登录
- 实在着急的话找网管去给你重新拿ip(不一定管用也，网管能做的也只是attach进主机dhcp一下，可能IP变不了)

### B. 登录相关

 1. 能输入密码，一直报错Permission Denied
 - 再次检查自己的大小写等等，如果不是
 - 大概率是自己不在这个container的login.user.allow里面，联系Container的主人进行添加

#### C. CUDA相关

> CUDA在服务器重启之后很容易出问题，稍安勿躁

0. 发现nvdiia-smi没这个命令
- 把`/usr/local/bin`加到环境变量PATH中 

1. 发现了Pytorch啊以及各种程序(nvidia-smi)报错 'cuda error'或者cuda不available
- 首先进入`/opt/cuda-x.x/samples/1_Utiliies/deviceQuery`(cuda版本以你具体用到的cuda版本一致)**以sudo形式**执行deviceQuery，观察是否PASS，如果PASS了，那大概率是可以ok了
- 如果报错了`ERROR_CODE=100`的话，先执行`sudo ldconfig`，再执行上面的deviceQuery
- 如果再出现了别的错误，可以先google一下报错指令，如果没有解答的话联系网管

3. deviceQuery能够PASS，但是之后nvidia-smi和启动都很慢
- sudo运行`/usr/local/nvidia/bin/nvidia-persistenced`

#### D. Container相关

0. To-fill

---

# 具体Case

## 1. Crontab自动删除路由

**问题描述**: 服务器有的时候会登录不上，usereg可以入网，但是ping不通ip，也无法登录
**原因**: 是因为服务器网络启动后，LXC container的路由第一条默认路由走向了10.0.3.1，需要删除掉才能正常的从外网登录；执行`ip route`命令可以看到红色框里的条目应该被删除(用户视角应该看不到这条路由，因为如果有的话会无法登录，但是之后可能在某次重启之后会出现)

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210908190049.png)

**具体解决方法**: 使用[crontab](https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html)脚本定期删除该条路由

具体步骤:
0. 进入root (`sudo su`)
1. 首先在自己的container中使用`service cron status`命令检测cron服务是否开启，下图为正确现象

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210908185524.png)

2. 阅读[crontab命令的使用方法](https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html)并进行操作
执行`crontab -e` 编辑文件

``` bash
* * * * * /sbin/route del default gw 10.0.3.1
```

表示了每1min执行一次删除默认走向10.3.0.1的路由的操作(调试用)
(注意一定要用`/sbin/route`命令的绝对路径，而不是直接用route，虽然在bash里可以直接用route命令，但是crontab中缺少一些环境变量会导致route命令找不到)
3. 等待1min，再次执行`service cron status`，如果看到这个命令正确执行了

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210908190525.png)

之后重新修改为`* * * * * /sbin/route del default gw 10.0.3.1`来每分钟删除一次
并且执行`service cron restart`来使其生效

---

## 2. 通过修改ifmetric修改route优先级

> eva7上由于ubuntu18.04文件系统更新了route的规则，导致往10.0.3.1的路由成为了第一条，会导致ping不通container1.

1. 安装ifmetric包  `sudo apt-get install ifmetric`
2. 创建 `/etc/networkd-dispatcher/routable.d/60-ifmetric` 文件

```
#!/bin/sh
ifmetric eth1 200
```

- ~~ 有些尴尬的是暂时看不到结果，该操作仅会在container重启之后生效，经过这样配置之后开机之后route将会正常。~~
- 如果想检查效果的话可以手动重启network服务  `systemctl restart systemd-networkd.service`, 发现出现了metric=200就说明成功了

- ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210918202607.png)




# 网管相关

### LDAP related

本质上是开机之后我们会与一个远程的服务器进行数据共通，拉取我们实验室账户的用户名密码，然后同步到服务器本地主机。(相当于全部用户都给useradd上去了)
另外container内部本质上也是通过和主机共享LDAP服务以达成的账户。

以及其实`/etc/login.user.allow`，本质上是一个与LXC或者LDAP invariant的东西，就是和ssh管理这一层的； 不过要注意自己useradd的用户不要和LDAP的用户重名

1. 登录LDAP系统
   * 通过22222port登录上205，然后将remote的80端口映射到本地12888端口
     * （由于默认打开方式会显示实验室网页，需要用modeHelper插件输入一个rule；Host； ldap.nics.cc; 开启之后就可以进入）
     * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210404142533.png)
2. 手动添加用户
   * 在用户列表中用搜索“Search”
   * 配置显示uidNumber
     * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210404142716.png)
   * 找到最后一个用户(uid最大的)，注意个数调大一点
     * 然后copy from existing profile

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210404143227.png)

3. 或者是让它们自己注册用户
   * 在实验室官网进Internal进行注册，完成之后应该就有实验室账号了
   * 验证问题: ****

## 服务器调试常用操作

1. 查看磁盘IO
   * 现象: vim保存文件卡顿，但是tmux切换窗口不卡顿
   * 使用`iotop`命令，可能需要手动安装一下，在container内部是会报OSError，需要主机上的sudo权限

2. (很关键) 查看服务器占用内存/CPU最多的程序
   * `ps auxw|head -1;ps auxw|sort -rn -k4|head -10 ` 内存（改成k3就是CPU，k4是内存，改成k5就是虚拟内存）
      * ps命令(process status):
        * 一些args: 一般用的是aux，其中au表示详细信息，x表示也列出其他用户所用的程序，e则是简略的，-u显示某个user的
        * 输出格式 `USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND`，所以上面的k3就是CPU
      * sort命令，通过管道实现
        * -n表示用数字来进行排序，默认是ascii字符码
        * -r 表示倒叙
        * -k 4 表示第四列   
   
3. 查看某个用户的进程`top -u zhaotianchen`

4. 发现谁在用卡去查一下ldap看一下个人信息
5. 查看服务:
  - `systemctl list-units --type=service`
  - `service --statua-all`

## 服务器配置相关

- [修改hostname](https://www.iplayio.cn/post/309740)


#### 配置CUDA

> 需要的素材: NVIDIA_DRIVER.RUN，两个配置相关的bash脚本，以及一个CUDA_xx.RUN(optional), 目前这些文件放在新eva0的/home/zhaotianchen/setup里面

1. Nvidia-Driver:

需要上nvidia官网下载，根据你卡的版本来定(我们基本上可以直接ref其他同类卡的服务器)
比如我们目前的`430.34.run / NVIDIA-Linux-x86_64-460.39-no-compat32.run`
记住不要**直接run这个文件，先配置以下文件！**

首先是`setup-nv-conf.sh`

``` bash
sudo touch /etc/ld.so.conf.d/nvidia.conf
# 将 nvidia 运行时库目录加入 ld 搜索目录
echo "/usr/local/nvidia/lib" | sudo tee /etc/ld.so.conf.d/nvidia.conf
ldconfig
 
# 将 /usr/local/nvidia/bin 加入 PATH
echo 'export PATH=$PATH:/usr/local/nvidia/bin' | sudo tee /etc/profile.d/nvidia.sh
sudo chmod +x /etc/profile.d/nvidia.sh
 
# 屏蔽 nouveau 模块
sudo tee /etc/modprobe.d/nvidia.conf < <(
cat << EOF
blacklist nouveau
options nouveau modeset=0
EOF
)
```

然后是执行`setup-nv-driver.sh`

``` bash
sudo apt-get install dkms

sudo ./NV-INSTALLER -a -s --dkms \
  --opengl-prefix="/usr/local/nvidia" \
  --installer-prefix="/usr/local/nvidia" \
  --utility-prefix="/usr/local/nvidia" \
  --no-install-compat32-libs
```

跑完之后应该就正常安装在`/usr/local/nvidia`的位置了
这个里面的bin文件夹中应该就已经有了nvidia-smi文件，正常情况就应该出东西了，安装完成

这之后应该安装CUDA，貌似可以直接从其他服务器的/opt/cuda-xx.； 或者使用CUDA_11.run的方式来安装

- **我们CUDA的位置和默认位置有区别，需要做以下调整，实现cuda标准化  **

由于我们的cuda以及nvidia-driver的位置与标准的不一样，正常都是在`/usr/local/lib`或者是`/usr/lib`当中，但是我们的cuda在`/opt/cuda`中，而nvidia-driver在`/usr/local/nvidai/lib`这样的位置，所以有一些例程会很容易报错在`/usr/lib`的位置找不到某些`cuda.h`或者是`libcuda.so`文件这样的问题。解决方案是手动添加软连接。

`ln -s /usr/local/nvidia/lib/* /usr/local/lib/`
`ln -s /opt/cuda-11.1/ /usr/local/cuda`


## LXC知识相关

* LXC能通过改变/var/lib/中的cfg以及其他内容动态管理各个container
* 常用命令
  * `lxc-ls -f` 查看各个container的信息
  * `lxc-start -n XXX` 启动container
  * `lxc-attach -n xxx` 从主机进入某个container
  * `lxc-top`
  * `lxc-monitor`

### Container维护相关

[ LXC Linux系统容器 - 某科学的最后之作 (yunfwe.cn)](https://yunfwe.cn/2019/09/23/2019/LXC Linux系统容器/)

##### 1. 如何开一个新的Container

   * 登录主机，进入root
   * /home/work/lxc 中有一个fork_lxc_new.py文件，有以下的输入args需要填写
     * -n： container的名字
     * -u： user，需要和/etc/login.user.allow对应，可以用逗号隔开
     * --rootfs-size: 一个int表示GB
     * --rootfs-prefix: 表示挂载在哪块硬盘上，输入比如`/mnt/sdd` (事先df -kh查下哪个盘有空间)
     * --bionic： 表示是否是18.04
   * 发个邮件告诉别人
   * 按照当前命令执行就可以从一些对应位置读取一些默认cfg并且执行将空白文件系统复制到对应的位置
     * /home/work/lxc/templates_lxc
       * config: lxc的cfg
       * login.user.allow（默认有网管的）
     * /var/lib/lxc/ubuntu-cuda-base/rootfs.new(默认的空白文件系统)

* 目前已经能够登录各个主机 eva*.nics.cc
  * 在本地config里是用ssh eva8 这种命令来直接登录
* srvAdmin组:
  * 代表了对主机和container的sudo权限
  * 虽然有container的sudo权限，但是还是需要container的进入权限；可以直接进入主机attach进去然后给`/etc/login.user.allow`
* [凯哥的网管部技术整理](https://www.yuque.com/doctor-kaizhong/lab-network)

#### 2. 改Rootfs的大小

* 先stop到当前的container
* 参考fork_lxc_new中脚本中的几行代码
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210730103216.png)
* 其中的rootfs path从`/var/lib/lxc/$NAME`中找rootfs所link到的位置，是`/data2/lxc/ztc`对应位置的确实是大文件
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210730103356.png)
* 放了一个脚本在`/home/woirk/lxc/resizefs.sh`

#### 3. 服务器自动入网:

* 入网的脚本auth-linux我放在205的`/home/eva_share`下了，之后可以从这里scp，以及跳板机上`scp -P 42222 zhaotianchen@101.6.64.67:/home/eva_share/auth-thu.linux.x86_64 ./`

* 在我的服务器的home目录下写了一个config，然后只要直接执行./auth就可以了，需要写一个crontab
  * [19. crontab 定时任务 — Linux Tools Quick Tutorial (linuxtools-rst.readthedocs.io)](https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html)
  * still not safe enough, since should save passwd

* 写一个crontab脚本 `ping info.tsinghua.edu.cn` 让它一直在内网中, and use the ztc.eva7.nics.cc to login

* 创建一个脚本之后，`crontab $FILENAME`,之后`contab -l`查看什么正在执行
  * `crontab -e `直接编辑
  * `crontab -r` 删除

4. eva7上的特殊路由方式，从某个端口登录服务器

* 在主机上用`iptables -t nat -L` 看了这两条rule，其中将32223端口绑定(目前还是把这种方式改掉了…重启之后就没了，留档)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210911200948.png)

#### 4. 修改rootfs-template

* 由于lxc可以在`/var/lib/lxc`中直接修改配置以及rootfs
* 所以直接在该目录下新建一个sandbox文件夹，并且拷贝一份其他container的config文件过来(需要修改name，rootfs的位置，以及网卡的hwaddr，否则会报错address already in use)，并且将需要修改的rootfs软连接到这里来

![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210909125956.png)

* lxc-ls查看应该能够看到新的sandbox container了，尝试start它，(可能会报错所以可以用 `lxc-start sandbox --logfile ./logs` 将报错信息打印到log文件中)
  * 如果启动遇到了一些问题需要改动这个rootfs，那么就将这个文件给挂载到某个目录下`mount rootfs /media/rootfs`,然后chroot到`chroot /media/rootfs`,进行修改文件系统
* lxc-attach进去，做改动，然后也不用保存，自动就改了

#### 5. 设置Route的Metric以Fix route问题

* 在ubuntu18.04之后由于route规则的改变，route规则会将`10.0.3.1`的一条放在首条，导致在外面无法登录
* 通过修改不同ip route规则的metric可以解决:
* 实现方式(目前已经在我们eva7上的base-rootfs中修改完成了)
  * 在`/etc/networkd-dispatcher/routable.d`中添加了一个脚本，内容为`ifmetric eth1 200` (通过ifmetric库实现，需要加到我们的默认rootfs里)
  * 这个脚本将eth1(主机上由lxc-bridge所产生的虚拟网卡的route)优先级变低，metric的值越高就越不优先
  * 另外还需要安装`ifmetric这个package`
* **注意其他服务器上的base-rootfs还是没有改过的！**

#### 6. 老服务器迁移计划

> revive的老服务器将以EOE命名，参考量产机，又名End Of Evagelion，表示退役之后重新加入使用的Eva
> EOE系列的container的Mac地址中的倒数第二个字节(2个16进制数)，为`dN`的形式，为了与eva的`eN`区分开，否则会造成dhcp错误
> EOE系列的tinc地址，将从`10. 4.205.50`开始进行排列(tinc-up与tinc-down中)

* [2021-09-11] 尝试将老eva7(现EOE-0)进行复活的时候，由于没有修改lxc config，导致container与新eva7的container中share了硬件hwaddr，从而导致DHCP步骤出错，让同一个ip地址被两个虚拟lxc环境，登录时出错： 
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210911195914.png)
* 需要修改lxc-config中的两个位置的hw-addr:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210911202514.png)
  * 我们一般的rule是e7表示eva7； f0表示eva0； 对eoe0，我暂时改成了d0。
  * `for name in `lxc-ls`; do sed -i "s/e5/d1/g" ./$name/config; done`
  * 完成了这一步理论上就可以正常登录了(用绝对IP)
* **目前对于eoe0只对了最新的wcy container进行了修改，如果老container需要起来的话，也需要做类似的操作，但是目前还没做**
* 为了让域名等生效，还需要配置好tinc以及dns，参考[凯哥的网管总结 - 网络配置](https://www.yuque.com/doctor-kaizhong/lab-network/lo5q51)中的整个流程
  * 修改hostname: 需要修改 `/etc/hostname` 以及 `/etc/hosts` ，使用hostname命令就可以看到hostname，由于实验室用到了tinc服务，在`/etc/tinc/tinc.conf`中也做修改，通过tinc将新的域名组推到205中，便于205上的DNS服务器更新域名解析。
* 即便最后`service tinc status`显示tinc-up有`exit with error code2`，但是能够正常的ping通`10.4.205.1`(205主机的ip)，暂时没有问题了；以及执行过了crontab脚本中将信息从tinc推送到205部分逻辑，也是ok的。
* 已经建立好的container的hostname，进入文件系统直接修改`/etc/hostname`这个是建立container的时候就确定的，在`fork_lxc_new.py`文件中，使用了`socket.gethostname()来获取本机的hostname`，下面一段逻辑是创建container的时候，将这些信息写入到了template_rootfs的`/etc`所对应的文件中
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20210911203937.png)


---

# SSH配置

* 现在的新的Config文件的内容

```
Host eva
	HostName 101.6.64.169
	Port 22
	User zhaotianchen
	ProxyCommand ssh zhaotianchen@101.6.64.67 -p 42222 -W %h:%p
Host fpga
	HostName 101.6.68.236
	Port 22
	User ztc
	ProxyCommand ssh zhaotianchen@101.6.64.67 -p 42222 -W %h:%p

Host 205
  ForwardAgent yes
  HostName 101.6.64.67
  User zhaotianchen
  Port 42222

Host eva*
  HostName %h.nics.cc
  User zhaotianchen
  ProxyCommand ssh zhaotianchen@205 nc %h %p

Host proxy-eva10
  HostName eva10.nics.cc
  Port 32222
  User zhaotianchen
  ProxyCommand ssh zhaotianchen@205 nc %h %p

Host ztc.eva10
  HostName foxfi-eva10
  User zhaotianchen
  ProxyCommand ssh -p 32222 zhaotianchen@eva10 nc %h %p

Host *.eva*
  HostName %h.nics.cc
  User zhaotianchen
  ForwardAgent yes
  ProxyCommand ssh zhaotianchen@205 nc %h %p

```

* 一些解释性的文字
  * nc表示不要再Proxy停下来
  * %h 表示了HostName
  * %p 表示了Port
* 经过这样的一套配置应该是可以用```ssh ztc.eva10```这样的命令直接跳过两次跳板机跳上去了
  * 对于一些额外的配置比如说端口映射，可以直接加载最后的这个指令中比如 ```ssh -L 10808:localhost:127.0.0.1 ztc.eva10``` 
* 关于上传私钥以免密码登录
  * 首先再本地进行ssh-keygen，会有一个id_rsa（私钥），还会有一个id_rsa.pub公钥（这个比第一个短一些）
  * 其实只是需要在服务器上将本地的id_rsa.pub的内容添加到./ssh/authorized_keys当中即可 
    * 用```echo XXX >> authorized_keys``` 这种形式
* If after loading keys, logging still requires passwd for eva-8/9
  * Try ```ssh-copy-key -i id_rsa.pub proxy-evaX```
* 可以通过添加 - 将服务器端口映射到本地 
  * ``LocalForward 127.0.0.1:$PORT_ON_SERVER 127.0.0.1:$LOCAL_PORT```
  * 等效于```ssh -L 16379:127.0.0.1:6379 user@host```
  * 反向的话可以用RemoteHost
* ssh的config相关的配置用```man ssh_config```可以查询
  * -L 表示LocalForward - 将本地端口映射到远端
  * -R 表示RemoteForward - 将远程端口映射到本地
* Github配置proxy加速clone
  1. 本地的v2ray在10809的端口上开启了http服务
  2. 通过端口映射在登录服务器的时候指定 ssh -R 12450:localhost:10809 建立remote的映射
     * 端口映射-L会出问题，不明原因
  3. 在git的config中这样写  ```[http] proxy=http://127.0.0.1:12450```
     * 在ssh_config当中可以 ```RemoteForward 127.0.0.1:12450(port on remote) 127.0.0.1:10809```
* 清华校园网准入服务器：
  * 从userreg还是太麻烦了
  * 使用更新之后的[auth-thu](https://github.com/z4yx/GoAuthing)，直接从release中下载可执行文件，然后直接调用 ```./auth-linux_x86_64 auth```输入用户名密码就可以了
* 注意SSH config中的`LocalForward`前面的是本地的IP，可以不用加localhost

---


## 联网相关(登录服务器)

> 联网真的是一件很费劲的事情

* 可能会出现登录不上服务器的情况
  * 大概率是因为没有准入

### 准入Tips

1. 对于不在罗姆楼网段的服务器(eva8/9/10)，我们一般在主机(eva8.nics.cc)上的32222端口会route到这台eva所对应的跳板机(proxy.eva8.nics.cc),该跳板机会同时在罗姆楼与其他服务器所在网段(如伟清楼)，在跳板机上可以登录到各个container

   * eva8主机(IP: eva8.nics.cc); eva8-proxy(IP: proxy.eva8.nics.cc); eva8的container(IP: foxfi.eva8.nics.cc) 三者IP都不一样，三者如果将其IP直接通过usereg准入，就可以直接登录

   * 登录不在同一网段的机器需要在cfg里额外有一个   

     ```
     Host proxy-eva8:
     	HostName: eva8.nics.cc
     	Port: 32222
     	User: 
     	Proxy: xxx
     	
     Host proxy-eva8:
     	HostName: proxy.eva8.nics.cc
     	User: 
     	Proxy: xxx
     ```

     * 这个cfg定义了eva8中的跳板机，其本质上也是一个container，开在主机的32222端口(主机通过修改路由NAT表自动将包都转发到了proxy container所在的IP) - 所以将主机准入或者是将proxy所在的container准入，都可以将其连入清华内网；proxy container将各个eva container与eva主机通过内网连接(不用每个container都准入，都可以通过这样跳转)
     * 当重启之后，只要将主机入网，就可以让所有的container通过方式A登录了。

2. 连外网

   * 一种是usereg直接输入ip
   * 一种是准入```~/auth-thu.linux.x86_64 auth```

3. 多服务器之间拷贝文件

   * 注意一下可能在cfg中有关于域名的alias


### SCP

* `zhaotianchen@foxfi-eva8:~$ scp ztc.eva7.nics.cc:/home/zhaotianchen/awnas/data/modelnet40_normal_resampled.zip ./`
* 某次出现了无论怎么准入都无法ping通的情况，发现是eva7的某个container的IP table出了问题，删除掉某条rule之后好了
  * 使用`route`命令，找到一个default的路由，gw是10开头的，手动删除
  * `sudo route del default gw 10.0.3.1`
* ![image-20210410105419818](C:\Users\A-suozhang\AppData\Roaming\Typora\typora-user-images\image-20210410105419818.png)

1. 各个服务器中间可能存数据集的地方

   * 只有eva7是`/data/eva_share_users`，其他的基本都是`/home/eva_share_users`


### 开机自启动

* [Linux开机自动化执行脚本的四种方法（真实案例分享）_清风阁-CSDN博客_linux开机自动运行脚本](https://blog.csdn.net/abc_12366/article/details/87552848)

## CUDA Stuff

* 如果smi不出来东西了去/opt/cuda-x.x中执行deviceQuery(可能需要sudo)

* 通过在bash中用 LD_LIBRARY_PATH来指定用什么cuda

  * 如果为空的话就会查询 PATH, 里面一般有 ```/usr/lib/cuda```, 查询之后应该在eva服务器中是一个软连接连接到了/opt/cuda里

  * $PATH变量本身通过"："分隔,后面的覆盖，常用的方式 ```PATH=PATH:some_new_path```


## Install  a few stuff

* 听从[初代目网管的意见](https://nicsefc.ee.tsinghua.edu.cn/wiki/dl:install)，安装MiniConda3
  * 文章中，conda公司和Intel有py，从它这里下的包CPU计算会更快说服了我（~~虽然不知道到底有多真实~~）
  * 而且conda也和pip兼容，何乐而不为呢
  * 之前不装conda是因为它起手就安了一万个包，原来是我没有发现miniconda

```
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod +x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh

// 打开一个新的Shell(重新登录一次)

conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes
```

* 安装了shadowsocks与Proxychains来加快git clone速度

* 安装opencv
  * ```conda install python-opencv失败```
    * ```pip3 install python-opencv```安装不上
    * 安装了conda之后，对应的python是pip对应的，而pip3对应的是老的python3
  * 中间报了  ```安装opencv时报错 ImportError: libXrender.so.1: cannot open shared object file: No such file or directory```
    * 直接 ```sudo apt-get install libxrender1```

* 安装tmux并配置
  * 按照网管给的tmux.conf启动
  * *修改一下tmux的快捷键prefix(CTRL+B实在是太远了,改成+x吧)*
    * 查看现在的Prefix Binidng```tmux show-options -g | grep prefix```
    * 在~/.tmux.conf中加上
      ```
       set -g prefix C-x
       unbind C-b
       bind C-x send-prefix
      ```
    * 使其生效:
      * 在tmux中 CTRL+B+(此时还没有改好)): 输入source-flie ~/.tmux.conf
      * 这之后的都ok了


* 对vim的安装和配置见[Ubuntu系统配置！](http://a-suozhang.xyz/2020/01/08/UbuntuSetup/)

* Pytorch的安装


  * 有的时候会无法自动匹配到所需要的cuda版本，或者是自动安装了更新版本的pytorch而导致无法cuda()
  * ck CUDA 版本`echo $PATH; echo $LD_LIBRARY_PATH`
  * 强制force安装某个版本 `pip install torch==1.7.0`

    * 但是即使这样有时候tuna mirror找不到当前版本，这个时候就要`pip install torch==1.7.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html`这样那直接从pytorch官方的stable源来下载

* eva0上nvcc貌似没有结果(不知道是不是和有好几个container相关)
  * 按照bing到的结果去看 /usr/local/cuda下的version.txt貌似也不是很行
  * 咨询了网管之后的结果
    * 在/opt/cuda-10.0目录底下有着cuda
    * *测试cuda是否能用*
      
      * 运行/opt/cuda-10.0/samples/1_Utilities/deviceQuery/下的deviceQuery可执行文件
      * **开机之后要首先运行一次这个脚本，否则cuda就用不了！**
      * 正确的现象
        
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191021215750.png)
      * 同时给出nvidia-smi结果
        
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191021200015.png)
      * 在pytorch官网选择安装: Conda-Python3.7-cuda10.1
        
        * *大概800M的下载之后就安装好了*
      * 测试cuda ok没有
      
        ``` python
        import torch
        torch.cuda.is_available()
        ```


* 🐛 Matplotlib.pyplot import的时候报错了 
    ```
    This application failed to start because it could not find or load the Qt platform plugin "xcb"in "".
    Available platform plugins are: eglfs, minimal, minimalegl, offscreen, vnc, xcb.
    Reinstalling the application may fix this problem.
    ```
    * 原因是缺少了Qt的支持
      * [在这里找到了解决方案 ]](https://github.com/OpenShot/openshot-qt/issues/1809)
      
      * ```sudo apt-get install libqt5gui5```
      
## SSH 配置相关-deprecated

> deprecated

* 那么如何登录eva呢？
  * 配置了Linux环境下通过跳板机直接访问目标机器
    * 一次ssh而不是两次
    * 主要参考了[这篇文章](https://blog.csdn.net/DiamondXiao/article/details/52474104)
    * 在.bashrc里面加上
    ``` bash
      alias eva="ssh zhaotianchen@101.6.64.144 -p 22 -o ProxyCommand='ssh -p 42222 zhaotianchen@101.6.64.67 -W %h:%p'"
      alias fpga="ssh ztc@fpga1.nics.cc -p 22 -o ProxyCommand='ssh -p 42222 zhaotianchen@101.6.64.67 -W %h:%p'"
    ```
  * 按照上面的配置之后还需要再进行一次秘钥copy
    * ```ssh-copy-id  -i id_rsa.pub zhaotianchen@101.6.64.144 -p 22 -o ProxyCommand='ssh -p 42222 zhaotianchen@101.6.64.67 -W %h:%p'```
    * 说明从跳板机登录（2次）需要的秘钥和一次的需要是不一样的
    * fpga1.nics.cc -> 101.6.68.236(由于用ssh config文件这种方式登录的话不能用域名)

* 服务器的跳板机一定要从一定端口登录，不然就登录Permission denied
	
* 比如说eva10.nics.cc必须要从32222 port访问
	
* 关于免密码登录
	* 本机的.ssh文件夹里有一对密钥，id_rsa.pub和id_rsa
	* 其中执行```ssh-copy-id -i id_rsa.pub $SERVER_NAME```实际完成的操作就是把id_rsa.pub里面的东西复制到服务器上的.ssh/authorized_keys里面
		* 由于win并没有ssh-copy-id 命令，可以直接登录服务器手动配置，或者是用scp把authorized_keys给传上去(在win下vim不好使的情况下)
		* 注意scp按照port来传输的命令是```scp -P xxx```
	* 完成复制之后，从**有私钥的服务器**登录到新的服务器，可以直接免密码登录（但是注意没有私钥的地方还是不行，所以从中间节点直接连最后是不行的）
	* 其他的比如known_hosts什么的其实不用改也可以

* ~~Tensorboard远程映射~~（Deprecated） Use ```ssh -L 10808:localhost:127.0.0.1 ztc.eva10``` 
  * [通过跳板机查看内网服务器的tensorbord](https://blog.csdn.net/Woody0729/article/details/80933014)
  * 配置新的登录Eva的方式(以支持端口传播,从而看到Tensorboard)
    * 需要登录两次,首先登录跳板机 with
      * ```ssh -L 16006:127.0.0.1:8008 account@跳板机ip```
    * 然后在跳板机上登录with 
      * ```ssh -L 8008:127.0.0.1:6006 account@服务器ip```
    * 最后直接在本地机器上的localhost:16006观察结果
    * *注意这边可能刷新和Load会有一定的延迟,稍微等一会Tensorboard的内容会逐渐load出来*

* 如果X11 Forwarding出了问题,打不开matplotlib,可以先

```
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
```


---

* [blog-1](https://deepzz.com/post/how-to-setup-ssh-config.html)
* [blog-2](https://mdgsf.github.io/2019/08/01/linux-ssh-study/)


---

# Ray Setup

1. 安装ray
2. 按照scripts/python ray备注中的格式设置ray(每个机器或者conda env只需要设置一次)
	* 需要记录下一个redis-dir
	* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200104165140.png)
3. 新建一个experiment文件夹,内部说明方法的名字
4. 写一个脚本生成新的yaml文件,放置在一个文件夹下
5. 调用脚本 python train_ray.py --redisdir=XXX $PATH_TO_YAML_DIR
	* 则会自动load下dir目录下的每个yaml文件
	* 每当一个实验完成之后会自动建立一个进程将新的实验添加进去
	* 有两个必须的arg
		* redis-addr 启动ray时候会给的地址
		* result-base-dir 存放log的地址
			* 会自动append上这次调整的参数的名字(也就是experiment的yaml setting所在的dir前面一层)
		* 再后面就是正常main函数的yaml所在的文件夹,会读取该文件夹中的所有yaml并且跑

* 记不住redis-addr怎么办,因为每一次start ray的时候会随机产生一个,所以我们可以指定一个,在ray的命令里
	* ```alias ray-start="CUDA_VISIBLE_DEVICES=0,1,2,3 ray start --head --num-cpus 10 --num-gpus 4 --object-store-memory 10000000 --redis-port=12450```

---

# Conda Virtual Env

* ```conda create -n $NAME python=3.7```
	* 最好直接在最后用python=3.7，这样会直接在当前环境下生成一个环境```miniconda3/envs/$NAME/bin/python```，更加不容易和global的python搞起来
	* 然后pip应该也是安装到了对应位置不用修改，然后用```pip install ipython```
	* 安装完成之后虽然```which ipython```正确，内容也正确，但是输入ipython还是global的 - 需要**decativate再重新登录虚拟环境以reload环境**
* ```conda deactivate``` 退出现有环境
* ```conda info -e``` 查看现有的环境
* ```conda remove -n $NAME --all```
* ```conda create -n $NEW --clone $OLD; conda remove -n $OLD --all```

* 有的时候更新了环境之后还pip还是指向了base里面的pip
	* 这个时候decativate一下然后再activate就可了
	* 注意一定要deactivate再activate，关掉panel再activate好像是没用的(貌似不是一直work)
		* 现在居然是先输入一个错的conda env名字然后再输入就好了……(人类迷惑)

* 有的时候会出现无论操作什么都segmentation fault的情况(甚至重装conda以及更新conda)
  * 这个是偶用```conda clean -a```来清楚本地的缓存(可能是因为中途网络错误导致状态崩颓)
  * [居然是一个CSDN的blog救了我](https://blog.csdn.net/u012110870/article/details/103800701)


* 如果需要rename一个env的话
	* 需要复制一个新环境 ```conda create --name NEW_NAME --clone OLD_NAME```
	* 然后删除老的环境 ```conda remove --name OLD_NAME --all```

* de了一个ipython环境的bug，一开始的错误提示是在环境中仍然使用了base环境的ipython
  * 由于$PATH内部.local/bin在conda/env/xxx的前面。所以优先调用了base环境下的ipython
  * 检查bashrc的时候发现并没有.local/bin这个东西，然后发现是在~/.profile当中改动了$PATH，将.local/bin放到PATH的最后
  * **基本conda环境中对不上的情况，包括pip和python，都是$PATH的问题，其运行准则是优先匹配前面的**

* [pack and move a conda env](https://superuser.com/questions/1287428/cant-rebind-ctrl-b-to-ctrl-a-in-tmux)



## 源码编译TF

> following [official guide](https://www.tensorflow.org/install/source)

1. 安装prequisite

``` py
pip install -U --user pip six numpy wheel setuptools mock 'future>=0.17.1'
pip install -U --user keras_applications --no-deps
pip install -U --user keras_preprocessing --no-deps
```

2. 安装[bazel](https://docs.bazel.build/versions/master/install-ubuntu.html)
  * **注意需要查看一下bazel的版本，在tensorflow源码的configure.py中有着对应版本所能接受的bazel版本**
  * 建议直接从官网，以及[官方git repo](https://github.com/bazelbuild/bazel/releases)下载binary安装包安装
  * ```chmod +X ./bazel-xxx```
  * ```./bazel-xxx --user```
    * 注意这里的--user很关键，会加入环境变量，并且默认是将当前版本的bazel安装到```~/bin/bazel```
  * 需要在bashrc中加一行
    * ```export PATH="$PATH:$HOME/bin"```
  * 还有一种替代方案，相当于是bazel的编译完的版本[bazelisk](https://github.com/bazelbuild/bazelisk)个人没有尝试
    * 其使用应该是替换bazel的，可以直接在bashrc中```alias bazel=$PATH_TO_BAZELISK```

3. 下载tf源码
  * (其实可能应该放在安装bazel之前)
  * ```git clone https://github.com/tensorflow/tensorflow.git```
  * 切换到想要的分支，默认为最新的master分只```git checkout branch_name  # r1.9, r1.10```

4. 配置Build
  * ```./configure``` - 别的都无所谓可以默认，就是要指定一下CUDA的位置```/opt/cuda-10.0```
  * 经过configure之后的内容是在
  * 有几个预设的bazel build指令
    * ```--config=mkl```
    * ```--config=v1``` 

5. 编译(最可能出错的一节)
* 经过我自己的体验，貌似checkout到老版本进行编译容易出问题(总之我是失败了)
* 默认的TF-2.0编译 ```bazel build //tensorflow/tools/pip_package:build_pip_package```
  * 对master分支编译1.x版本 ```bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package```
  * CUDA版本 ```bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package```
    * 上次跑这个是失败了
* 注意如果需要在同一个目录下编译多次tf，需要执行```bazel clean```
* 该步骤生成了一个```build_pip_package```的可执行文件，下一步使用

6. 生成whl
* 这一步貌似不容易出问题
* ```./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg```
  * 官方说如果是最新的需要加上一个nightly flag，```./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg```
* ````pip install /tmp/tensorflow_pkg/tensorflow-version-tags.whl```

### 安装nccl

> 不确定可能编译出来的tf需要安装这个东西

* [Nvidia官网进行下载](https://developer.nvidia.com/nccl/nccl-download)
  * 下载对应的cuda版本，需要的平台(x86)，需要的发行版(ubuntu 18.04)
  * (当前NCCL好像要求cuda>10.0/cuda-driver>一定版本)
* ```sudo dkpg -i nccl-xxx.deb```安装
* 添加key ```sudo apt-key add /var/nccl-repo-2.6.4-ga-cuda10.0/7fa2af80.pub```
  * 甚至我添加key还少了一个库 ```sudo apt-get install gnupg```
* ```sudo apt-get update```
* ```sudo apt install libnccl2 libnccl-dev```
* 安装的nccl.h在```/usr/include/nccl.h```

* 由于我的cuda并不在标准位置```/usr/lib```，所以需要在cuda的对应文件夹里创建两个软连接，才能让tf的configure能够detect到
  * ```/usr/lib/nccl.h``` -> ```$CUDA_PATH/include```
  * ```/usr/lib/x86_64-linux-gnu/libnccl.so.2``` -> ```$CUDA_PATH/LIB64```
### 安装android studio

* [Tutorial](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)
* [官网安装]()

