---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      A Survey Of Knowledge Distillation              # æ ‡é¢˜ 
subtitle:   The Most Advanced & Elegant Compression        #å‰¯æ ‡é¢˜
date:       2019-09-14              # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/bg-term.png  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - DL
    - ç»¼è¿°
---

# Knowledge Distillation
> è¿™ä¸å‡ºæ„å¤–æ˜¯ä¸€ä¸ªå¤§å‘,å°†æ¥ä¼šå°†å¾ˆå¤šèµ„æ–™æ•´åˆåˆ°è¿™ç¯‡åšæ–‡ä¹‹ä¸­,å…¶ä¸­æ¯ä¸ªå°éƒ¨åˆ†å¯èƒ½ç‹¬ç«‹ä¸ºå…¶ä»–æ–‡ç« 
> å¸Œæœ›èƒ½å¤Ÿå¥½å¥½åšæŒåšå®Œ

# Overview & ğŸ¤”Thinking

> åœ¨ç†Ÿæ‚‰ä¸€ä¸ªé¢†åŸŸä¹‹å‰,é¦–å…ˆæœ‰ä¸€äº›æ„Ÿæ€§çš„è®¤è¯†,å¯ä»¥è¾¹è¯»è¾¹æ€è€ƒ,å°†æ¥çš„ä¸€äº›æ€è€ƒå†…å®¹ä¹Ÿå°†æ±‡å…¥è¿™é‡Œ

* **Why Knowledge Distillation**
  * The Model Is Too BIG (~~å±è¯~~)
* **What Is Knowledge Distillation?**
  * Distill/Transfer Knowledge from (a set of)large cumbersome models to a light single model (without significant performance loss)
* **Trade Off** Bigger Model(If Properly Designed) Could **Perform Better** But **More Computation**
  * æç«¯æƒ…å†µ: Ensemble - å‡ ä¹èƒ½ä¿è¯æé«˜performance,ä½†æ˜¯çº¿æ€§å¢åŠ è®¡ç®—é‡
* Knowledge - Where did it from?
    1. The *Params W* in NN
        * è¿™ä¸ªè§†è§’æŒ‡å‘äº†*finetune*:
          * å…ˆåœ¨å¤§æ•°æ®é›†(ImageNet)ä¸Šè®­ç»ƒ,ç„¶åå†åœ¨å°æ•°æ®é›†ä¸Šå¾®è°ƒ(COCO),è®­ç»ƒçš„æ˜¯åŒä¸€ä¸ªç½‘ç»œ 
    2. The Learned **Mapping** (More Abstract)
        * Knowledege Distillation ä»è¿™ä¸ªè§†è§’åˆ‡å…¥
* **How Do Knowledge Distillation Do**
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910165840.png)
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910171736.png)
* Stuä¸æ˜¯ç›´æ¥ä»Training Dataä¸­è·å–æ•°æ®,è€Œæ˜¯å­¦ç€å»followTCH
  * æ‰€ä»¥å­¦ä¹ çš„æ˜¯Soft Target: ä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹å­,ä¸æ˜¯ä¸€ä¸ªone-hotçš„vectorè€Œæ˜¯å„ç±»åˆ«çš„softmax
  * ä¸ºä»€ä¹ˆè¿™æ ·å¥½?
    * ç›´è§‚çš„çœ‹,æ›´å¥½çš„è¿˜åŸçš„äº‹å®,å¦‚æœteacher model trainçš„ä¸é”™çš„è¯,ç¡®å®ä¼šæœ‰å‡ ç±»æ¯”è¾ƒéš¾åŒºåˆ†çš„,è¿™ä¸ªæ¯”åªæœ‰ä¸€ç±»æ˜¯å¯¹çš„æ›´åŠ æ¥è¿‘çœŸå®
      * å¯ä»¥è®¤ä¸ºæ˜¯TCHå­¦åˆ°äº†è¿™å‡ ç±»æ¯”è¾ƒç›¸ä¼¼,å¹¶ä¸”å°†è¿™ä¸ªä¿¡æ¯ä¼ é€’ç»™STU
    * é“ç†ä¸Šè®² **è¿™æ ·çš„Targetå…·æœ‰æ›´å¤§çš„ç†µ!** è€Œä¸”è¿™æ ·çš„ç±»ä¹‹é—´æ¢¯åº¦çš„æ–¹å·®æ›´å°
* Use **Softmax With Tempeature**

$$ q_i = \frac{e^{\frac{z_i}{T}}}{\sum_j{e^{\frac{z_j}{T}}}}$$

  * Distillation Lossåšäº†ä»€ä¹ˆ?
    * ç”±äºæ­£å¸¸NNè¾“å‡ºçš„ç»“æœä¸€èˆ¬æŸä¸€ç±»çš„ç½®ä¿¡åº¦å¾ˆé«˜,å­¦ä¸å¤ªå‡ºæ¥ä¸œè¥¿,ç”¨æ¸©åº¦å› å­T(*ä¸€èˆ¬åœ¨1~20*)å°†å„ç§ç±»åˆ«çš„åˆ†å¸ƒå¹³ç¼“ä¸€ä¸‹(ä¸ç„¶çš„è¯å°±æ˜¯ç±»ä¼¼ä¸€ä¸ªå†²æ¿€,~~æœ‰ç‚¹å±•å®½æ—ç“£çš„æ„æ€~~)
* ç›®æ ‡å‡½æ•°:
  1. åªç”¨Soft Target,è’¸é¦çš„æ—¶å€™TCHä½¿ç”¨Softmax with Täº§ç”ŸSoft Target,STUä½¿ç”¨æ–°çš„softmaxåœ¨transfer set(è¿ç§»ç”¨çš„æ•°æ®set,åŒºåˆ†äºTraining set),STUä¸TCHä½¿ç”¨ç›¸åŒçš„T
  2. åŒæ—¶ä½¿ç”¨soft&hard,STUçš„æŸå¤±å‡½æ•°æ˜¯Hardä¸Soft Targetçš„åŠ æƒå’Œ
     * Hintonç»™äº†ä¸€ä¸ªDark Magic: è®©Hard Targetçš„æƒé‡å°(å› ä¸ºåœ¨æ±‚å¯¼æ•°çš„æ—¶å€™æ–°çš„ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ç›¸å½“äºéƒ½ä¹˜ä¸€ä¸ª1/T^2,soft targetå¯¹æ¢¯åº¦çš„å½±å“å¤©ç„¶æ¯”Hardå°) 
  3. ä¸ç”¨soft ~~åºŸè¯~~ 
* Distillationä¸å…¶ä»–çš„Model Compressionçš„æ–¹æ³•(æ¯”å¦‚Pruning,Quantize)æ˜¯å¯ä»¥ç»“åˆçš„
* è¯•éªŒè¯æ˜Soft Targetæ˜¯å¯ä»¥èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨çš„
* **åç»­çš„å¾ˆå¤šç ”ç©¶è¯´æ˜ï¼Œæˆ–è®¸KDå¹¶ä¸éœ€è¦ä¸€ä¸ªå¾ˆå¥½çš„TCH**
  * ğŸ¤”æ˜¯å¦åªæ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„Soft Labelèµ·åˆ°äº†æ•ˆæœè€Œå¹¶éæ˜¯KD
  * å…¶ä»–ç ”ç©¶ä¸­çš„Dataset DistillationéªŒè¯äº†è¯¥è§‚ç‚¹ 
* KDå¯ä»¥å¹¿æ³›ä½¿ç”¨ä¸æ¯”å¦‚RLï¼Œåœ¨
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913111544.png)
* å‰æœŸå¤§å®¶æ–¹æ³•çš„ä¸»è¦åŒºåˆ«æ˜¯**What To Distill**
  * Origianl-KD: OUTPUT with Soft Target
  * ThinNet:     Intermediate Layer Weight
  * A Gift:      Flow Between Layer
* **Why Soft Target Work**
  * è®©å­¦ç”Ÿå­¦åˆ°ä¸œè¥¿,ä¸èƒ½åªç»™ä¸€ä¸ªç­”æ¡ˆ,éœ€è¦ç»™ä¸€ä¸ªå®Œæ•´çš„è§£ç­”è¿‡ç¨‹
* è¿‡å°‘çš„æ•°æ®ä¸èƒ½å¾ˆå¥½è¡¨è¾¾Teacherçš„æ‰€æœ‰Knowledge,æ‰€ä»¥éœ€è¦ä¸€äº›é¢å¤–çš„Data Augmentation
* **What To Worry**
  * ç›®å‰çš„ä¸€äº›Knowledge Distillationçš„æ–¹å¼æœ‰äº›çœ‹èµ·æ¥è¿˜æ˜¯æ¯”è¾ƒåƒåœ¨ç‚¼ä¸¹...é‚£ä¸ªå‹ç¼©BERTçš„æ–‡ç« çœ‹ä¸Šå»å¾ˆAppealing,ä½†æ˜¯åªæ˜¯æ¯”æœªè’¸é¦çš„é«˜äº†5ä¸ªç‚¹,è¿˜è¿œè¾¾ä¸åˆ°èƒ½å¤Ÿæ›¿ä»£æˆ–è€…æ˜¯ä¸Šçº¿
    * ç›®å‰æ°´æ–‡ç« çš„ä¸€äº›äººè¿˜æ˜¯å»å…³æ³¨æ€ä¹ˆæ ·é€šè¿‡ä¸€äº›trickå»è¯æ˜â€œèƒ½å­¦å‡ºæ¥â€ï¼Œå»æå‡ä¸€äº›ç‚¹
  * ä¸çŸ¥é“æ˜¯è’¸é¦èµ·äº†ä½œç”¨è¿˜æ˜¯å•çº¯çš„æ•°æ®é‡å¢å¤šäº†
    * å•çº¯çš„å¯¹æ¯”å°æ¨¡å‹ç›´æ¥è®­ç»ƒï¼Œå¾ˆå®¹æ˜“å°±èƒ½è¶…è¿‡
  * æ¦‚å¿µå¾ˆç®€å•,ä½†æ˜¯å®é™…å®è·µèµ·æ¥æœ‰å¾ˆå¤šTrickå±‚é¢çš„ä¸œè¥¿
  * ï¼ˆä¸ªäººæ„Ÿè§‰ï¼‰è¿™ç§æ–¹å¼æœ¬è´¨ä¸Šè¿˜æ˜¯é’ˆå¯¹OverParamï¼Œå¯¹äºä¸€ä¸ªè®¾è®¡å¾—æ¯”è¾ƒelegantçš„ç½‘ç»œæ¶æ„å‹ç¼©æ•ˆç‡ä¸é«˜ï¼ˆç±»ä¼¼äºDeep Compressionå¯¹VGGå’ŒResNetï¼‰
    * ç°æœ‰çš„è®ºæ–‡å°è¯•å»Distill BERTæ•ˆæœåªæ˜¯ç¨æœ‰æå‡ï¼Œå’ŒåŸæ¨¡å‹å·®è·è¿˜æ˜¯å¾ˆè¿œ...ï¼ˆæ„Ÿè§‰é‚£ç¯‡æ–‡ç« ä¸å…¶è¯´æ˜¯å»å‹ç¼©BERTä¸å¦‚è¯´æ˜¯åˆ©ç”¨BERTçš„çŸ¥è¯†ï¼ˆä½†æ˜¯ä½œè€…å¥½åƒæœ¬æ„å°±æ˜¯åè€…ï¼Œåªæ˜¯æ ‡é¢˜çš„æ„æ€æœ‰ç‚¹åƒå‰è€…ï¼‰ï¼‰
    * **Not Too Bad**ç†è®ºä¸Šä¸ä¼šå‡ºç°â€œå¯¹äºä¸€ä¸ªdistillå¾—æ¯”è¾ƒå¥½çš„ç½‘ç»œå°±ä¸èƒ½ç”¨å‰ªæå®šç‚¹å»å‹ç¼©â€è¿™æ ·å­çš„é—®é¢˜
    * ä½†æ˜¯æˆ‘ä»¬ä¹Ÿä¸æ˜¯åªç”¨æ¥åšå‹ç¼©ï¼Œå¯èƒ½è¿˜è¦è€ƒè™‘è¿™ä¸ªä¸œè¥¿å¯¹Transferçš„åº”ç”¨ï¼ˆæ„Ÿè§‰è‚¯å®šæœ‰äººæ‹¿distillationåštransferè¿™æ ·çš„ï¼‰
* **Where Did This Field GO**
  * è¯¥é¢†åŸŸåŸæœ¬æå‡ºæ˜¯ä¸ºäº†ä»å¤§æ¨¡å‹ä¹‹ä¸­æç‚¼å‡ºä¸€ä¸ªæ¯”è¾ƒå°çš„æ¨¡å‹(å¦‚å…¶åå­—æ‰€è¿°)
  * ä½†æ˜¯åç»­æ–¹å‘æœ‰äº›å˜åŒ–
    * å•çº¯ç”¨æ¥æ›´å¥½åœ°æå‡å•ä¸ªæ¨¡å‹çš„æ•ˆæœ,ç”šè‡³æœ‰ä¸€äº›æŠ›å¼ƒäº†TCH,åªæ˜¯äº’ç›¸å­¦ä¹ (Deep Mutual Learning)
      * æœ‰ä¸€äº›ç‚¼ä¸¹çš„æ„å‘³
      * æœ‰ä¸€å®šçš„åº”ç”¨ä»·å€¼
    * åº”ç”¨äºTransfer Learning
      * æ„Ÿè§‰ä¸æ˜¯å¾ˆå¤š...(å¾…å¯»)
      * A Gift è¿™ç¯‡æ–‡ç« æåˆ°è¿‡,ä½†æ˜¯ä¸æ˜¯ç‰¹åˆ«åˆé€‚
    * æ·±å…¥åœ°è§£é‡Šè’¸é¦çš„å«ä¹‰,æ›´å¤štrick,æ›´å¤šæ°´æ–‡ç« ...
      * Distill BERT
      * æ„Ÿè§‰æœ‰äº›è¢«å¼•ç”³ä¸ºæ€ä¹ˆæå–å‡ºå¤§æ¨¡å‹ä¸­æœ‰ä»·å€¼çš„éƒ¨åˆ†è€Œæ›´åŠ æ·±å…¥ç ”ç©¶DLçš„æœ¬è´¨äº†,æœ‰ç‚¹é£˜...

# Papers List
### 1. [FitNets](https://arxiv.org/abs/1412.6550.pdf)
  * Yohsua Bengio ICLR2015
  * åœ¨KDæå‡ºä¹‹åï¼Œæ›´è¿›ä¸€æ­¥ï¼Œæå‡º*ä¸ä»…ä½¿ç”¨æœ€åçš„ç»“æœ*è€Œä¸”ä¹Ÿä½¿ç”¨*TCHçš„ä¸­é—´å±‚çš„ç»“æœ*ï¼ˆç”±äºTCHä¸€èˆ¬æ¯”STUå¤§ï¼Œæ‰€ä»¥è¿˜éœ€è¦å¼•å…¥é¢å¤–çš„å‚æ•°æ¥åšä¸€ä¸ªæ˜ å°„ï¼‰ï¼Œå¦‚æ ‡é¢˜è¯´ï¼Œç»™äº†STUä¸€ä¸ª**hint**ï¼Œè®©STUå˜å¾—**Deeper&Thiner**
    * hintæŒ‡TCHçš„æŸä¸€å±‚ï¼ŒSTUéœ€è¦å»æ‹Ÿåˆè¿™ä¸€å±‚ ï¼ˆåœ¨Lossä¸­åŠ å…¥ä¸€é¡¹æ˜¯ä»–ä»¬ä¸¤å±‚çš„L2NORMï¼‰
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910211632.png)

### 2. [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
  * Hinton 
  * é¢†åŸŸçš„å¼€å±±ï¼Œä¸»è¦è´¡çŒ®å°±æ˜¯æå‡ºäº† Softmax-With-Temperature
  * Experimentï¼š
    * Mnist
      * å¸¦Distillçš„å°ç½‘ç»œï¼ˆä»å¤§ç½‘ç»œä¸­è·å¾—äº†ä¿¡æ¯ï¼‰æ¯”ç›´æ¥è®­ç»ƒå°ç½‘ç»œçš„æ•ˆæœæ˜¾è‘—æå‡ï¼Œå¹¶ä¸”ä¿æŒäº†æ³›åŒ–èƒ½åŠ›
      * å¦å¤–ï¼ŒæŠŠå®éªŒæ•°æ®é‡Œé¢çš„æ‰€æœ‰â€œ3â€åˆ é™¤ï¼ˆå¯¹äºå°ç½‘ç»œæ¥è¯´3åœ¨trainingsetä¸­ä¸å­˜åœ¨,åªæ¥è‡ªTCHï¼‰ä»ç„¶å¯ä»¥å–å¾—è¾ƒå¥½çš„æ•ˆæœï¼ˆè€Œä¸”å…¶å®ç²¾åº¦æŸå¤±å¾ˆå¤§ç¨‹åº¦ä¸Šæ¥è‡ªäºè®­ç»ƒæ•°æ®imbalanceæ‰€å¸¦æ¥çš„biasï¼‰

### 3. [A Gift from Knowledge Distillation](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)
  * CVPR 2017
  * Another Perspeciveï¼š**The Flow Between Layers**
  * æ•ˆæœ
    1. æ›´å¿«æ”¶æ•›
    2. æ•ˆæœæ›´å¥½
    3. å…·æœ‰ä¸€å®šè¿ç§»èƒ½åŠ›ï¼ˆdifferent taskï¼‰
  * ä¸¾äº†ä¸€ä¸ªä¾‹å­ï¼šè¾“å…¥æ˜¯é—®é¢˜ï¼Œè¾“å‡ºæ˜¯ç»“æœï¼Œä¸­é—´è¿‡ç¨‹å°±æ˜¯ä¸­é—´æ­¥éª¤
  * Core:**The FLow Of The Solution** - Direction Between Features Of 2 Layers  
    * The **FSP Matrix**

$$ G_{i,j}(s;W)=\sum_{s=1}^{h}{\sum_{t=1}^{w}{\frac{F_{s,t,i}^1(x;W)xF^2_{s,t,j}(x;W)}{h \times W}}}  $$

  * F1,F2æ˜¯ä¸¤ä¸ªWxHxmçš„Feature Map
  * é€‰æ‹©TCHå’ŒSTUåˆ†åˆ«ç”Ÿæˆçš„Nä¸ªFSP MatrixåšL2èŒƒæ•°åŠ å…¥Lossï¼ˆä¸¤è€…çš„Fsp Matrixå¤§å°éœ€è¦ç›¸åŒï¼ˆæ–‡ç« ä¸­å¯¹äºä¸ä¸€æ ·åˆ†è¾¨ç‡çš„ä½¿ç”¨äº†Max-pooling
    * ~~Not That Elegant~~
    * ä¸¤ä¸ªç½‘ç»œåˆ†è¾¨ç‡éœ€è¦ç›¸åŒæ„Ÿè§‰å°±åƒæ˜¯resembleï¼Œè€Œä¸å¤ªæ˜¯distillï¼ˆä¸ªäººè§‚ç‚¹ï¼‰
      * ä½œè€…è¯´æ²¡æœ‰å¤šå°‘restrictï¼Œä½†æ˜¯è¿™ä¸ªå¯¹STUçš„ç»“æ„æ„Ÿè§‰é™åˆ¶æœ‰ç‚¹å¤šäº†
  ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910220809.png)
    * å®é™…è®­ç»ƒæ—¶**2 Stage**å­¦ä¹ FSPçŸ©é˜µå’Œå­¦ä¹ ä»»åŠ¡æ˜¯åˆ†å¼€çš„ ~~Not So Elegant~~
  * å®éªŒ
    * éƒ½ä½¿ç”¨ResNet on Cifar10/Cifar100
    * é”¤äº†FitNet
    * å®éªŒä¸­è¿˜å°è¯•äº†ç›´æ¥æŠŠweightå¤åˆ¶è¿‡å»ï¼Œè¿™ç¡®å®æ˜¯ä¸€ç§æç«¯æƒ…å†µ...
    * å®éªŒä¸­è¿˜è¯æ˜äº†TCHçš„FeatureChannelæ˜¯å¯ä»¥shuffleçš„ï¼Œä¹Ÿå°±æ˜¯FSPæ˜¯å¯ä»¥shuffledçš„
    * Transfer Learning
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910223127.png)

### 4. [Net2Net: Accelerating Learning via Knowledge Transfer](https://arxiv.org/abs/1511.05641)
  * Tianqi Chen & Ian Goodfellow
  * ICLR 2018
  * å¼ºè°ƒçš„æ˜¯**Knowledge Transfer**å’ŒDistillationä¸å®Œå…¨ç±»ä¼¼,æœ‰ä¸€å®šè”ç³»
  * æ›´åƒæ˜¯æè¿°ä¸€ç§**Design Flow**ï¼Œä¸“æ³¨è§£å†³çš„é—®é¢˜æ˜¯**Training Every Net From Scratch**æ˜¯æµªè´¹ï¼Œè§£å†³çš„æ˜¯**Accelerate Training**
    * ä¸ä¸€èˆ¬çš„Design Flowçš„æœ€å¤§åŒºåˆ«æ˜¯ï¼Œ**åˆ©ç”¨**é¢„å…ˆè®­ç»ƒçš„ç»“æ„ä¸æ¨¡å‹è€Œä¸ä¸¢å¼ƒå®ƒ ï¼ˆæœ‰é‚£ä¹ˆä¸€ç‚¹Incrementalçš„æ„å‘³ï¼Œä½œè€…åœ¨æ–‡ä¸­æåˆ°è¿™ç§æ–¹æ³•å¯ä»¥Smoothly instantiate a larger modelï¼‰
  * æ–¹æ³•åŸºäºfunction preserving transformation(æœ‰ç‚¹æ•°å­¦)ç›®æ ‡æ˜¯initialize STU 2 Represent The Same Function As Teacher
  * **Implemention**
    1. STUåŠ å…¥ä¸€äº›"Teacher Prediction Layer"ï¼Œå¹¶ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±é¢å¤–å±‚å»æ¥è¿‘TCHç½‘ç»œä¸­çš„æŸä¸€å±‚
      * ä½œè€…è®¤ä¸ºåŸç†æ˜¯ï¼š ```Teacher could provide a good interna Representation for the task```
      * è¿™ç§æ–¹æ³•ç±»ä¼¼FitNets
      * ä½œè€…è¡¨ç¤ºå®éªŒè¯´æ˜è¿™ç§æ–¹æ³•å¹¶æ²¡æœ‰æ¯”ä¼ ç»Ÿæ–¹æ³•å¥½å¤ªå¤š  

### 5.[Deep Mutual Learning](https://arxiv.org/abs/1706.00384)
  * CVPR 2018
  * å•TCHå•STU -> å•TCHå¤šSTU(STUä¹‹é—´è¿˜æœ‰Mutual Learning) -> ä¸éœ€è¦TCH
  * ä¸ä»…å¯ä»¥ä»å¤§æ¨¡å‹ä¸­å¾—åˆ°ä¸€ä¸ªå°æ¨¡å‹ï¼Œå•çº¯çš„å¯¹å¤§æ¨¡å‹è¿›è¡ŒMutualLearningçš„æ•ˆæœä¹Ÿæ¯”å•çº¯è®­ç»ƒå¥½
  > æ„Ÿè§‰è¿™å°±æ˜¯æ¢äº†ä¸€ç§å½¢å¼çš„Ensemble(æœ€å¤§çš„åŒºåˆ«å°±æ˜¯å®é™…ä¸Šçº¿çš„æ—¶å€™ä¸éœ€è¦å¤šä¸ªNetorwkéƒ½è·‘å–å¹³å‡)
  ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913101825.png)
  * Losså‡½æ•°æ˜¯æ­£å¸¸çš„åˆ†ç±»LossåŠ ä¸Šè¯¥ç½‘ç»œä¸å…¶ä»–ç½‘ç»œçš„åéªŒæ¦‚ç‡çš„KLæ•£åº¦
    * å¯¹å•ç½‘ç»œæ¥è¯´ç›¸å½“äºæ˜¯åŠ ä¸Šäº†ä¸€ä¸ªå¤šä½™çš„æƒ©ç½šé¡¹ï¼Ÿï¼ˆæƒ©ç½šå„ä¸ªç½‘ç»œè®­ç»ƒå‡ºæ¥çš„å·®è·è¿‡å¤§ï¼Ÿï¼‰
    * å¤šä¸ªSTUçš„æ—¶å€™å¯ä»¥æŠŠå…¶ä»–K-1ä¸ªSTUçš„Ensembleå½“æˆä¸€ä¸ªTCH
  * ğŸ¤”è¿™ä¸ªä¸œè¥¿çš„æå‡ºæ˜¯ä¸æ˜¯è¯´æ˜äº†ä¹‹å‰çš„å¤§æ¨¡å‹ä¸å¿…è¦ï¼Ÿä¸éœ€è¦ä¸€ä¸ªéå¸¸æ¥è¿‘æ•ˆæœå¥½çš„æ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰ï¼Œåªéœ€è¦ä¸€ä¸ªæ¯”è¾ƒæ¥è¿‘æ•ˆæœå¥½çš„æ¨¡å‹ï¼ˆï¼Ÿï¼‰ï¼Œè¯´ä¸æ¸…æ¥š
  * å®éªŒ
    * æ•°æ®é›†
      * Cifar100
      * Market 1501 ï¼ˆPerson ReID - Fine-Grainedï¼‰
    * æ¨¡å‹
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913103015.png)
      * å°çš„æ˜¯STU å¤§çš„æ˜¯TCH ï¼ˆInception V1ï¼› Wide ResNetï¼‰
    * Results
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913103633.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913103702.png)
      * ä¸Distilationå¯¹æ¯”ï¼š
        * Peer As Teacher Is Betterï¼
        * More Peersï¼šEven Better Than More Network Ensemble    
    * Why Does This Work?
      * ä¸å»å¯»æ‰¾ä¸€ä¸ªå¾ˆé™¡å³­å¾ˆDeepçš„Train Setæœ€ä¼˜ï¼Œè€Œæ˜¯å»å¯»æ‰¾ä¸€å—ç›¸å¯¹Robustçš„æœ€å°å€¼

### 6. [Born Again Neural Network](https://arxiv.org/abs/1805.04770)
  * Cite 76
  * ICML 2018
  * The BAN could OUTPERFORM the teacher in Image & Language Field
  * TCHä¸STUæœ‰å®Œå…¨ç›¸åŒçš„ç»“æ„
  * (**Main Contribution**)è®¤ä¸ºKDçš„å†…å®¹å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªTerm
    * ä¸€ä¸ªè•´å«ç€é”™è¯¯ä¿¡æ¯çš„Dark Knowledge  DKPP
    * ä¸€ä¸ªGTç›¸å…³çš„ï¼Œåªç›¸å½“äºåŸå…ˆgradientçš„ç®€å•rescaleï¼ˆ?ï¼‰  CWTM
  * Lossè¿˜æ˜¯TCHå’ŒSTUçš„Soft Targetçš„Cross Entropy
    * è®¤ä¸ºKDä¸éœ€è¦ä¸€ä¸ªStrong Master (withå®éªŒ)
  * å®éªŒ(æ¯”è¾ƒå¥å…¨ï¼Œä¹‹åå…·ä½“åˆ†æ)
    * æ•°æ®é›†ï¼š cifar100

### 7. [Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy](https://arxiv.org/abs/1711.05852)
  * Low Precison + KD (å¼ºè°ƒKDå¯ä»¥æ˜¾è‘—æé«˜low-bitç½‘è·¯çš„å‡†ç¡®åº¦)
  * **å…ˆç”¨pretrainçš„Full-Precisionï¼Œåšé‡åŒ–ä¹‹åï¼Œå†ç”¨KDåšFinetuneï¼Œè¿™æ ·æ•ˆæœæœ€å¥½**
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190913145408.png)

### 8. [Feature Fusion for Online Mutual Knowledge Distillation](https://arxiv.org/abs/1904.09058)ã€
  * **ä»ä¸åŒçš„Sub-Networkä¸­æ±‡é›†Feature Mapï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ç§Ensembleçš„æ–¹å¼** 
  * æ¯”è¾ƒappealingçš„æ˜¯æå‡ºäº†ä¸åŒçš„subnetworkå¯ä»¥ç”¨ä¸åŒçš„typeï¼ˆå› ä¸ºæå–çš„æ˜¯feature mapï¼‰ **But Not Different Task**
  * å®éªŒä¸­å¯¹æ¯”è¾ƒå’Œ

### 9. [Knowledge Distillation by On-the-Fly Native Ensemble](https://www.semanticscholar.org/paper/Knowledge-Distillation-by-On-the-Fly-Native-Lan-Zhu/c864e3785a9aecf25296781c272980eaed78e51a )
  * **è„±ç¦»Pretrained Teacher**
  * å•çº¯çš„é€šè¿‡KD,æ¥æå‡è®­ç»ƒæŸä¸ªæ¨¡å‹çš„æ•ˆæœ(ä¸€ä¸ªè¯¥é¢†åŸŸç›®å‰çš„ä¸»è¦åº”ç”¨ç‚¹ğŸ¤”æ„Ÿè§‰è¿˜æ˜¯æœ‰ä¸€ç‚¹åç‚¼ä¸¹çš„æ„Ÿè§‰)

### 10. [EnsembleNet: End-to-End Optimization of Multi-headed Models](https://arxiv.org/abs/1905.09979)


# Refs
> ä»¥ä¸‹æ–‡ç« æŒ‰ç…§å®è·µé¡ºåºæ’åˆ—,ä¸”ä¸å¯é¿å…çš„ä¼šæœ‰ä¸€äº›æ¯”è¾ƒæ²¡ç”¨çš„ä¸œè¥¿
1. [ã€ŠDistilling the Knowledge in a Neural Networkã€‹é˜…è¯»ç¬”è®°](https://zhuanlan.zhihu.com/p/51550142)
2. [çŸ¥è¯†è’¸é¦knowledge distillation åœ¨è‡ªç„¶è¯­è¨€å¤„ç†NLPä¸­æœ‰å“ªäº›æ–¹é¢çš„åº”ç”¨æˆ–å‘å±•ï¼Ÿ](https://www.zhihu.com/question/333196499)
3. [Git-Awesome-Knowledge-Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)
4. [æ•°æ®é›†è’¸é¦](https://zhuanlan.zhihu.com/p/56328042)
5. [â€œåœ¨çº¿è’¸é¦â€è®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œ](https://zhuanlan.zhihu.com/p/35698635) Hinton,Google Brain; å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ 
6. [ICML 2018 å†ç”Ÿç¥ç»ç½‘ç»œï¼šåˆ©ç”¨çŸ¥è¯†è’¸é¦æ”¶æ•›åˆ°æ›´ä¼˜çš„æ¨¡å‹](https://zhuanlan.zhihu.com/p/37384778)
