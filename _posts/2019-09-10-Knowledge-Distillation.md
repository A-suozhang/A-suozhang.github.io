---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      A Survey Of Knowledge Distillation              # æ ‡é¢˜ 
subtitle:   The Most Advanced & Elegant Compression        #å‰¯æ ‡é¢˜
date:       2019-09-10              # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/bg-term.jpg  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - DL
    - ç»¼è¿°
---

# Knowledge Distillation
> è¿™ä¸å‡ºæ„å¤–æ˜¯ä¸€ä¸ªå¤§å‘,å°†æ¥ä¼šå°†å¾ˆå¤šèµ„æ–™æ•´åˆåˆ°è¿™ç¯‡åšæ–‡ä¹‹ä¸­,å…¶ä¸­æ¯ä¸ªå°éƒ¨åˆ†å¯èƒ½ç‹¬ç«‹ä¸ºå…¶ä»–æ–‡ç« 
> å¸Œæœ›èƒ½å¤Ÿå¥½å¥½åšæŒåšå®Œ

# Overview & ğŸ¤”Thinking
> åœ¨ç†Ÿæ‚‰ä¸€ä¸ªé¢†åŸŸä¹‹å‰,é¦–å…ˆæœ‰ä¸€äº›æ„Ÿæ€§çš„è®¤è¯†,å¯ä»¥è¾¹è¯»è¾¹æ€è€ƒ,å°†æ¥çš„ä¸€äº›æ€è€ƒå†…å®¹ä¹Ÿå°†æ±‡å…¥è¿™é‡Œ
* **Why Knowledge Distillation**
  * The Model Is Too BIG (~~å±è¯~~)
* **What Is Knowledge Distillation?**
  * Distill/Transfer Knowledge from (a set of)large cumbersome models to a light single model (without significant performance loss)
* **Trade Off** Bigger Model(If Properly Designed) Could **Perform Better** But **More Computation**
  * æç«¯æƒ…å†µ: Ensemble - å‡ ä¹èƒ½ä¿è¯æé«˜performance,ä½†æ˜¯çº¿æ€§å¢åŠ è®¡ç®—é‡
* Knowledge - Where did it from?
    1. The *Params W* in NN
        * è¿™ä¸ªè§†è§’æŒ‡å‘äº†*finetune*:
          * å…ˆåœ¨å¤§æ•°æ®é›†(ImageNet)ä¸Šè®­ç»ƒ,ç„¶åå†åœ¨å°æ•°æ®é›†ä¸Šå¾®è°ƒ(COCO),è®­ç»ƒçš„æ˜¯åŒä¸€ä¸ªç½‘ç»œ 
    2. The Learned **Mapping** (More Abstract)
        * Knowledege Distillation ä»è¿™ä¸ªè§†è§’åˆ‡å…¥
* **How Do Knowledge Distillation Do**
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910165840.png)
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910171736.png)
* Stuä¸æ˜¯ç›´æ¥ä»Training Dataä¸­è·å–æ•°æ®,è€Œæ˜¯å­¦ç€å»followTCH
  * æ‰€ä»¥å­¦ä¹ çš„æ˜¯Soft Target: ä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹å­,ä¸æ˜¯ä¸€ä¸ªone-hotçš„vectorè€Œæ˜¯å„ç±»åˆ«çš„softmax
  * ä¸ºä»€ä¹ˆè¿™æ ·å¥½?
    * ç›´è§‚çš„çœ‹,æ›´å¥½çš„è¿˜åŸçš„äº‹å®,å¦‚æœteacher model trainçš„ä¸é”™çš„è¯,ç¡®å®ä¼šæœ‰å‡ ç±»æ¯”è¾ƒéš¾åŒºåˆ†çš„,è¿™ä¸ªæ¯”åªæœ‰ä¸€ç±»æ˜¯å¯¹çš„æ›´åŠ æ¥è¿‘çœŸå®
      * å¯ä»¥è®¤ä¸ºæ˜¯TCHå­¦åˆ°äº†è¿™å‡ ç±»æ¯”è¾ƒç›¸ä¼¼,å¹¶ä¸”å°†è¿™ä¸ªä¿¡æ¯ä¼ é€’ç»™STU
    * é“ç†ä¸Šè®² **è¿™æ ·çš„Targetå…·æœ‰æ›´å¤§çš„ç†µ!**
* Use **Softmax With Tempeature**
$$ q_i = \frac{e^{\frac{z_i}{T}}}{\sum_j{e^{\frac{z_j}{T}}}}$$
  * Distillation Lossåšäº†ä»€ä¹ˆ?
    * ç”±äºæ­£å¸¸NNè¾“å‡ºçš„ç»“æœä¸€èˆ¬æŸä¸€ç±»çš„ç½®ä¿¡åº¦å¾ˆé«˜,å­¦ä¸å¤ªå‡ºæ¥ä¸œè¥¿,ç”¨æ¸©åº¦å› å­T(*ä¸€èˆ¬åœ¨1~20*)å°†å„ç§ç±»åˆ«çš„åˆ†å¸ƒå¹³ç¼“ä¸€ä¸‹(ä¸ç„¶çš„è¯å°±æ˜¯ç±»ä¼¼ä¸€ä¸ªå†²æ¿€,~~æœ‰ç‚¹å±•å®½èƒ–èƒ–ç“£çš„æ„æ€~~)
* ç›®æ ‡å‡½æ•°:
  1. åªç”¨Soft Target,è’¸é¦çš„æ—¶å€™TCHä½¿ç”¨Softmax with Täº§ç”ŸSoft Target,STUä½¿ç”¨æ–°çš„softmaxåœ¨transfer set(è¿ç§»ç”¨çš„æ•°æ®set,åŒºåˆ†äºTraining set),STUä¸TCHä½¿ç”¨ç›¸åŒçš„T
  2. åŒæ—¶ä½¿ç”¨soft&hard,STUçš„æŸå¤±å‡½æ•°æ˜¯Hardä¸Soft Targetçš„åŠ æƒå’Œ
     * Hintonç»™äº†ä¸€ä¸ªDark Magic: è®©Hard Targetçš„æƒé‡å°(å› ä¸ºåœ¨æ±‚å¯¼æ•°çš„æ—¶å€™æ–°çš„ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ç›¸å½“äºéƒ½ä¹˜ä¸€ä¸ª1/T^2,soft targetå¯¹æ¢¯åº¦çš„å½±å“å¤©ç„¶æ¯”Hardå°) 
  3. ä¸ç”¨soft ~~åºŸè¯~~ 
* Distillationä¸å…¶ä»–çš„Model Compressionçš„æ–¹æ³•(æ¯”å¦‚Pruning,Quantize)æ˜¯å¯ä»¥ç»“åˆçš„
* è¯•éªŒè¯æ˜Soft Targetæ˜¯å¯ä»¥èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨çš„
* **Why Soft Target Work**
  * è®©å­¦ç”Ÿå­¦åˆ°ä¸œè¥¿,ä¸èƒ½åªç»™ä¸€ä¸ªç­”æ¡ˆ,éœ€è¦ç»™ä¸€ä¸ªå®Œæ•´çš„è§£ç­”è¿‡ç¨‹
* è¿‡å°‘çš„æ•°æ®ä¸èƒ½å¾ˆå¥½è¡¨è¾¾Teacherçš„æ‰€æœ‰Knowledge,æ‰€ä»¥éœ€è¦ä¸€äº›é¢å¤–çš„Data Augmentation
* **What To Worry**
  * ç›®å‰çš„ä¸€äº›Knowledge Distillationçš„æ–¹å¼æœ‰äº›çœ‹èµ·æ¥è¿˜æ˜¯æ¯”è¾ƒåƒåœ¨ç‚¼ä¸¹...é‚£ä¸ªå‹ç¼©BERTçš„æ–‡ç« çœ‹ä¸Šå»å¾ˆAppealing,ä½†æ˜¯åªæ˜¯æ¯”æœªè’¸é¦çš„é«˜äº†5ä¸ªç‚¹,è¿˜è¿œè¾¾ä¸åˆ°èƒ½å¤Ÿæ›¿ä»£æˆ–è€…æ˜¯ä¸Šçº¿
  * ä¸çŸ¥é“æ˜¯è’¸é¦èµ·äº†ä½œç”¨è¿˜æ˜¯å•çº¯çš„æ•°æ®é‡å¢å¤šäº†
  * æ¦‚å¿µå¾ˆç®€å•,ä½†æ˜¯å®é™…å®è·µèµ·æ¥æœ‰å¾ˆå¤šTrickå±‚é¢çš„ä¸œè¥¿

# Papers List
### 1. [FitNets](https://arxiv.org/abs/1412.6550.pdf)
  * Yohsua Bengio ICLR2015

### 2. [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
  * Hinton 
  * é¢†åŸŸçš„å¼€å±±

## Clips
* [â€œåœ¨çº¿è’¸é¦â€è®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œ](https://zhuanlan.zhihu.com/p/35698635) Hinton,Google Brain; å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ 
* 

# Refs
> ä»¥ä¸‹æ–‡ç« æŒ‰ç…§å®è·µé¡ºåºæ’åˆ—,ä¸”ä¸å¯é¿å…çš„ä¼šæœ‰ä¸€äº›æ¯”è¾ƒæ²¡ç”¨çš„ä¸œè¥¿
1. [ã€ŠDistilling the Knowledge in a Neural Networkã€‹é˜…è¯»ç¬”è®°](https://zhuanlan.zhihu.com/p/51550142)
2. [çŸ¥è¯†è’¸é¦knowledge distillation åœ¨è‡ªç„¶è¯­è¨€å¤„ç†NLPä¸­æœ‰å“ªäº›æ–¹é¢çš„åº”ç”¨æˆ–å‘å±•ï¼Ÿ](https://www.zhihu.com/question/333196499)
3. [Git-Awesome-Knowledge-Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)
4. [æ•°æ®é›†è’¸é¦](https://zhuanlan.zhihu.com/p/56328042)