---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      A Survey Of Knowledge Distillation              # æ ‡é¢˜ 
subtitle:   The Most Advanced & Elegant Compression        #å‰¯æ ‡é¢˜
date:       2019-09-10              # æ—¶é—´
author:     tianchen                      # ä½œè€…
header-img:  img/bg-term.png  #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - DL
    - ç»¼è¿°
---

# Knowledge Distillation
> è¿™ä¸å‡ºæ„å¤–æ˜¯ä¸€ä¸ªå¤§å‘,å°†æ¥ä¼šå°†å¾ˆå¤šèµ„æ–™æ•´åˆåˆ°è¿™ç¯‡åšæ–‡ä¹‹ä¸­,å…¶ä¸­æ¯ä¸ªå°éƒ¨åˆ†å¯èƒ½ç‹¬ç«‹ä¸ºå…¶ä»–æ–‡ç« 
> å¸Œæœ›èƒ½å¤Ÿå¥½å¥½åšæŒåšå®Œ

# Overview & ğŸ¤”Thinking
> åœ¨ç†Ÿæ‚‰ä¸€ä¸ªé¢†åŸŸä¹‹å‰,é¦–å…ˆæœ‰ä¸€äº›æ„Ÿæ€§çš„è®¤è¯†,å¯ä»¥è¾¹è¯»è¾¹æ€è€ƒ,å°†æ¥çš„ä¸€äº›æ€è€ƒå†…å®¹ä¹Ÿå°†æ±‡å…¥è¿™é‡Œ
* **Why Knowledge Distillation**
  * The Model Is Too BIG (~~å±è¯~~)
* **What Is Knowledge Distillation?**
  * Distill/Transfer Knowledge from (a set of)large cumbersome models to a light single model (without significant performance loss)
* **Trade Off** Bigger Model(If Properly Designed) Could **Perform Better** But **More Computation**
  * æç«¯æƒ…å†µ: Ensemble - å‡ ä¹èƒ½ä¿è¯æé«˜performance,ä½†æ˜¯çº¿æ€§å¢åŠ è®¡ç®—é‡
* Knowledge - Where did it from?
    1. The *Params W* in NN
        * è¿™ä¸ªè§†è§’æŒ‡å‘äº†*finetune*:
          * å…ˆåœ¨å¤§æ•°æ®é›†(ImageNet)ä¸Šè®­ç»ƒ,ç„¶åå†åœ¨å°æ•°æ®é›†ä¸Šå¾®è°ƒ(COCO),è®­ç»ƒçš„æ˜¯åŒä¸€ä¸ªç½‘ç»œ 
    2. The Learned **Mapping** (More Abstract)
        * Knowledege Distillation ä»è¿™ä¸ªè§†è§’åˆ‡å…¥
* **How Do Knowledge Distillation Do**
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910165840.png)
![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910171736.png)
* Stuä¸æ˜¯ç›´æ¥ä»Training Dataä¸­è·å–æ•°æ®,è€Œæ˜¯å­¦ç€å»followTCH
  * æ‰€ä»¥å­¦ä¹ çš„æ˜¯Soft Target: ä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹å­,ä¸æ˜¯ä¸€ä¸ªone-hotçš„vectorè€Œæ˜¯å„ç±»åˆ«çš„softmax
  * ä¸ºä»€ä¹ˆè¿™æ ·å¥½?
    * ç›´è§‚çš„çœ‹,æ›´å¥½çš„è¿˜åŸçš„äº‹å®,å¦‚æœteacher model trainçš„ä¸é”™çš„è¯,ç¡®å®ä¼šæœ‰å‡ ç±»æ¯”è¾ƒéš¾åŒºåˆ†çš„,è¿™ä¸ªæ¯”åªæœ‰ä¸€ç±»æ˜¯å¯¹çš„æ›´åŠ æ¥è¿‘çœŸå®
      * å¯ä»¥è®¤ä¸ºæ˜¯TCHå­¦åˆ°äº†è¿™å‡ ç±»æ¯”è¾ƒç›¸ä¼¼,å¹¶ä¸”å°†è¿™ä¸ªä¿¡æ¯ä¼ é€’ç»™STU
    * é“ç†ä¸Šè®² **è¿™æ ·çš„Targetå…·æœ‰æ›´å¤§çš„ç†µ!** è€Œä¸”è¿™æ ·çš„ç±»ä¹‹é—´æ¢¯åº¦çš„æ–¹å·®æ›´å°
* Use **Softmax With Tempeature**
$$ q_i = \frac{e^{\frac{z_i}{T}}}{\sum_j{e^{\frac{z_j}{T}}}}$$
  * Distillation Lossåšäº†ä»€ä¹ˆ?
    * ç”±äºæ­£å¸¸NNè¾“å‡ºçš„ç»“æœä¸€èˆ¬æŸä¸€ç±»çš„ç½®ä¿¡åº¦å¾ˆé«˜,å­¦ä¸å¤ªå‡ºæ¥ä¸œè¥¿,ç”¨æ¸©åº¦å› å­T(*ä¸€èˆ¬åœ¨1~20*)å°†å„ç§ç±»åˆ«çš„åˆ†å¸ƒå¹³ç¼“ä¸€ä¸‹(ä¸ç„¶çš„è¯å°±æ˜¯ç±»ä¼¼ä¸€ä¸ªå†²æ¿€,~~æœ‰ç‚¹å±•å®½æ—ç“£çš„æ„æ€~~)
* ç›®æ ‡å‡½æ•°:
  1. åªç”¨Soft Target,è’¸é¦çš„æ—¶å€™TCHä½¿ç”¨Softmax with Täº§ç”ŸSoft Target,STUä½¿ç”¨æ–°çš„softmaxåœ¨transfer set(è¿ç§»ç”¨çš„æ•°æ®set,åŒºåˆ†äºTraining set),STUä¸TCHä½¿ç”¨ç›¸åŒçš„T
  2. åŒæ—¶ä½¿ç”¨soft&hard,STUçš„æŸå¤±å‡½æ•°æ˜¯Hardä¸Soft Targetçš„åŠ æƒå’Œ
     * Hintonç»™äº†ä¸€ä¸ªDark Magic: è®©Hard Targetçš„æƒé‡å°(å› ä¸ºåœ¨æ±‚å¯¼æ•°çš„æ—¶å€™æ–°çš„ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ç›¸å½“äºéƒ½ä¹˜ä¸€ä¸ª1/T^2,soft targetå¯¹æ¢¯åº¦çš„å½±å“å¤©ç„¶æ¯”Hardå°) 
  3. ä¸ç”¨soft ~~åºŸè¯~~ 
* Distillationä¸å…¶ä»–çš„Model Compressionçš„æ–¹æ³•(æ¯”å¦‚Pruning,Quantize)æ˜¯å¯ä»¥ç»“åˆçš„
* è¯•éªŒè¯æ˜Soft Targetæ˜¯å¯ä»¥èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨çš„
* å‰æœŸå¤§å®¶æ–¹æ³•çš„ä¸»è¦åŒºåˆ«æ˜¯**What To Distill**
  * Origianl-KD: OUTPUT with Soft Target
  * ThinNet:     Intermediate Layer Weight
  * A Gift:      Flow Between Layer
* **Why Soft Target Work**
  * è®©å­¦ç”Ÿå­¦åˆ°ä¸œè¥¿,ä¸èƒ½åªç»™ä¸€ä¸ªç­”æ¡ˆ,éœ€è¦ç»™ä¸€ä¸ªå®Œæ•´çš„è§£ç­”è¿‡ç¨‹
* è¿‡å°‘çš„æ•°æ®ä¸èƒ½å¾ˆå¥½è¡¨è¾¾Teacherçš„æ‰€æœ‰Knowledge,æ‰€ä»¥éœ€è¦ä¸€äº›é¢å¤–çš„Data Augmentation
* **What To Worry**
  * ç›®å‰çš„ä¸€äº›Knowledge Distillationçš„æ–¹å¼æœ‰äº›çœ‹èµ·æ¥è¿˜æ˜¯æ¯”è¾ƒåƒåœ¨ç‚¼ä¸¹...é‚£ä¸ªå‹ç¼©BERTçš„æ–‡ç« çœ‹ä¸Šå»å¾ˆAppealing,ä½†æ˜¯åªæ˜¯æ¯”æœªè’¸é¦çš„é«˜äº†5ä¸ªç‚¹,è¿˜è¿œè¾¾ä¸åˆ°èƒ½å¤Ÿæ›¿ä»£æˆ–è€…æ˜¯ä¸Šçº¿
  * ä¸çŸ¥é“æ˜¯è’¸é¦èµ·äº†ä½œç”¨è¿˜æ˜¯å•çº¯çš„æ•°æ®é‡å¢å¤šäº†
  * æ¦‚å¿µå¾ˆç®€å•,ä½†æ˜¯å®é™…å®è·µèµ·æ¥æœ‰å¾ˆå¤šTrickå±‚é¢çš„ä¸œè¥¿
  * ï¼ˆä¸ªäººæ„Ÿè§‰ï¼‰è¿™ç§æ–¹å¼æœ¬è´¨ä¸Šè¿˜æ˜¯é’ˆå¯¹OverParamï¼Œå¯¹äºä¸€ä¸ªè®¾è®¡å¾—æ¯”è¾ƒelegantçš„ç½‘ç»œæ¶æ„å‹ç¼©æ•ˆç‡ä¸é«˜ï¼ˆç±»ä¼¼äºDeep Compressionå¯¹VGGå’ŒResNetï¼‰
    * ç°æœ‰çš„è®ºæ–‡å°è¯•å»Distill BERTæ•ˆæœåªæ˜¯ç¨æœ‰æå‡ï¼Œå’ŒåŸæ¨¡å‹å·®è·è¿˜æ˜¯å¾ˆè¿œ...ï¼ˆæ„Ÿè§‰é‚£ç¯‡æ–‡ç« ä¸å…¶è¯´æ˜¯å»å‹ç¼©BERTä¸å¦‚è¯´æ˜¯åˆ©ç”¨BERTçš„çŸ¥è¯†ï¼ˆä½†æ˜¯ä½œè€…å¥½åƒæœ¬æ„å°±æ˜¯åè€…ï¼Œåªæ˜¯æ ‡é¢˜çš„æ„æ€æœ‰ç‚¹åƒå‰è€…ï¼‰ï¼‰
    * **Not Too Bad**ç†è®ºä¸Šä¸ä¼šå‡ºç°â€œå¯¹äºä¸€ä¸ªdistillå¾—æ¯”è¾ƒå¥½çš„ç½‘ç»œå°±ä¸èƒ½ç”¨å‰ªæå®šç‚¹å»å‹ç¼©â€è¿™æ ·å­çš„é—®é¢˜
    * ä½†æ˜¯æˆ‘ä»¬ä¹Ÿä¸æ˜¯åªç”¨æ¥åšå‹ç¼©ï¼Œå¯èƒ½è¿˜è¦è€ƒè™‘è¿™ä¸ªä¸œè¥¿å¯¹Transferçš„åº”ç”¨ï¼ˆæ„Ÿè§‰è‚¯å®šæœ‰äººæ‹¿distillationåštransferè¿™æ ·çš„ï¼‰

# Papers List
### 1. [FitNets](https://arxiv.org/abs/1412.6550.pdf)
  * Yohsua Bengio ICLR2015
  * åœ¨KDæå‡ºä¹‹åï¼Œæ›´è¿›ä¸€æ­¥ï¼Œæå‡º*ä¸ä»…ä½¿ç”¨æœ€åçš„ç»“æœ*è€Œä¸”ä¹Ÿä½¿ç”¨*TCHçš„ä¸­é—´å±‚çš„ç»“æœ*ï¼ˆç”±äºTCHä¸€èˆ¬æ¯”STUå¤§ï¼Œæ‰€ä»¥è¿˜éœ€è¦å¼•å…¥é¢å¤–çš„å‚æ•°æ¥åšä¸€ä¸ªæ˜ å°„ï¼‰ï¼Œå¦‚æ ‡é¢˜è¯´ï¼Œç»™äº†STUä¸€ä¸ª**hint**ï¼Œè®©STUå˜å¾—**Deeper&Thiner**
    * hintæŒ‡TCHçš„æŸä¸€å±‚ï¼ŒSTUéœ€è¦å»æ‹Ÿåˆè¿™ä¸€å±‚ ï¼ˆåœ¨Lossä¸­åŠ å…¥ä¸€é¡¹æ˜¯ä»–ä»¬ä¸¤å±‚çš„L2NORMï¼‰
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910211632.png)

### 2. [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
  * Hinton 
  * é¢†åŸŸçš„å¼€å±±ï¼Œä¸»è¦è´¡çŒ®å°±æ˜¯æå‡ºäº† Softmax-With-Temperature
  * Experimentï¼š
    * Mnist
      * å¸¦Distillçš„å°ç½‘ç»œï¼ˆä»å¤§ç½‘ç»œä¸­è·å¾—äº†ä¿¡æ¯ï¼‰æ¯”ç›´æ¥è®­ç»ƒå°ç½‘ç»œçš„æ•ˆæœæ˜¾è‘—æå‡ï¼Œå¹¶ä¸”ä¿æŒäº†æ³›åŒ–èƒ½åŠ›
      * å¦å¤–ï¼ŒæŠŠå®éªŒæ•°æ®é‡Œé¢çš„æ‰€æœ‰â€œ3â€åˆ é™¤ï¼ˆå¯¹äºå°ç½‘ç»œæ¥è¯´3åœ¨trainingsetä¸­ä¸å­˜åœ¨,åªæ¥è‡ªTCHï¼‰ä»ç„¶å¯ä»¥å–å¾—è¾ƒå¥½çš„æ•ˆæœï¼ˆè€Œä¸”å…¶å®ç²¾åº¦æŸå¤±å¾ˆå¤§ç¨‹åº¦ä¸Šæ¥è‡ªäºè®­ç»ƒæ•°æ®imbalanceæ‰€å¸¦æ¥çš„biasï¼‰

### 3. [A Gift from Knowledge Distillation](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)
  * CVPR 2017
  * Another Perspeciveï¼š**The Flow Between Layers**
  * æ•ˆæœ
    1. æ›´å¿«æ”¶æ•›
    2. æ•ˆæœæ›´å¥½
    3. å…·æœ‰ä¸€å®šè¿ç§»èƒ½åŠ›ï¼ˆdifferent taskï¼‰
  * ä¸¾äº†ä¸€ä¸ªä¾‹å­ï¼šè¾“å…¥æ˜¯é—®é¢˜ï¼Œè¾“å‡ºæ˜¯ç»“æœï¼Œä¸­é—´è¿‡ç¨‹å°±æ˜¯ä¸­é—´æ­¥éª¤
  * Core:**The FLow Of The Solution** - Direction Between Features Of 2 Layers  
    * The **FSP Matrix**

### 4.[Deep Mutual Learning](https://arxiv.org/abs/1706.00384)
  * CVPR 2018

$$ G_{i,j}(s;W)=\sum_{s=1}^{h}{\sum_{t=1}^{w}{\frac{F_{s,t,i}^1(x;W)xF^2_{s,t,j}(x;W)}{h \times W}}}  $$
    * F1,F2æ˜¯ä¸¤ä¸ªWxHxmçš„Feature Map
    * é€‰æ‹©TCHå’ŒSTUåˆ†åˆ«ç”Ÿæˆçš„Nä¸ªFSP MatrixåšL2èŒƒæ•°åŠ å…¥Lossï¼ˆä¸¤è€…çš„Fsp Matrixå¤§å°éœ€è¦ç›¸åŒï¼ˆæ–‡ç« ä¸­å¯¹äºä¸ä¸€æ ·åˆ†è¾¨ç‡çš„ä½¿ç”¨äº†Max-poolingï¼‰~~Not That Elegant  ~~ï¼‰
      * ä¸¤ä¸ªç½‘ç»œåˆ†è¾¨ç‡éœ€è¦ç›¸åŒæ„Ÿè§‰å°±åƒæ˜¯resembleï¼Œè€Œä¸å¤ªæ˜¯distillï¼ˆä¸ªäººè§‚ç‚¹ï¼‰
        * ä½œè€…è¯´æ²¡æœ‰å¤šå°‘restrictï¼Œä½†æ˜¯è¿™ä¸ªå¯¹STUçš„ç»“æ„æ„Ÿè§‰é™åˆ¶æœ‰ç‚¹å¤šäº†
  ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910220809.png)
    * å®é™…è®­ç»ƒæ—¶**2 Stage**å­¦ä¹ FSPçŸ©é˜µå’Œå­¦ä¹ ä»»åŠ¡æ˜¯åˆ†å¼€çš„ ~~ Not So Elegant ~~
  * å®éªŒ
    * éƒ½ä½¿ç”¨ResNet on Cifar10/Cifar100
    * é”¤äº†FitNet
    * å®éªŒä¸­è¿˜å°è¯•äº†ç›´æ¥æŠŠweightå¤åˆ¶è¿‡å»ï¼Œè¿™ç¡®å®æ˜¯ä¸€ç§æç«¯æƒ…å†µ...
    * å®éªŒä¸­è¿˜è¯æ˜äº†TCHçš„FeatureChannelæ˜¯å¯ä»¥shuffleçš„ï¼Œä¹Ÿå°±æ˜¯FSPæ˜¯å¯ä»¥shuffledçš„
    * Transfer Learning
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20190910223127.png)


### 4. [Born Again Neural Network](https://arxiv.org/abs/1805.04770)
### 5. [Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy](https://arxiv.org/abs/1711.05852)
### 6. [Feature Fusion for Online Mutual Knowledge Distillation](https://arxiv.org/abs/1904.09058)
### 7. [Knowledge Distillation by On-the-Fly Native Ensemble](https://www.semanticscholar.org/paper/Knowledge-Distillation-by-On-the-Fly-Native-Lan-Zhu/c864e3785a9aecf25296781c272980eaed78e51a )
### 8. [EnsembleNet: End-to-End Optimization of Multi-headed Models](https://arxiv.org/abs/1905.09979)

## Clips
* [â€œåœ¨çº¿è’¸é¦â€è®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œ](https://zhuanlan.zhihu.com/p/35698635) Hinton,Google Brain; å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ 
* 

# Refs
> ä»¥ä¸‹æ–‡ç« æŒ‰ç…§å®è·µé¡ºåºæ’åˆ—,ä¸”ä¸å¯é¿å…çš„ä¼šæœ‰ä¸€äº›æ¯”è¾ƒæ²¡ç”¨çš„ä¸œè¥¿
1. [ã€ŠDistilling the Knowledge in a Neural Networkã€‹é˜…è¯»ç¬”è®°](https://zhuanlan.zhihu.com/p/51550142)
2. [çŸ¥è¯†è’¸é¦knowledge distillation åœ¨è‡ªç„¶è¯­è¨€å¤„ç†NLPä¸­æœ‰å“ªäº›æ–¹é¢çš„åº”ç”¨æˆ–å‘å±•ï¼Ÿ](https://www.zhihu.com/question/333196499)
3. [Git-Awesome-Knowledge-Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)
4. [æ•°æ®é›†è’¸é¦](https://zhuanlan.zhihu.com/p/56328042)