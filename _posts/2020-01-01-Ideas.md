---
layout:     post                    # 使用的布局（不需要改）
title:      Wild Ideas           # 标题 
subtitle:   一些不太成熟的idea，期待打脸       #副标题
date:       2020-01-01             # 时间
author:     tianchen                      # 作者
header-img:  img/bg-term.jpg  #这篇文章标题背景图片  
catalog: false                     # 是否归档
tags:                               #标签
     - private
---


# 2019-10-11 ~ 2020-02 Online Learning on Edge

* 借用给振华做PPT的机会梳理一下这个东西
  * (待续……)


# 2020-02 GATES

* 提升基于predictor的NAS
* embedding-将离散的网络架构映射到一个离散空间
* NAS的两个主要模块
  * Searching module - Controller
    * 基于predictor来sample，更好的sample
  * Architecture Evaluation - Predictor
    * 主要问题 - 慢（对于每个架构都要训练好多个epoch）
    * 解决方案
      * shared weights - 一个supernet训练各个部分
        * 加速architecture的evaluation过程
      * predictor是增加sample efficiency（与shared weights对应）(提高sample efficiency也可以用优化controller)
* 早期的encoding基于序列化的方法
  * 人为选择一个方法（规则）把一个图给序列化
  * 而本文使用了GCN的方式（还对gcn进行了一些修改）
    * 传统的GCN无法处理边上带有信息的（边只是表示连接或者不连接）
    * 传统GCN的边的意义是affinity（相似性），edge在NN中是information flow（相连的节点不一定相似），GCN节点上的信息是attribute也就是属性。
      * 有的文章直接把两个节点的embedding拼接起来（并不能保证同构的图能够获得一样的embedding）
      * 本文的目的GATES是去建模NN的information flow，首先将NN建模为一个异构图（包含了数据节点和op节点），information（是一个vector）经过每一个op，会做一个attention，相当于对information加上了一个该op所对应的mask，经过所有op之后最后的那个vector就是该网络的embedding


# 2020-03 SemiNAS

1. 对arch角度的Semi
  * Predictor训练的时候相当于只用到了少量的Label数据(Evaluation可以看作一个Label的过程)而大量的SS中Unlabel的数据并没有被利用起来
  * 还可以付诸到ActiveLearning，挑选一些比较Hard的样本来做Evaluation以提高效率
  * 经过一系列的Survey之后我们发现 - 主流的处理方法(SemiSupervised&ActiveLearning)
    * 都是建立在一个分类问题去Push Back Decision Boundary的前提上去做的
    * 基础都是分类，而NAS更是一个回归(或者是Rank)
    * 而且输入图片的空间(高维度)与输入Arch的空间上有一定差异性
  * 可不可以把MoCo的想法结合起来(主要这个Dictionary Learning的问题……看上去好像和NAS不是很搭)
  * **最大的问题是图这种结构本身是否符合Semi有意义的**
    * 或者说是Semi这个到底好在哪里？是利用的input数据当中的一些相关性，还是说Embedding空间本身更合理
      * 我觉得如果说是原本的那些TrainingTrick，并没有对embedding空间有一个多大的改变
      * 但是GAN或者是新的Unsupervised的一些方法是能够做到去获得一个更合理的Embedding空间的？
      * 甚至我们的问题Setting不是要把每一类分好，只是需要一个较好的簇就可以了
  * 不过这种Encoder做Momentum的操作倒是可能可以用在MultiStage的GATES当中（不知道是不是只能够Smooth训练还是有具体作用）

2.1 通过NAS去搜到一个对Semi更有效的架构
  * 类似RobustnessNAS / FewShotNAS等套起来的方法
  * 听起来比较naive没有太大的创新点，而且主观的想用处不会特大

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200326201819.png)

2.2 在数据(而不是Architecture)是Semi的情况下完成NAS
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200326113947.png)
  * Motivation: 目前完成NAS算法需要完全的数据来做训练，当我的数据集还未完全形成的时候，如果我希望拿到一个比较好的架构来做后期的训练(比如说训练是在线做的，Label的数据是在线来的时候，我就需要预先通过半监督的方式来获得一个很好的架构)
  * 实际上因为NAS对架构是一个Rank的过程，是否不需要完全的数据集，也可以完成训练(Evaluation)获得不同arch之间Arch的相对关系
  * (目前常用的NAS算法(Shared Weights/Predictor)的Evaluation相关性都相对较低，而Semi直观看来也会降低一定的相关性，是否能够容忍，或者加以设计以保证相关性不会太低。
  * TrainingTricks - 可以协助训练，但是不能作为衡量的标准
  * WorkFlow：
    * (主要对Shared Weights做)
  * 找到了一支用Self-Supervised的方法，其中DataEfficientSemiwithCPC
    * 这一支已经走到了Representation Learning一支的深处……开始很尼玛玄学了
    * 这支现在看上去还是比较新的，而老的一些基于GAN的Representative的方法在2016年之后就已经比较少见了
  * 对于基于GAN的方法，搜Encoder本身(本质上是一个CNN)是可以，没有什么问题
  * 后面接的那个Classifier可以直接换成Rank-based?（会带来什么好处嘛）？
  * 对于Unsupervised等领域，其实是找到了一个更好的Representation，是否可以我不需要去找最好的representation我只需要找到能对Rank有最大区分度的Representation就可以了(听上去好像不是太靠谱)，是否可以把这个思想融入到设计中

  * 对于MultiObj
    * 有一个Obj它的分辨率很低(代表着我们实际Valid的)，还有一个Obj的分辨率相对高，对应着Unlabel上的，但是会相对间接; 如何处理相对关系
    * 比如在一个上高出一定阈值，就可以不用看Unsupervised了，如果确实是可比的
    * 设置一个Reward的Threshold，如果小于Threshold的时候，我们才更加倾向Unsupervised
    * 在训练RL Controller的时候，基于Ranking的RL方法(不一定是RL)，当保序的时候希望把它拉大 - 感觉这个点上Semi就不是很关键了
    * 还可能有controller不训练的(SA/EA)，感觉就费劲了